{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathbolla/The-LLM-Cookbook-Practical-Recipes-for-Fine-Tuning-Optimization-and-Deployment/blob/main/Chapter_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub import login\n",
        "\n",
        "api = HfApi()\n",
        "whoami = api.whoami(token=\"hf_xxxxxxxxxxxxxx\")\n",
        "print(whoami)\n",
        "login(\"hf_xxxxxxxxxxxxxxxxxxxxxxxx\")"
      ],
      "metadata": {
        "id": "Y39Hr892m9r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-1: Generating QA Pairs with a Judge Model\n"
      ],
      "metadata": {
        "id": "gn09-krNugW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Procedure 1: Generating Question-Answer Pairs with a Judge Model ---\n",
        "# Goal: Use a multi-model, generate-and-rank pipeline to create high-quality synthetic data.\n",
        "# Prerequisites: pip install transformers torch accelerate bitsandbytes\n",
        "# Note: This procedure loads TWO models. While smaller models are used here to reduce VRAM,\n",
        "#       it can still be resource-intensive (>16GB recommended).\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import json\n",
        "import re\n",
        "\n",
        "# 1. --- Configuration ---\n",
        "# A small, fast model to generate multiple candidate answers\n",
        "GENERATOR_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# A separate, powerful model (small but strong reasoning) to evaluate the candidates\n",
        "JUDGE_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# 2. --- Setup the Generator and Judge Model Pipelines ---\n",
        "print(f\"Loading GENERATOR model: {GENERATOR_MODEL_ID}\")\n",
        "try:\n",
        "    generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL_ID)\n",
        "    generator_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=GENERATOR_MODEL_ID,\n",
        "        tokenizer=generator_tokenizer,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    print(\"Generator model pipeline loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading generator model pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nLoading JUDGE model: {JUDGE_MODEL_ID}\")\n",
        "try:\n",
        "    judge_tokenizer = AutoTokenizer.from_pretrained(JUDGE_MODEL_ID, trust_remote_code=True)\n",
        "    judge_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=JUDGE_MODEL_ID,\n",
        "        tokenizer=judge_tokenizer,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    print(\"Judge model pipeline loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading judge model pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 3. --- Define the Document and Prompt Templates ---\n",
        "document = \"\"\"\n",
        "Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy, through a process that converts carbon dioxide and water into sugars (glucose) and oxygen. This process is crucial for life on Earth as it produces most of the oxygen in the atmosphere. The chemical equation for photosynthesis is 6CO2 + 6H2O â†’ C6H12O6 + 6O2. Chlorophyll is the primary pigment used in photosynthesis; it absorbs blue and red light and reflects green light, which is why plants appear green.\n",
        "\"\"\"\n",
        "\n",
        "generator_prompt_template = \"\"\"\n",
        "Given the following document, generate three distinct question-answer pairs based on its content. The questions should be insightful and the answers should be accurate and concise. Format the output as a list of JSON objects, where each object has a \"question\" and an \"answer\" key.\n",
        "\n",
        "Document:\n",
        "\\\"\\\"\\\"\n",
        "{document}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "JSON Output:\n",
        "\"\"\"\n",
        "\n",
        "judge_prompt_template = \"\"\"\n",
        "You are an expert evaluator. Your task is to analyze multiple sets of synthetically generated question-answer pairs based on a source document. Evaluate them based on the following criteria:\n",
        "1.  **Accuracy**: Is the answer factually correct according to the document?\n",
        "2.  **Relevance**: Does the question directly relate to a key concept in the document?\n",
        "3.  **Clarity**: Are the question and answer easy to understand?\n",
        "\n",
        "Below are the source document and the candidate QA sets.\n",
        "\n",
        "Source Document:\n",
        "\\\"\\\"\\\"\n",
        "{document}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "---\n",
        "Candidate QA Sets:\n",
        "{candidate_sets}\n",
        "---\n",
        "\n",
        "First, provide a brief step-by-step reasoning for your choice, explaining which set is superior and why.\n",
        "Finally, on a new line, state your final choice by reprinting the single best JSON list.\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "Final Choice:\n",
        "\"\"\"\n",
        "\n",
        "# 4. --- Generate Multiple Candidate QA Sets ---\n",
        "prompt = generator_prompt_template.format(document=document)\n",
        "print(\"\\nGenerating multiple candidate synthetic data sets...\")\n",
        "candidate_responses = []\n",
        "try:\n",
        "    # Generate two different sets of responses\n",
        "    outputs = generator_pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=300,\n",
        "        num_return_sequences=2, # Generate two candidates\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        eos_token_id=generator_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    for i, out in enumerate(outputs):\n",
        "        generated_text = out['generated_text']\n",
        "        response_part = generated_text[len(prompt):]\n",
        "        # Find the JSON list within the response\n",
        "        json_match = re.search(r'\\[.*\\]', response_part, re.DOTALL)\n",
        "        if json_match:\n",
        "            candidate_responses.append(json_match.group(0))\n",
        "            print(f\"\\n--- Candidate {i+1} ---\")\n",
        "            print(candidate_responses[-1])\n",
        "\n",
        "    if not candidate_responses:\n",
        "        raise ValueError(\"No valid JSON lists were generated by the generator model.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during candidate generation: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 5. --- Use the Judge Model to Rank and Select the Best Set ---\n",
        "print(\"\\n--- Submitting candidates to the Judge Model for evaluation ---\")\n",
        "\n",
        "# Format the candidates for the judge prompt\n",
        "formatted_candidates = \"\"\n",
        "for i, candidate_json in enumerate(candidate_responses):\n",
        "    formatted_candidates += f\"Candidate {i+1}:\\n{candidate_json}\\n\\n\"\n",
        "\n",
        "judge_prompt = judge_prompt_template.format(\n",
        "    document=document,\n",
        "    candidate_sets=formatted_candidates\n",
        ")\n",
        "\n",
        "try:\n",
        "    judge_outputs = judge_pipe(\n",
        "        judge_prompt,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False, # We want a deterministic judgment\n",
        "        eos_token_id=judge_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    judge_response = judge_outputs[0]['generated_text'][len(judge_prompt):]\n",
        "\n",
        "    print(\"\\n--- Judge's Full Response ---\")\n",
        "    print(judge_response)\n",
        "\n",
        "    # --- Extract the Final Curated Dataset ---\n",
        "    final_choice_match = re.search(r'Final Choice:\\s*(\\[.*\\])', judge_response, re.DOTALL)\n",
        "\n",
        "    if final_choice_match:\n",
        "        final_json_str = final_choice_match.group(1)\n",
        "        parsed_json = json.loads(final_json_str)\n",
        "        pretty_json = json.dumps(parsed_json, indent=2)\n",
        "\n",
        "        print(\"\\n--- Final Curated QA Pairs (Selected by Judge) ---\")\n",
        "        print(pretty_json)\n",
        "    else:\n",
        "        print(\"\\n--- Could not parse the final choice from the judge's output ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during judgment: {e}\")\n"
      ],
      "metadata": {
        "id": "U33t8VaoAd4T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-31T07:27:55.793561Z",
          "iopub.execute_input": "2025-08-31T07:27:55.794130Z",
          "iopub.status.idle": "2025-08-31T07:30:50.994415Z",
          "shell.execute_reply.started": "2025-08-31T07:27:55.794095Z",
          "shell.execute_reply": "2025-08-31T07:30:50.993642Z"
        },
        "colab": {
          "referenced_widgets": [
            "2b8372d888b343ca8d50fffe4eac1196",
            "7e1b2c07b3e0482b9090924b489b4d17",
            "aee8a6159a9b41aa8b81ffd15facbcf3",
            "73cabd96e59f43a79abaaf5a8f42cb22",
            "211292f7c1dc41bd92306e199ec8eeb4",
            "15da7f10af5c45f1a866e166a33344fa",
            "51497e08e7804720ba6f236f07d08632",
            "6bb46cb9ab12408297a891217d175e0f",
            "271eb3d5c5864af2addca9ada3a0d101",
            "9ab69acf300941cab1caf7dd987460fc",
            "86c15fd81208492ab427c012240d314a",
            "cb69893f20924be0b22c7e1840e09638",
            "949c6235f9b448d6bd97cbca90ecc405",
            "fb957a58fed44b6f9e1dd7ee255ad331",
            "36601cbbba8e4a0c80cd0b24e06561f0",
            "4210728c7c574f92a0ab5681c5fd477e",
            "060e4b9a4e15405683e16cb8feb525bb",
            "58b8dff3a41745079ab5d1b61dfb987a",
            "ef5f280998b14bbb95641b645d10dc3d",
            "b9419f90903e481da04fbc250db10026",
            "7b58dacad945490f969bb351215ddb5c",
            "f00f6f73496b4a618b7671b3a6ea62f0",
            "eab27bd83a604fa8aabf7b6a6856e9d4",
            "c5fab23faefe497590e2d9ad27f4f29e",
            "75dfb6987a7243c18ee91ce5e8af63bc"
          ]
        },
        "outputId": "bdb32302-f9f4-4238-e94a-30a79b1084f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-08-31 07:28:07.922171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756625288.250429      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756625288.344962      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading GENERATOR model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b8372d888b343ca8d50fffe4eac1196"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e1b2c07b3e0482b9090924b489b4d17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aee8a6159a9b41aa8b81ffd15facbcf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73cabd96e59f43a79abaaf5a8f42cb22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "211292f7c1dc41bd92306e199ec8eeb4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15da7f10af5c45f1a866e166a33344fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51497e08e7804720ba6f236f07d08632"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Generator model pipeline loaded successfully.\n\nLoading JUDGE model: HuggingFaceH4/zephyr-7b-beta\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bb46cb9ab12408297a891217d175e0f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "271eb3d5c5864af2addca9ada3a0d101"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ab69acf300941cab1caf7dd987460fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86c15fd81208492ab427c012240d314a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb69893f20924be0b22c7e1840e09638"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "949c6235f9b448d6bd97cbca90ecc405"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb957a58fed44b6f9e1dd7ee255ad331"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36601cbbba8e4a0c80cd0b24e06561f0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4210728c7c574f92a0ab5681c5fd477e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "060e4b9a4e15405683e16cb8feb525bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58b8dff3a41745079ab5d1b61dfb987a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef5f280998b14bbb95641b645d10dc3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9419f90903e481da04fbc250db10026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b58dacad945490f969bb351215ddb5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f00f6f73496b4a618b7671b3a6ea62f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eab27bd83a604fa8aabf7b6a6856e9d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5fab23faefe497590e2d9ad27f4f29e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75dfb6987a7243c18ee91ce5e8af63bc"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Judge model pipeline loaded successfully.\n\nGenerating multiple candidate synthetic data sets...\n\n--- Candidate 1 ---\n[\n    {\"question\": \"What is photosynthesis and how does it produce oxygen?\", \"answer\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy through a process that converts carbon dioxide and water into sugars (glucose) and oxygen. This process is crucial for life on Earth as it produces most of the oxygen in the atmosphere.\"},\n    {\"question\": \"What is chlorophyll and how does it contribute to photosynthesis?\", \"answer\": \"Chlorophyll is the primary pigment used in photosynthesis. It absorbs blue and red light and reflects green light, which is why plants appear green. Chlorophyll is responsible for converting light energy into chemical energy.\"},\n    {\"question\": \"What is the chemical equation for photosynthesis and how does it work?\", \"answer\": \"The chemical equation for photosynthesis is 6CO2 + 6H2O â†’ C6H12O6 + 6O2. The carbon dioxide (CO2) and water (H2O) are absorbed by chlorophyll, and oxygen is produced.\"}\n]\n\n--- Candidate 2 ---\n[\n    {\n        \"question\": \"What is the primary pigment used in photosynthesis?\",\n        \"answer\": \"Chlorophyll\"\n    },\n    {\n        \"question\": \"How does photosynthesis convert light energy into chemical energy?\",\n        \"answer\": \"The chemical equation for photosynthesis is 6CO2 + 6H2O â†’ C6H12O6 + 6O2\"\n    },\n    {\n        \"question\": \"What is the primary pigment used in photosynthesis?\",\n        \"answer\": \"Chlorophyll\"\n    }\n]\n\n--- Submitting candidates to the Judge Model for evaluation ---\n\n--- Judge's Full Response ---\n\n```json\n[\n    {\"question\": \"What is photosynthesis and how does it produce oxygen?\", \"answer\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy through a process that converts carbon dioxide and water into sugars (glucose) and oxygen. This process is crucial for life on Earth as it produces most of the oxygen in the atmosphere.\"},\n    {\"question\": \"What is chlorophyll and how does it contribute to photosynthesis?\", \"answer\": \"Chlorophyll is the primary pigment used in photosynthesis. It absorbs blue and red light and reflects green light, which is why plants appear green. Chlorophyll is responsible for converting light energy into chemical energy.\"},\n    {\"question\": \"What is the chemical equation for photosynthesis and how does it work?\", \"answer\": \"The chemical equation for photosynthesis is 6CO2 + 6H2O â†’ C6H12O6 + 6O2. The carbon dioxide (CO2) and water (H2O) are absorbed by chlorophyll, and oxygen is produced.\"}\n]\n\n\nCandidate 1 is superior to Candidate 2 because it provides more detailed and comprehensive answers. Candidate 1's answers accurately describe the process of photosynthesis, the role of chlorophyll, and the chemical equation for photosynthesis. Candidate 2's answers, on the other hand, repeat the same answer for the second question, which is not relevant to the topic. Therefore, Candidate 1 is more accurate, relevant, and clear than Candidate 2.\n\n```json\n[\n    {\"question\": \"What is photosynthesis and how does it produce oxygen?\", \"answer\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy through a process that converts carbon dioxide and water into sugars (glucose) and oxygen. This process is crucial for life on Earth as it produces most of the oxygen in the atmosphere.\"},\n    {\"question\": \"What is chlorophyll and how does it contribute to photosynthesis?\", \"answer\": \"Chlorophyll is the primary pigment used in\n\n--- Could not parse the final choice from the judge's output ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-2: Generating Instructions (Self-Instruct Style)"
      ],
      "metadata": {
        "id": "ymDSt2AAupN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Generating Instructions (Self-Instruct Style) ---\n",
        "# Goal: Use an LLM to generate new, diverse instructions based on seed examples.\n",
        "# Method: Prompts an instruction-tuned LLM with examples to generate more instructions.\n",
        "# Note: This only generates the *instructions*, not the corresponding outputs.\n",
        "#       Quality depends heavily on the LLM and the seed examples.\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "# Use a strong instruction-tuned model if possible\n",
        "MODEL_ID = \"google/gemma-2b-it\" # Or Mistral-Instruct, etc.\n",
        "MAX_NEW_TOKENS_INSTR = 300 # Max tokens for the list of new instructions\n",
        "\n",
        "# Use GPU if available\n",
        "device_index = 0 if torch.cuda.is_available() else -1\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
        "\n",
        "# --- 1. Load Model and Tokenizer (via Pipeline) ---\n",
        "print(f\"Loading pipeline for model: {MODEL_ID}\")\n",
        "try:\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=MODEL_ID,\n",
        "        tokenizer=MODEL_ID,\n",
        "        torch_dtype=dtype,\n",
        "        device=device_index,\n",
        "    )\n",
        "    if generator.tokenizer.pad_token is None: generator.tokenizer.pad_token = generator.tokenizer.eos_token\n",
        "    if generator.model.config.pad_token_id is None: generator.model.config.pad_token_id = generator.tokenizer.pad_token_id\n",
        "    print(\"Pipeline loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Define Seed Instructions ---\n",
        "# Provide a diverse set of examples to guide the generation\n",
        "seed_instructions = [\n",
        "    \"Explain the difference between supervised and unsupervised learning.\",\n",
        "    \"Write a short email inviting a colleague to a project meeting.\",\n",
        "    \"Generate a list of 5 creative names for a new coffee shop.\",\n",
        "    \"What are the main benefits of using renewable energy sources?\",\n",
        "    \"Provide Python code to read a CSV file into a pandas DataFrame.\",\n",
        "]\n",
        "\n",
        "# --- 3. Craft the Prompt for Instruction Generation ---\n",
        "# The prompt asks the model to act as an instruction generator\n",
        "# and create new instructions similar to, but distinct from, the seeds.\n",
        "prompt = f\"\"\"You are an expert instruction generator. Your task is to create a list of 5 new, diverse instructions that are different from the examples provided below, but cover a similar range of topics and task types (e.g., explanation, writing, brainstorming, coding, Q&A).\n",
        "\n",
        "Do not repeat the examples. Ensure the new instructions are clear and actionable.\n",
        "\n",
        "Examples of existing instructions:\n",
        "\\\"\\\"\\\"\n",
        "- {seed_instructions[0]}\n",
        "- {seed_instructions[1]}\n",
        "- {seed_instructions[2]}\n",
        "- {seed_instructions[3]}\n",
        "- {seed_instructions[4]}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Generate 5 new instructions below, formatted as a numbered list:\n",
        "1. \"\"\" # Start the list for the model\n",
        "\n",
        "print(\"--- Generating New Instructions ---\")\n",
        "print(f\"Prompt being sent to model:\\n{prompt}\")\n",
        "\n",
        "# --- 4. Generate New Instructions ---\n",
        "try:\n",
        "    outputs = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=MAX_NEW_TOKENS_INSTR,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.8, # Higher temperature for more diversity\n",
        "        top_p=0.95,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = outputs[0]['generated_text']\n",
        "    # Extract the generated list part\n",
        "    instruction_list_str = \"1. \" + generated_text.split(\"Generate 5 new instructions below, formatted as a numbered list:\\n1. \")[-1].strip()\n",
        "\n",
        "    print(\"\\n--- Generated Instructions ---\")\n",
        "    print(instruction_list_str)\n",
        "\n",
        "    # --- 5. Post-processing (Optional) ---\n",
        "    # Split the string into a list of instructions\n",
        "    generated_instructions = [line.split('. ', 1)[-1] for line in instruction_list_str.split('\\n') if line.strip() and line[0].isdigit()]\n",
        "    print(\"\\n--- Parsed Instructions ---\")\n",
        "    print(generated_instructions)\n",
        "    # Next step would be to filter these and then generate outputs for them (using another LLM call).\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during instruction generation: {e}\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "id": "ipH5K2mGups0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-31T07:41:35.589407Z",
          "iopub.execute_input": "2025-08-31T07:41:35.590065Z",
          "iopub.status.idle": "2025-08-31T07:42:06.205082Z",
          "shell.execute_reply.started": "2025-08-31T07:41:35.590042Z",
          "shell.execute_reply": "2025-08-31T07:42:06.204433Z"
        },
        "colab": {
          "referenced_widgets": [
            "01ea176497714ad3b2f5992fcba81f3a",
            "b10798db61d44655bae512d731cefa58",
            "48418b738c0c4bcfa3b4f703f9bbaf5c",
            "96130c79b4fd4825a3589d3777554d46",
            "cc8af38d489f403d8886cffcb63f3f4b",
            "ad3561bc0f284226a31c1e756e05f8bf",
            "0c3e5505f4c14f31a492968ef29c8d98",
            "531a6e600f7d4a299f4a57a7051287e1",
            "530249f797414da0a43952c51c21f9f3",
            "f176822646f840b9a61e2cf6dee4cfaa",
            "47bee1c22bda42a19c17a0097d28a100"
          ]
        },
        "outputId": "2b6ec538-5613-4111-9ee5-91a135bd729d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading pipeline for model: google/gemma-2b-it\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01ea176497714ad3b2f5992fcba81f3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b10798db61d44655bae512d731cefa58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48418b738c0c4bcfa3b4f703f9bbaf5c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96130c79b4fd4825a3589d3777554d46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc8af38d489f403d8886cffcb63f3f4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad3561bc0f284226a31c1e756e05f8bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c3e5505f4c14f31a492968ef29c8d98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "531a6e600f7d4a299f4a57a7051287e1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "530249f797414da0a43952c51c21f9f3"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f176822646f840b9a61e2cf6dee4cfaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47bee1c22bda42a19c17a0097d28a100"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Pipeline loaded.\n--- Generating New Instructions ---\nPrompt being sent to model:\nYou are an expert instruction generator. Your task is to create a list of 5 new, diverse instructions that are different from the examples provided below, but cover a similar range of topics and task types (e.g., explanation, writing, brainstorming, coding, Q&A).\n\nDo not repeat the examples. Ensure the new instructions are clear and actionable.\n\nExamples of existing instructions:\n\"\"\"\n- Explain the difference between supervised and unsupervised learning.\n- Write a short email inviting a colleague to a project meeting.\n- Generate a list of 5 creative names for a new coffee shop.\n- What are the main benefits of using renewable energy sources?\n- Provide Python code to read a CSV file into a pandas DataFrame.\n\"\"\"\n\nGenerate 5 new instructions below, formatted as a numbered list:\n1. \n\n--- Generated Instructions ---\n1. ðŸ¤– Design a 3D printed object that can float in the air.\n2. Create a persuasive argument for why pets deserve the same rights as humans.\n3. Develop a detailed storyboard outlining the entire process of creating a new video game.\n4. Solve a complex math equation using only mental math.\n5. Share a personal story of overcoming a significant obstacle and how it shaped your life.\n\n--- Parsed Instructions ---\n['ðŸ¤– Design a 3D printed object that can float in the air.', 'Create a persuasive argument for why pets deserve the same rights as humans.', 'Develop a detailed storyboard outlining the entire process of creating a new video game.', 'Solve a complex math equation using only mental math.', 'Share a personal story of overcoming a significant obstacle and how it shaped your life.']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-3 Data Augmentation via Transformation"
      ],
      "metadata": {
        "id": "5qewASOsnRrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Procedure 3: Data Augmentation via Transformation ---\n",
        "# Goal: Augment a small dataset using a variety of LLM-based and rule-based transformation techniques.\n",
        "# Prerequisites: pip install transformers[sentencepiece] torch accelerate\n",
        "# Note: This procedure uses multiple models and can be resource-intensive.\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import random\n",
        "\n",
        "# 1. --- Setup Augmentation Models ---\n",
        "print(\"Setting up augmentation models...\")\n",
        "# Use translation pipelines for multilingual augmentation (back-translation)\n",
        "try:\n",
        "    translator_en_de = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", device=1)\n",
        "    translator_de_en = pipeline(\"translation_de_to_en\", model=\"Helsinki-NLP/opus-mt-de-en\", device=1)\n",
        "    print(\"Translation models loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load translation models: {e}\")\n",
        "    translator_en_de, translator_de_en = None, None\n",
        "\n",
        "# Use a text generation model for more sophisticated transformations\n",
        "try:\n",
        "    augmenter_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device=1\n",
        "    )\n",
        "    print(\"Augmenter LLM loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load augmenter LLM: {e}\")\n",
        "    augmenter_pipe = None\n",
        "\n",
        "# 2. --- Original Data Point ---\n",
        "original_text = \"The financial report indicates a significant increase in quarterly revenue due to strong sales in the technology sector.\"\n",
        "print(f\"\\nOriginal Text:\\n\\\"{original_text}\\\"\")\n",
        "\n",
        "# Dictionary to store all generated variations\n",
        "augmented_samples = {\"Original\": original_text}\n",
        "\n",
        "# 3. --- Apply Transformations ---\n",
        "\n",
        "# a) LLM-Based Paraphrasing\n",
        "print(\"\\n--- Applying LLM-Based Paraphrasing ---\")\n",
        "if augmenter_pipe:\n",
        "    paraphrase_prompt = f\"<|system|>\\nYou are a helpful assistant.</s>\\n<|user|>\\nParaphrase the following sentence while preserving its core meaning: \\\"{original_text}\\\"</s>\\n<|assistant|>\\n\"\n",
        "    paraphrased_text = augmenter_pipe(paraphrase_prompt, max_new_tokens=70, do_sample=True, temperature=0.7)[0]['generated_text'].split(\"<|assistant|>\")[-1].strip()\n",
        "    augmented_samples[\"Paraphrased\"] = paraphrased_text\n",
        "\n",
        "# b) Multilingual Augmentation (Back-Translation)\n",
        "print(\"\\n--- Applying Back-Translation (English -> German -> English) ---\")\n",
        "if translator_en_de and translator_de_en:\n",
        "    translated_to_de = translator_en_de(original_text)[0]['translation_text']\n",
        "    back_translated_text = translator_de_en(translated_to_de)[0]['translation_text']\n",
        "    augmented_samples[\"Back-Translated\"] = back_translated_text\n",
        "\n",
        "# c) Length Variations (Summarize and Elaborate)\n",
        "print(\"\\n--- Applying Length Variations ---\")\n",
        "if augmenter_pipe:\n",
        "    summarize_prompt = f\"<|system|>\\nYou are a helpful assistant.</s>\\n<|user|>\\nSummarize this sentence in fewer words: \\\"{original_text}\\\"</s>\\n<|assistant|>\\n\"\n",
        "    summarized_text = augmenter_pipe(summarize_prompt, max_new_tokens=25, do_sample=False)[0]['generated_text'].split(\"<|assistant|>\")[-1].strip()\n",
        "    augmented_samples[\"Summarized\"] = summarized_text\n",
        "\n",
        "    elaborate_prompt = f\"<|system|>\\nYou are a helpful assistant.</s>\\n<|user|>\\nElaborate on this sentence, adding more specific but plausible details: \\\"{original_text}\\\"</s>\\n<|assistant|>\\n\"\n",
        "    elaborated_text = augmenter_pipe(elaborate_prompt, max_new_tokens=80, do_sample=True, temperature=0.8)[0]['generated_text'].split(\"<|assistant|>\")[-1].strip()\n",
        "    augmented_samples[\"Elaborated\"] = elaborated_text\n",
        "\n",
        "# d) Counterfactual Generation\n",
        "print(\"\\n--- Applying Counterfactual Generation ---\")\n",
        "if augmenter_pipe:\n",
        "    counterfactual_prompt = f\"<|system|>\\nYou are a helpful assistant.</s>\\n<|user|>\\nRewrite the following sentence to describe the opposite scenario (a negative outcome): \\\"{original_text}\\\"</s>\\n<|assistant|>\\n\"\n",
        "    counterfactual_text = augmenter_pipe(counterfactual_prompt, max_new_tokens=70, do_sample=True, temperature=0.7)[0]['generated_text'].split(\"<|assistant|>\")[-1].strip()\n",
        "    augmented_samples[\"Counterfactual\"] = counterfactual_text\n",
        "\n",
        "# e) Domain-Specific Augmentation (Style Transfer)\n",
        "print(\"\\n--- Applying Domain-Specific Style Transfer (to Legal) ---\")\n",
        "if augmenter_pipe:\n",
        "    style_transfer_prompt = f\"<|system|>\\nYou are a helpful assistant.</s>\\n<|user|>\\nRewrite the following sentence in a formal, legal style: \\\"{original_text}\\\"</s>\\n<|assistant|>\\n\"\n",
        "    legal_style_text = augmenter_pipe(style_transfer_prompt, max_new_tokens=80, do_sample=True, temperature=0.5)[0]['generated_text'].split(\"<|assistant|>\")[-1].strip()\n",
        "    augmented_samples[\"Legal Style\"] = legal_style_text\n",
        "\n",
        "# f) Basic Transformations (Synonym Replacement)\n",
        "print(\"\\n--- Applying Basic Transformation (Synonym Replacement) ---\")\n",
        "def synonym_replacement(sentence, synonyms, n=1):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "    replaceable_words = [word for word in words if word.lower().strip(\".,\") in synonyms]\n",
        "    if not replaceable_words:\n",
        "        return sentence\n",
        "\n",
        "    for _ in range(n):\n",
        "        word_to_replace = random.choice(replaceable_words)\n",
        "        # Preserve punctuation\n",
        "        punctuation = ''\n",
        "        clean_word = word_to_replace\n",
        "        if not word_to_replace[-1].isalnum():\n",
        "            punctuation = word_to_replace[-1]\n",
        "            clean_word = word_to_replace[:-1]\n",
        "\n",
        "        synonym = random.choice(synonyms[clean_word.lower()])\n",
        "\n",
        "        for i, word in enumerate(new_words):\n",
        "            if word == word_to_replace:\n",
        "                new_words[i] = synonym + punctuation\n",
        "                break\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "synonym_dict = {\n",
        "    \"report\": [\"statement\", \"summary\", \"document\"],\n",
        "    \"significant\": [\"notable\", \"substantial\", \"major\"],\n",
        "    \"increase\": [\"growth\", \"rise\", \"expansion\"],\n",
        "    \"strong\": [\"robust\", \"powerful\", \"vigorous\"]\n",
        "}\n",
        "synonym_replaced_text = synonym_replacement(original_text, synonym_dict, n=2)\n",
        "augmented_samples[\"Synonym Replaced\"] = synonym_replaced_text\n",
        "\n",
        "# 4. --- Display All Augmented Samples ---\n",
        "print(\"\\n\\n==============================================\")\n",
        "print(\"      Data Augmentation Results             \")\n",
        "print(\"==============================================\")\n",
        "for augmentation_type, text in augmented_samples.items():\n",
        "    print(f\"\\n--- {augmentation_type} ---\")\n",
        "    print(f\"\\\"{text}\\\"\")\n",
        "print(\"\\n==============================================\")\n"
      ],
      "metadata": {
        "id": "QbxrXpKyAZui",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-31T08:08:51.765221Z",
          "iopub.execute_input": "2025-08-31T08:08:51.765833Z",
          "iopub.status.idle": "2025-08-31T08:09:02.051963Z",
          "shell.execute_reply.started": "2025-08-31T08:08:51.765807Z",
          "shell.execute_reply": "2025-08-31T08:09:02.051207Z"
        },
        "outputId": "7ba07326-d19f-4330-ce6e-ab4ce1bc4d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Setting up augmentation models...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:1\nDevice set to use cuda:1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Translation models loaded.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Augmenter LLM loaded.\n\nOriginal Text:\n\"The financial report indicates a significant increase in quarterly revenue due to strong sales in the technology sector.\"\n\n--- Applying LLM-Based Paraphrasing ---\n\n--- Applying Back-Translation (English -> German -> English) ---\n\n--- Applying Length Variations ---\n\n--- Applying Counterfactual Generation ---\n\n--- Applying Domain-Specific Style Transfer (to Legal) ---\n\n--- Applying Basic Transformation (Synonym Replacement) ---\n\n\n==============================================\n      Data Augmentation Results             \n==============================================\n\n--- Original ---\n\"The financial report indicates a significant increase in quarterly revenue due to strong sales in the technology sector.\"\n\n--- Paraphrased ---\n\"Le rapport financier indique une augmentation significative de la semaine passÃ©e en termes de revenu, grÃ¢ce Ã  une augmentation significative du marchÃ© des produits techniques.\"\n\n--- Back-Translated ---\n\"The financial report indicates a significant increase in quarterly sales due to strong sales in the technology sector.\"\n\n--- Summarized ---\n\"The financial report indicates a significant increase in quarterly revenue due to strong sales in the technology sector.\"\n\n--- Elaborated ---\n\"The financial report highlights that the company's quarterly revenue increased significantly in the recent period, indicating that the company's focus on the technology sector, specifically in the software and technology services industry, has paid off. The specific sector mentioned is the technology sector, which includes hardware, software, and services in the areas of computer technology, telecommunications, and information technology. The quarter\"\n\n--- Counterfactual ---\n\"The financial report suggests a significant decrease in quarterly revenue due to weak sales in the technology sector.\"\n\n--- Legal Style ---\n\"The financial report indicates that the Company's revenue for the quarter ending December 31, 2021, increased significantly due to strong sales in the Technology sector.\"\n\n--- Synonym Replaced ---\n\"The financial report indicates a notable increase in quarterly revenue due to robust sales in the technology sector.\"\n\n==============================================\n\n--- Quality Verification Step (Conceptual) ---\nThe generated variations would now be passed through a filtering pipeline (like in Procedure 3) to ensure quality before being added to the final dataset.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}