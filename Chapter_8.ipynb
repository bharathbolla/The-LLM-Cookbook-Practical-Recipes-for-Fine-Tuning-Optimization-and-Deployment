{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathbolla/The-LLM-Cookbook-Practical-Recipes-for-Fine-Tuning-Optimization-and-Deployment/blob/main/Chapter_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-1: Standard LoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "xF7JNVHVbO8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub import login\n",
        "\n",
        "api = HfApi()\n",
        "whoami = api.whoami(token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "print(whoami)\n",
        "login(\"hf_xxxxxxxxxxxxxxxx\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T10:23:22.719229Z",
          "iopub.execute_input": "2025-08-30T10:23:22.719633Z",
          "iopub.status.idle": "2025-08-30T10:23:23.476684Z",
          "shell.execute_reply.started": "2025-08-30T10:23:22.719605Z",
          "shell.execute_reply": "2025-08-30T10:23:23.475869Z"
        },
        "id": "kiA1OCXmh2oW",
        "outputId": "7e40d77e-6163-4b11-9b09-366b7ace94c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'type': 'user', 'id': '65feba1b57cc48d9d30d11cf', 'name': 'kalpasubbaiah', 'fullname': 'Kalpa Subbaiah', 'email': 'kalpa.subbaiah@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/319094e0eb55ce89334d7bd3685ceeb0.svg', 'orgs': [{'type': 'org', 'id': '681b0cb0dba891d54be0773d', 'name': 'mcp-course', 'fullname': 'Hugging Face MCP Course', 'email': None, 'canPay': False, 'periodEnd': None, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/itgTDqMrnvgNfJZJ4YmCt.png', 'roleInOrg': 'read', 'isEnterprise': False}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'hugging_face_token_read', 'role': 'read', 'createdAt': '2025-04-22T09:03:46.223Z'}}}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Standard LoRA Fine-Tuning ---\n",
        "# Goal: Fine-tune a base model using LoRA with the Hugging Face PEFT library.\n",
        "# Method: Adapts the IFT recipe (Ch 7) using Gemma-2B and Dolly subset.\n",
        "# Libraries: transformers, datasets, peft, torch, accelerate, bitsandbytes (optional, for bf16/fp16)\n",
        "# Note: Requires significant VRAM even without quantization for Gemma-2B base.\n",
        "#       Installs: pip install transformers datasets peft accelerate torch bitsandbytes sentencepiece\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_CHECKPOINT = \"google/gemma-2b\" # Base model\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\" # Instruction dataset\n",
        "OUTPUT_DIR = \"./lora_finetune_output\"\n",
        "# LoRA Config\n",
        "LORA_R = 16 # LoRA rank (e.g., 8, 16, 32, 64)\n",
        "LORA_ALPHA = 32 # LoRA alpha (scaling factor, often 2*r)\n",
        "LORA_DROPOUT =.05\n",
        "# Specify target modules for LoRA. Common practice for Gemma/Llama-like models:\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# Training Params\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 1 # Very small batch size for LoRA on Gemma-2B without QLoRA\n",
        "GRADIENT_ACCUMULATION_STEPS = 16 # Effective batch size 1*16=16\n",
        "LEARNING_RATE = 1e-4 # LoRA often uses higher LR than full FT\n",
        "MAX_LENGTH = 512 # Max sequence length\n",
        "NUM_SAMPLES_PER_DATASET = 500 # Use small subset for demo\n",
        "\n",
        "# --- 1. Load Tokenizer & Define Prompt Template (Same as IFT Chapter) ---\n",
        "print(f\"Loading tokenizer for checkpoint: {MODEL_CHECKPOINT}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Set PAD token to EOS token: {tokenizer.pad_token}\")\n",
        "\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T09:25:19.075980Z",
          "iopub.execute_input": "2025-08-30T09:25:19.076263Z",
          "iopub.status.idle": "2025-08-30T09:25:29.091341Z",
          "shell.execute_reply.started": "2025-08-30T09:25:19.076240Z",
          "shell.execute_reply": "2025-08-30T09:25:29.090685Z"
        },
        "id": "i9Ogfie8h2oX",
        "outputId": "61853d5b-04fb-4d5e-f3f0-5a5a0ffc1b2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-08-30 09:25:24.896833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756545924.920257     138 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756545924.927528     138 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading tokenizer for checkpoint: google/gemma-2b\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing Function (Same as IFT Chapter) ---\n",
        "def format_and_tokenize(example, dataset_type='dolly'): # Defaulting to Dolly structure\n",
        "    \"\"\"Formats (Alpaca style), tokenizes, and prepares labels with masking for IFT.\"\"\"\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_context = example.get(\"context\", \"\") # Dolly uses 'context'\n",
        "    output = example.get(\"response\", \"\")      # Dolly uses 'response'\n",
        "\n",
        "    if input_context and input_context.strip():\n",
        "        prompt_start = PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        prompt_start = PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "    full_text = prompt_start + output + tokenizer.eos_token\n",
        "    tokenized_full = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    tokenized_prompt = tokenizer(prompt_start, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    prompt_length = len(tokenized_prompt[\"input_ids\"])\n",
        "    labels = copy.deepcopy(tokenized_full[\"input_ids\"])\n",
        "\n",
        "    # Mask prompt tokens\n",
        "    for i in range(prompt_length):\n",
        "         if i < len(labels): labels[i] = -100\n",
        "\n",
        "    tokenized_full[\"labels\"] = labels\n",
        "    return tokenized_full\n",
        "\n",
        "print(f\"\\n--- Processing Dataset: {DATASET_NAME} ---\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(DATASET_NAME, split=f\"train[:{NUM_SAMPLES_PER_DATASET}]\")\n",
        "    tokenized_dataset = raw_dataset.map(\n",
        "        format_and_tokenize, remove_columns=raw_dataset.column_names\n",
        "    )\n",
        "    split_ds = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    final_datasets = {\"train\": split_ds[\"train\"], \"validation\": split_ds[\"test\"]}\n",
        "    print(\"Dataset processed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Load Base Model (Not Quantized for standard LoRA) ---\n",
        "print(f\"\\nLoading base model: {MODEL_CHECKPOINT}\")\n",
        "print(\"Loading in bf16/fp16 for memory efficiency...\")\n",
        "# Determine compute dtype\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_CHECKPOINT,\n",
        "        torch_dtype=compute_dtype, # Load in lower precision\n",
        "        device_map=\"auto\" # Distribute across GPUs if available\n",
        "    )\n",
        "    # Ensure pad token ID is set\n",
        "    if model.config.pad_token_id is None:\n",
        "         model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Optional: Prepare model for k-bit training if using gradient checkpointing with lower precision loading\n",
        "    # model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # Set use_gradient_checkpointing based on TrainingArgs\n",
        "\n",
        "    print(f\"Base model loaded in {compute_dtype}. Device map: {model.hf_device_map}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading base model: {e}\")\n",
        "    print(\"Standard LoRA on Gemma-2B might still require > 24GB VRAM. Consider QLoRA.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 4. Configure LoRA ---\n",
        "print(\"\\nConfiguring LoRA...\")\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\", # Typically 'none', 'all', or 'lora_only'\n",
        "    task_type=TaskType.CAUSAL_LM # Important for Causal LM tasks\n",
        ")\n",
        "\n",
        "# --- 5. Wrap Model with PEFT ---\n",
        "print(\"Applying PEFT LoRA adapters to the model...\")\n",
        "try:\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    print(\"PEFT model created successfully.\")\n",
        "    model.print_trainable_parameters() # Shows the small percentage of trainable parameters\n",
        "except Exception as e:\n",
        "    print(f\"Error applying PEFT: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 6. Training Arguments ---\n",
        "print(\"\\nDefining Training Arguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    # Optimizer choice can matter, AdamW 8bit is memory efficient if bitsandbytes installed\n",
        "    # optim=\"paged_adamw_8bit\", # Or adamw_torch\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    fp16= (compute_dtype == torch.float16), # Enable based on compute_dtype\n",
        "    bf16= (compute_dtype == torch.bfloat16), # Enable based on compute_dtype\n",
        "    gradient_checkpointing=True, # Often needed for larger models/LoRA\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# --- 7. Data Collator ---\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# --- 8. Initialize Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model, # Pass the PEFT model\n",
        "    args=training_args,\n",
        "    train_dataset=final_datasets[\"train\"],\n",
        "    eval_dataset=final_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# --- 9. Train ---\n",
        "print(\"\\nStarting LoRA fine-tuning...\")\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    # Note: PEFT saves only the adapter weights by default\n",
        "    trainer.save_model(OUTPUT_DIR) # Saves adapter config & weights to OUTPUT_DIR\n",
        "    print(f\"LoRA training finished. Adapter saved to {OUTPUT_DIR}\")\n",
        "    # Log metrics\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "except Exception as e:\n",
        "    print(f\"Error during LoRA training: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 10. Evaluate ---\n",
        "print(\"\\nEvaluating final LoRA model...\")\n",
        "try:\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation Results (Loss):\")\n",
        "    print(eval_results)\n",
        "    # perplexity = math.exp(eval_results['eval_loss'])\n",
        "    # print(f\"Perplexity: {perplexity:.2f}\")\n",
        "    trainer.log_metrics(\"eval\", eval_results)\n",
        "    trainer.save_metrics(\"eval\", eval_results)\n",
        "except Exception as e:\n",
        "    print(f\"Error during evaluation: {e}\")\n",
        "\n",
        "# --- End of Recipe ---"
      ],
      "metadata": {
        "id": "2OSKMLK_bIYA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T09:25:35.678489Z",
          "iopub.execute_input": "2025-08-30T09:25:35.679666Z",
          "iopub.status.idle": "2025-08-30T09:26:42.643113Z",
          "shell.execute_reply.started": "2025-08-30T09:25:35.679637Z",
          "shell.execute_reply": "2025-08-30T09:26:42.642270Z"
        },
        "colab": {
          "referenced_widgets": [
            "3040d1766f5b4513a744ec2ba752ce84",
            "e11772ca9b7149968758a11cbfeee5ba"
          ]
        },
        "outputId": "9acacf7e-398d-4ad2-d540-94878930a2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Processing Dataset: databricks/databricks-dolly-15k ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3040d1766f5b4513a744ec2ba752ce84"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset processed.\n\nLoading base model: google/gemma-2b\nLoading in bf16/fp16 for memory efficiency...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e11772ca9b7149968758a11cbfeee5ba"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Base model loaded in torch.bfloat16. Device map: {'model.embed_tokens': 0, 'lm_head': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.norm': 1, 'model.rotary_emb': 1}\n\nConfiguring LoRA...\nApplying PEFT LoRA adapters to the model...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_138/3386262005.py:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "PEFT model created successfully.\ntrainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765\n\nDefining Training Arguments...\n\nStarting LoRA fine-tuning...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error during LoRA training: element 0 of tensors does not require grad and does not have a grad_fn\n\nEvaluating final LoRA model...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='None' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      None\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>2.353364</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Evaluation Results (Loss):\n{'eval_loss': 2.353363513946533}\n***** eval metrics *****\n  eval_loss = 2.3534\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-2: QLoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "q15YH0GZbZmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T10:23:49.187404Z",
          "iopub.execute_input": "2025-08-30T10:23:49.188096Z",
          "iopub.status.idle": "2025-08-30T10:25:01.256207Z",
          "shell.execute_reply.started": "2025-08-30T10:23:49.188047Z",
          "shell.execute_reply": "2025-08-30T10:25:01.253987Z"
        },
        "id": "hIVvUVW5h2oY",
        "outputId": "c8d0c3ae-a8a5-4b4a-a096-9360c16e7e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting bitsandbytes\n  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.47.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: QLoRA on a Budget ---\n",
        "# Goal: Fine-tune a base model using QLoRA (4-bit quantization + LoRA).\n",
        "# Method: Uses Gemma-2B, Dolly subset, bitsandbytes for 4-bit loading, and peft.\n",
        "# Libraries: transformers, datasets, peft, accelerate, torch, bitsandbytes, sentencepiece\n",
        "# Note: Should fit in GPUs with >= 12-16GB VRAM. Install bitsandbytes: pip install bitsandbytes\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig # Needed for quantization config\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "# --- Configuration (Similar to LoRA recipe) ---\n",
        "MODEL_CHECKPOINT = \"google/gemma-2b\"\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR = \"./qlora_finetune_output\"\n",
        "# LoRA Config\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "# Training Params\n",
        "NUM_EPOCHS = 1\n",
        "# Can potentially use larger batch size with QLoRA vs standard LoRA\n",
        "BATCH_SIZE = 2 # Start small, increase if memory allows\n",
        "GRADIENT_ACCUMULATION_STEPS = 8 # Effective batch size 2*8=16\n",
        "LEARNING_RATE = 1e-4 # QLoRA might tolerate slightly higher LR sometimes\n",
        "MAX_LENGTH = 512\n",
        "NUM_SAMPLES_PER_DATASET = 500\n",
        "\n",
        "# --- 1. Load Tokenizer & Define Prompt Template (Same as LoRA recipe) ---\n",
        "print(f\"Loading tokenizer for checkpoint: {MODEL_CHECKPOINT}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing Function (Same as LoRA recipe) ---\n",
        "def format_and_tokenize(example, dataset_type='dolly'):\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_context = example.get(\"context\", \"\")\n",
        "    output = example.get(\"response\", \"\")\n",
        "    if input_context and input_context.strip():\n",
        "        prompt_start = PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        prompt_start = PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "    full_text = prompt_start + output + tokenizer.eos_token\n",
        "    tokenized_full = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    tokenized_prompt = tokenizer(prompt_start, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    prompt_length = len(tokenized_prompt[\"input_ids\"])\n",
        "    labels = copy.deepcopy(tokenized_full[\"input_ids\"])\n",
        "    for i in range(prompt_length):\n",
        "         if i < len(labels): labels[i] = -100\n",
        "    tokenized_full[\"labels\"] = labels\n",
        "    return tokenized_full\n",
        "\n",
        "print(f\"\\n--- Processing Dataset: {DATASET_NAME} ---\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(DATASET_NAME, split=f\"train[:{NUM_SAMPLES_PER_DATASET}]\")\n",
        "    tokenized_dataset = raw_dataset.map(\n",
        "        format_and_tokenize, remove_columns=raw_dataset.column_names\n",
        "    )\n",
        "    split_ds = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    final_datasets = {\"train\": split_ds[\"train\"], \"validation\": split_ds[\"test\"]}\n",
        "    print(\"Dataset processed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Configure Quantization (BitsAndBytes) ---\n",
        "print(\"\\nConfiguring 4-bit quantization...\")\n",
        "# Determine compute dtype\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "print(f\"Using compute dtype: {compute_dtype}\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # Use NF4 (NormalFloat4) data type for optimal results\n",
        "    bnb_4bit_compute_dtype=compute_dtype, # Computations done in bf16/fp16\n",
        "    bnb_4bit_use_double_quant=True, # Optional: Use double quantization for extra memory savings\n",
        ")\n",
        "\n",
        "# --- 4. Load Base Model in 4-bit ---\n",
        "print(f\"\\nLoading base model ({MODEL_CHECKPOINT}) in 4-bit...\")\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_CHECKPOINT,\n",
        "        quantization_config=bnb_config, # Apply quantization config\n",
        "        device_map={'': 0},              #  force all weights to cuda:0\n",
        "        torch_dtype=compute_dtype\n",
        "    )\n",
        "    # Ensure pad token ID is set\n",
        "    if model.config.pad_token_id is None:\n",
        "         model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    print(f\"Base model loaded in 4-bit. Device map: {model.hf_device_map}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading base model in 4-bit: {e}\")\n",
        "    print(\"Ensure 'bitsandbytes' is installed correctly for your CUDA version.\")\n",
        "    exit()\n",
        "\n",
        "# --- 5. Prepare Model for PEFT & Configure LoRA ---\n",
        "# Prepare for k-bit training must be called BEFORE applying PEFT config\n",
        "# Enable gradient checkpointing for more memory savings\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "print(\"Model prepared for k-bit training.\")\n",
        "\n",
        "print(\"\\nConfiguring LoRA...\")\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES, # Target modules might differ slightly per model, check docs\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# --- 6. Wrap Model with PEFT ---\n",
        "print(\"Applying PEFT LoRA adapters to the 4-bit model...\")\n",
        "try:\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    print(\"PEFT model created successfully.\")\n",
        "    model.print_trainable_parameters() # Shows the small percentage of trainable parameters\n",
        "except Exception as e:\n",
        "    print(f\"Error applying PEFT: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 7. Training Arguments ---\n",
        "# Use paged optimizer for more memory efficiency with QLoRA\n",
        "use_paged_optimizer = True # Requires bitsandbytes >= 0.41.1\n",
        "optim_choice = \"paged_adamw_8bit\" if use_paged_optimizer else \"adamw_torch\"\n",
        "print(f\"Using optimizer: {optim_choice}\")\n",
        "\n",
        "print(\"\\nDefining Training Arguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    optim=optim_choice, # Use paged optimizer\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    fp16= (compute_dtype == torch.float16), # Enable based on compute_dtype\n",
        "    bf16= (compute_dtype == torch.bfloat16), # Enable based on compute_dtype\n",
        "    gradient_checkpointing=True, # Crucial for QLoRA memory saving\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    per_device_eval_batch_size=1\n",
        ")\n",
        "\n",
        "# --- 8. Data Collator ---\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# --- 9. Initialize Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model, # Pass the QLoRA PEFT model\n",
        "    args=training_args,\n",
        "    train_dataset=final_datasets[\"train\"],\n",
        "    eval_dataset=final_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# --- 10. Train ---\n",
        "print(\"\\nStarting QLoRA fine-tuning...\")\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    trainer.save_model(OUTPUT_DIR) # Saves adapter config & weights\n",
        "    print(f\"QLoRA training finished. Adapter saved to {OUTPUT_DIR}\")\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "except Exception as e:\n",
        "    print(f\"Error during QLoRA training: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 11. Evaluate ---\n",
        "print(\"\\nEvaluating final QLoRA model...\")\n",
        "try:\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation Results (Loss):\")\n",
        "    print(eval_results)\n",
        "    trainer.log_metrics(\"eval\", eval_results)\n",
        "    trainer.save_metrics(\"eval\", eval_results)\n",
        "except Exception as e:\n",
        "    print(f\"Error during evaluation: {e}\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "id": "Kxqa3e4_baBb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T09:08:10.237564Z",
          "iopub.execute_input": "2025-08-30T09:08:10.237884Z",
          "execution_failed": "2025-08-30T09:23:01.758Z"
        },
        "colab": {
          "referenced_widgets": [
            "89d2a12fe5b94f63a5b031602ee42f85",
            "7fbf84b4f46a4695a7fe2410bc50992e",
            "4d01415081a245c08bb93f362ea44c75",
            "5e9c546c15b7440ba310d24b657f11d8",
            "a755545a3fc844b79c4021e1890ad78d",
            "4736eebdec45459e8a588dfc7f95bec8",
            "601bd29e669d4a2083ba355383f5b782",
            "56b2f8df2bb7456cb30a7daa008baea0",
            "c237ddd1833540b694b8b674ee25578e",
            "26f4918dc7ae4bb0af7f9c62ec87bbbd",
            "b2dc897ad0f0432e98db0cc3975c91d8",
            "c231ffaa56b345e7985daff794fc700e",
            "f1bb968d20d94d53a0fbdac868956d98",
            "27e7f75374034e9e821620c3b30c2450",
            "e2fe2b2bc76944c99bbae7185d0f435c"
          ]
        },
        "outputId": "3556ad4b-332d-4e3f-8525-cf38ee283a6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-08-30 09:08:22.972850: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756544903.148890      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756544903.200057      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading tokenizer for checkpoint: google/gemma-2b\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89d2a12fe5b94f63a5b031602ee42f85"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fbf84b4f46a4695a7fe2410bc50992e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d01415081a245c08bb93f362ea44c75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e9c546c15b7440ba310d24b657f11d8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--- Processing Dataset: databricks/databricks-dolly-15k ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a755545a3fc844b79c4021e1890ad78d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4736eebdec45459e8a588dfc7f95bec8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "601bd29e669d4a2083ba355383f5b782"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56b2f8df2bb7456cb30a7daa008baea0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset processed.\n\nConfiguring 4-bit quantization...\nUsing compute dtype: torch.bfloat16\n\nLoading base model (google/gemma-2b) in 4-bit...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c237ddd1833540b694b8b674ee25578e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26f4918dc7ae4bb0af7f9c62ec87bbbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2dc897ad0f0432e98db0cc3975c91d8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c231ffaa56b345e7985daff794fc700e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1bb968d20d94d53a0fbdac868956d98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27e7f75374034e9e821620c3b30c2450"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2fe2b2bc76944c99bbae7185d0f435c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Base model loaded in 4-bit. Device map: {'': 0}\nModel prepared for k-bit training.\n\nConfiguring LoRA...\nApplying PEFT LoRA adapters to the 4-bit model...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_31/3582303178.py:178: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "PEFT model created successfully.\ntrainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765\nUsing optimizer: paged_adamw_8bit\n\nDefining Training Arguments...\n\nStarting QLoRA fine-tuning...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='15' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14/14 12:49, Epoch 0.99/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-3: Accelerated PEFT with UnslothAI"
      ],
      "metadata": {
        "id": "cUODf6Spbpl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unsloth"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T09:43:05.926168Z",
          "iopub.execute_input": "2025-08-30T09:43:05.926477Z",
          "iopub.status.idle": "2025-08-30T09:47:30.626779Z",
          "shell.execute_reply.started": "2025-08-30T09:43:05.926449Z",
          "shell.execute_reply": "2025-08-30T09:47:30.625717Z"
        },
        "id": "IOae7CoCh2oa",
        "outputId": "0ebb8fc2-e240-4ec5-be9b-fbe891a9416b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting unsloth\n  Downloading unsloth-2025.8.10-py3-none-any.whl.metadata (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.8.9 (from unsloth)\n  Downloading unsloth_zoo-2025.8.9-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.5.1+cu124)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.47.0)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.30-py3-none-any.whl.metadata (11 kB)\nCollecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 (from unsloth)\n  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.5.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.3.0)\nCollecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n  Downloading trl-0.22.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nCollecting huggingface_hub>=0.34.0 (from unsloth)\n  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.11.16)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.13.1)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.34.0->unsloth)\n  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (2024.11.6)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth)\n  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting accelerate>=0.34.1 (from unsloth)\n  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.8.9->unsloth) (0.10.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.8.9->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.8.9->unsloth) (11.1.0)\nCollecting msgspec (from unsloth_zoo>=2025.8.9->unsloth)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\nCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting nvidia-nccl-cu12==2.27.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.1.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (14.0.0)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<4.0.0,>=3.4.1->unsloth) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<4.0.0,>=3.4.1->unsloth) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<4.0.0,>=3.4.1->unsloth) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<4.0.0,>=3.4.1->unsloth) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<4.0.0,>=3.4.1->unsloth) (1.19.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\nDownloading unsloth-2025.8.10-py3-none-any.whl (312 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading trl-0.22.1-py3-none-any.whl (544 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.8.9-py3-none-any.whl (196 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.30-py3-none-any.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, hf-xet, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface_hub, tyro, tokenizers, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, accelerate, trl, xformers, unsloth_zoo, torchvision, unsloth\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.5.147\n    Uninstalling nvidia-curand-cu12-10.3.5.147:\n      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.30.2\n    Uninstalling huggingface-hub-0.30.2:\n      Successfully uninstalled huggingface-hub-0.30.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.3.0\n    Uninstalling accelerate-1.3.0:\n      Successfully uninstalled accelerate-1.3.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.8.0 which is incompatible.\ntorchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.8.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.10.1 cut_cross_entropy-25.1.1 fsspec-2024.12.0 hf-xet-1.1.9 huggingface_hub-0.34.4 msgspec-0.19.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 shtab-1.7.2 sympy-1.14.0 tokenizers-0.22.0 torch-2.8.0 torchvision-0.23.0 transformers-4.56.0 triton-3.4.0 trl-0.22.1 tyro-0.9.30 unsloth-2025.8.10 unsloth_zoo-2025.8.9 xformers-0.0.32.post2\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Turbocharged PEFT with UnslothAI ---\n",
        "# Goal: Demonstrate accelerated LoRA/QLoRA fine-tuning using UnslothAI.\n",
        "# Method: Adapts the QLoRA recipe showing minimal code changes for Unsloth.\n",
        "# Libraries: unsloth, torch, datasets, peft, transformers, sentencepiece\n",
        "# Note: Requires UnslothAI installation specific to your environment (GPU/PyTorch/CUDA).\n",
        "#       Example install: pip install \"unsloth[pytorch-ampere-torch210]\" --extra-index-url https://pypi.unsloth.ai\n",
        "#       Check UnslothAI GitHub for correct install command.\n",
        "import unsloth\n",
        "import torch\n",
        "import copy\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# --- UnslothAI Import ---\n",
        "from unsloth import FastLanguageModel # Main import\n",
        "from unsloth import is_bfloat16_supported\n",
        "from peft import LoraConfig, TaskType # PEFT imports remain similar\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig # Still used for quantization config if desired\n",
        ")\n",
        "\n",
        "\n",
        "# --- Configuration (Similar to QLoRA recipe) ---\n",
        "MODEL_CHECKPOINT = \"google/gemma-2b\" # Unsloth supports many models\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR = \"./unsloth_qlora_finetune_output\"\n",
        "# LoRA Config (Same as before)\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "# Target modules might be automatically inferred by Unsloth for some models,\n",
        "# but specifying is often still good practice or required. Check Unsloth docs.\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "# Training Params\n",
        "NUM_EPOCHS = 1\n",
        "# Unsloth's memory savings might allow larger batch sizes vs standard QLoRA\n",
        "BATCH_SIZE = 4 # Try increasing this (e.g., 8, 16) if memory allows!\n",
        "GRADIENT_ACCUMULATION_STEPS = 4 # Adjust so BATCH_SIZE * GRAD_ACCUM is your target effective batch size\n",
        "LEARNING_RATE = 1e-4\n",
        "MAX_LENGTH = 512\n",
        "NUM_SAMPLES_PER_DATASET = 500\n",
        "\n",
        "# --- 1. Load Tokenizer & Define Prompt Template (Same as LoRA/QLoRA recipe) ---\n",
        "print(f\"Loading tokenizer for checkpoint: {MODEL_CHECKPOINT}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing Function (Same as LoRA/QLoRA recipe) ---\n",
        "def format_and_tokenize(example, dataset_type='dolly'):\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_context = example.get(\"context\", \"\")\n",
        "    output = example.get(\"response\", \"\")\n",
        "    if input_context and input_context.strip():\n",
        "        prompt_start = PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        prompt_start = PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "    full_text = prompt_start + output + tokenizer.eos_token\n",
        "    tokenized_full = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    tokenized_prompt = tokenizer(prompt_start, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    prompt_length = len(tokenized_prompt[\"input_ids\"])\n",
        "    labels = copy.deepcopy(tokenized_full[\"input_ids\"])\n",
        "    for i in range(prompt_length):\n",
        "         if i < len(labels): labels[i] = -100\n",
        "    tokenized_full[\"labels\"] = labels\n",
        "    return tokenized_full\n",
        "\n",
        "print(f\"\\n--- Processing Dataset: {DATASET_NAME} ---\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(DATASET_NAME, split=f\"train[:{NUM_SAMPLES_PER_DATASET}]\")\n",
        "    tokenized_dataset = raw_dataset.map(\n",
        "        format_and_tokenize, remove_columns=raw_dataset.column_names\n",
        "    )\n",
        "    split_ds = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    final_datasets = {\"train\": split_ds[\"train\"], \"validation\": split_ds[\"test\"]}\n",
        "    print(\"Dataset processed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Load Model with UnslothAI ---\n",
        "# Unsloth handles quantization and optimizations internally during loading.\n",
        "print(f\"\\nLoading model ({MODEL_CHECKPOINT}) with UnslothAI...\")\n",
        "# Determine max_seq_length based on data preprocessing\n",
        "max_seq_length = MAX_LENGTH\n",
        "# Determine dtype (\"auto\", None, torch.float16, torch.bfloat16)\n",
        "dtype = None # Autodetect\n",
        "load_in_4bit = True # Enable QLoRA optimizations\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = MODEL_CHECKPOINT,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        # token = \"hf_...\", # Add token if using gated models like Llama\n",
        "    )\n",
        "    print(\"Unsloth FastLanguageModel loaded successfully.\")\n",
        "     # Ensure pad token ID is set correctly after potential Unsloth modifications\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Set PAD token to EOS token: {tokenizer.pad_token}\")\n",
        "    # Unsloth might handle model's pad_token_id internally, but check if needed\n",
        "    # if model.config.pad_token_id is None: model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model with UnslothAI: {e}\")\n",
        "    print(\"Ensure UnslothAI is installed correctly for your environment.\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. Apply PEFT (LoRA) Configuration ---\n",
        "# Unsloth integrates with PEFT. Model is already prepared.\n",
        "print(\"\\nApplying PEFT LoRA adapters using Unsloth's optimized method...\")\n",
        "try:\n",
        "    # Use FastLanguageModel.get_peft_model instead of standard get_peft_model\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model, # Pass the Unsloth model object\n",
        "        r = LORA_R,\n",
        "        target_modules = LORA_TARGET_MODULES,\n",
        "        lora_alpha = LORA_ALPHA,\n",
        "        lora_dropout = LORA_DROPOUT,\n",
        "        bias = \"none\", # Or \"all\" or \"lora_only\"\n",
        "        use_gradient_checkpointing = True, # Recommended by Unsloth\n",
        "        random_state = 3407, # From Unsloth examples\n",
        "        max_seq_length = max_seq_length, # Optional but good practice\n",
        "        use_rslora = False,  # Rank Stable LoRA (experimental)\n",
        "        loftq_config = None, # LoftQ configuration (experimental)\n",
        "    )\n",
        "    print(\"Unsloth PEFT model created successfully.\")\n",
        "    # Unsloth model object might not have print_trainable_parameters, but PEFT is applied.\n",
        "except Exception as e:\n",
        "    print(f\"Error applying PEFT with Unsloth: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 5. Training Arguments ---\n",
        "# Arguments are largely the same as standard Trainer\n",
        "print(\"\\nDefining Training Arguments...\")\n",
        "bf16_supported = is_bfloat16_supported() # Use unsloth's check\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    # Unsloth works with standard optimizers, paged optimizers often still good\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    fp16=not bf16_supported, # Use fp16 if bf16 not available\n",
        "    bf16=bf16_supported, # Use bf16 if available\n",
        "    # gradient_checkpointing is handled by Unsloth's get_peft_model\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# --- 6. Data Collator ---\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# --- 7. Initialize Trainer ---\n",
        "# Use standard Hugging Face Trainer\n",
        "trainer = Trainer(\n",
        "    model=model, # Pass the Unsloth PEFT model\n",
        "    args=training_args,\n",
        "    train_dataset=final_datasets[\"train\"],\n",
        "    eval_dataset=final_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# --- 8. Train ---\n",
        "print(\"\\nStarting Unsloth accelerated fine-tuning...\")\n",
        "# Expect faster training and potentially lower memory usage compared to standard QLoRA\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    # Unsloth saves adapters in a compatible format\n",
        "    # Use trainer.save_model() which internally calls model.save_pretrained()\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    print(f\"Unsloth training finished. Adapter saved to {OUTPUT_DIR}\")\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "except Exception as e:\n",
        "    print(f\"Error during Unsloth training: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 9. Evaluate ---\n",
        "print(\"\\nEvaluating final Unsloth PEFT model...\")\n",
        "try:\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation Results (Loss):\")\n",
        "    print(eval_results)\n",
        "    trainer.log_metrics(\"eval\", eval_results)\n",
        "    trainer.save_metrics(\"eval\", eval_results)\n",
        "except Exception as e:\n",
        "    print(f\"Error during evaluation: {e}\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "id": "vZwt8usNbp2D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T09:50:31.668816Z",
          "iopub.execute_input": "2025-08-30T09:50:31.669675Z",
          "iopub.status.idle": "2025-08-30T09:56:19.676833Z",
          "shell.execute_reply.started": "2025-08-30T09:50:31.669644Z",
          "shell.execute_reply": "2025-08-30T09:56:19.676116Z"
        },
        "colab": {
          "referenced_widgets": [
            "9a486a2d5a6c48ddb2983b7838b3e2ae",
            "3314efc5d2e54b20a145d70b244f6c6b",
            "41efd30e7cf74a91b2c7313172a369ed",
            "969e7657483e407eabbc74dde7523bca",
            "23630996ed5745469177a9b5f6779556",
            "af1ba3cad2ca4827b8e5792e8589508e",
            "6c9f8cdc70194ab6a18d138dc15d55f5",
            "d41a272d25a44aa1b68c1422989203e6",
            "fd6bee61d452417c9d2a88f79c9b9c32",
            "70c2905b2af449d3906750abf13b59b1",
            "1ead7c3906e045a3acf60a47e0848754",
            "8ec46b1d8b30452da32963edfe029788",
            "10fbe5a78c5b4ecfb464f176284dfd3e",
            "876c0757f41e4fdeb554c1d6eabb0a3d",
            "61bfd820662f4169b0670f18e7e62c58",
            "d0734c37377540c9b48c072b3d8de7dd",
            "d843562a636f463fa032c804a61e8fce",
            "f5aacf13e6204c4ea466adc1422df339"
          ]
        },
        "outputId": "2593c155-0203-4c2a-e075-dde46e55340e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2025-08-30 09:50:41.025774: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756547441.380821      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756547441.486828      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "🦥 Unsloth Zoo will now patch everything to make training faster!\nLoading tokenizer for checkpoint: google/gemma-2b\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a486a2d5a6c48ddb2983b7838b3e2ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3314efc5d2e54b20a145d70b244f6c6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41efd30e7cf74a91b2c7313172a369ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "969e7657483e407eabbc74dde7523bca"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--- Processing Dataset: databricks/databricks-dolly-15k ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23630996ed5745469177a9b5f6779556"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af1ba3cad2ca4827b8e5792e8589508e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c9f8cdc70194ab6a18d138dc15d55f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d41a272d25a44aa1b68c1422989203e6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset processed.\n\nLoading model (google/gemma-2b) with UnslothAI...\n==((====))==  Unsloth 2025.8.10: Fast Gemma patching. Transformers: 4.56.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd6bee61d452417c9d2a88f79c9b9c32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70c2905b2af449d3906750abf13b59b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ead7c3906e045a3acf60a47e0848754"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ec46b1d8b30452da32963edfe029788"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10fbe5a78c5b4ecfb464f176284dfd3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "876c0757f41e4fdeb554c1d6eabb0a3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61bfd820662f4169b0670f18e7e62c58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0734c37377540c9b48c072b3d8de7dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d843562a636f463fa032c804a61e8fce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5aacf13e6204c4ea466adc1422df339"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Unsloth FastLanguageModel loaded successfully.\n\nApplying PEFT LoRA adapters using Unsloth's optimized method...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Unsloth 2025.8.10 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Unsloth PEFT model created successfully.\n\nDefining Training Arguments...\n\nStarting Unsloth accelerated fine-tuning...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_31/1401954764.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 450 | Num Epochs = 1 | Total steps = 15\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n \"-____-\"     Trainable parameters = 19,611,648 of 2,525,784,064 (0.78% trained)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Unsloth: Will smartly offload gradients to save VRAM!\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 03:33, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.590900</td>\n      <td>2.349695</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Unsloth: Not an error, but GemmaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Unsloth training finished. Adapter saved to ./unsloth_qlora_finetune_output\n***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =  2576846GF\n  train_loss               =     2.5511\n  train_runtime            = 0:03:54.48\n  train_samples_per_second =      1.919\n  train_steps_per_second   =      0.064\n\nEvaluating final Unsloth PEFT model...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:08]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Evaluation Results (Loss):\n{'eval_loss': 2.3496947288513184, 'eval_runtime': 10.5971, 'eval_samples_per_second': 4.718, 'eval_steps_per_second': 0.377, 'epoch': 1.0}\n***** eval metrics *****\n  epoch                   =        1.0\n  eval_loss               =     2.3497\n  eval_runtime            = 0:00:10.59\n  eval_samples_per_second =      4.718\n  eval_steps_per_second   =      0.377\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Below recipes are not part of the book"
      ],
      "metadata": {
        "id": "751u1rQzjDnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe: Merging LoRA Adapters\n",
        "### This recipe not needed"
      ],
      "metadata": {
        "id": "aWL88iAccDqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Baking the Adapters In (Merging LoRA Adapters) ---\n",
        "# Goal: Merge trained LoRA adapter weights into the base model for deployment.\n",
        "# Method: Loads base model and adapter, then uses peft's merge_and_unload().\n",
        "# Libraries: transformers, peft, torch, bitsandbytes (if merging into quantized)\n",
        "# Note: Ensure the adapter was trained for the specified base model.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel # Import PeftModel for loading adapters\n",
        "\n",
        "# --- Configuration ---\n",
        "# Base model checkpoint MUST match the one used during PEFT training\n",
        "BASE_MODEL_CHECKPOINT = \"google/gemma-2b\"\n",
        "# Path to the saved PEFT adapter weights (e.g., from LoRA, QLoRA, or Unsloth recipe)\n",
        "ADAPTER_PATH = \"./qlora_finetune_output\" # Example: Use QLoRA output\n",
        "# Path to save the merged model\n",
        "MERGED_MODEL_OUTPUT_DIR = \"./merged_qlora_model\"\n",
        "\n",
        "# --- Options for Merging ---\n",
        "# 1. Merge into original precision model (requires loading base model in fp16/bf16/fp32)\n",
        "#    - Result: Standard HF model, no PEFT needed at inference. Larger size.\n",
        "# 2. Merge into quantized model (e.g., 4-bit QLoRA merged into 4-bit base)\n",
        "#    - Result: Quantized model with adapter baked in. Still requires bitsandbytes at inference. Smaller size.\n",
        "MERGE_INTO_QUANTIZED = True # Set to False to merge into original precision\n",
        "\n",
        "print(f\"Base Model: {BASE_MODEL_CHECKPOINT}\")\n",
        "print(f\"Adapter Path: {ADAPTER_PATH}\")\n",
        "print(f\"Merge into Quantized: {MERGE_INTO_QUANTIZED}\")\n",
        "\n",
        "# --- 1. Load Base Model ---\n",
        "print(\"\\nLoading base model...\")\n",
        "try:\n",
        "    if MERGE_INTO_QUANTIZED:\n",
        "        print(\"Loading base model in 4-bit for merging...\")\n",
        "        compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "        )\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            BASE_MODEL_CHECKPOINT,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map={'':torch.cuda.current_device()}, # Load on GPU if possible\n",
        "            trust_remote_code=True # If needed for model\n",
        "        )\n",
        "        print(\"Base model loaded in 4-bit.\")\n",
        "    else:\n",
        "        print(\"Loading base model in native precision (fp16/bf16)...\")\n",
        "        compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            BASE_MODEL_CHECKPOINT,\n",
        "            torch_dtype=compute_dtype,\n",
        "            device_map={'':torch.cuda.current_device()},\n",
        "            trust_remote_code=True # If needed\n",
        "        )\n",
        "        print(f\"Base model loaded in {compute_dtype}.\")\n",
        "\n",
        "    # Load tokenizer associated with the base model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT)\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "    if base_model.config.pad_token_id is None: base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading base model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load PEFT Adapter ---\n",
        "print(f\"\\nLoading PEFT adapter from: {ADAPTER_PATH}\")\n",
        "try:\n",
        "    # Load the PEFT model - this attaches the adapter weights to the base model\n",
        "    # Ensure the base_model is already loaded on the correct device(s) via device_map\n",
        "    model_with_adapter = PeftModel.from_pretrained(\n",
        "        base_model, # Pass the loaded base model\n",
        "        ADAPTER_PATH,\n",
        "        is_trainable=False # Load for inference/merging\n",
        "    )\n",
        "    print(\"PEFT adapter loaded onto the base model.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading PEFT adapter: {e}\")\n",
        "    print(\"Ensure the adapter path is correct and compatible with the base model.\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Merge Adapter Weights ---\n",
        "print(\"\\nMerging adapter weights into the base model...\")\n",
        "try:\n",
        "    # merge_and_unload() merges weights and removes PEFT layers/hooks\n",
        "    # If merging into quantized, the result is still quantized but with updated weights\n",
        "    merged_model = model_with_adapter.merge_and_unload()\n",
        "    print(\"Adapter merged successfully.\")\n",
        "    # merged_model is now a standard Hugging Face model (potentially quantized)\n",
        "    # It no longer requires the `peft` library for inference.\n",
        "    # If MERGE_INTO_QUANTIZED=True, it still requires `bitsandbytes`.\n",
        "except Exception as e:\n",
        "    print(f\"Error merging adapter: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. Save Merged Model ---\n",
        "print(f\"\\nSaving merged model to: {MERGED_MODEL_OUTPUT_DIR}\")\n",
        "try:\n",
        "    merged_model.save_pretrained(MERGED_MODEL_OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(MERGED_MODEL_OUTPUT_DIR) # Save tokenizer with merged model\n",
        "    print(\"Merged model and tokenizer saved.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving merged model: {e}\")\n",
        "\n",
        "# --- 5. Test Merged Model (Optional) ---\n",
        "print(\"\\nTesting merged model (optional)...\")\n",
        "try:\n",
        "    prompt = \"Instruction: Say hello.\\nResponse:\" # Example prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(merged_model.device)\n",
        "    outputs = merged_model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Merged Model Response: {response}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during merged model inference test: {e}\")\n",
        "\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "id": "AScZ-RKRcD6D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T10:22:01.607024Z",
          "iopub.execute_input": "2025-08-30T10:22:01.607925Z",
          "iopub.status.idle": "2025-08-30T10:22:10.604660Z",
          "shell.execute_reply.started": "2025-08-30T10:22:01.607893Z",
          "shell.execute_reply": "2025-08-30T10:22:10.604030Z"
        },
        "colab": {
          "referenced_widgets": [
            "21b3299350ab47d093bf669bc836ecb7"
          ]
        },
        "outputId": "824763b6-70eb-413b-aac8-2a2d5568aa70"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Base Model: google/gemma-2b\nAdapter Path: ./qlora_finetune_output\nMerge into Quantized: True\n\nLoading base model...\nLoading base model in 4-bit for merging...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21b3299350ab47d093bf669bc836ecb7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Base model loaded in 4-bit.\n\nLoading PEFT adapter from: ./qlora_finetune_output\nError loading PEFT adapter: Can't find 'adapter_config.json' at './qlora_finetune_output'\nEnsure the adapter path is correct and compatible with the base model.\n\nMerging adapter weights into the base model...\nError merging adapter: name 'model_with_adapter' is not defined\n\nSaving merged model to: ./merged_qlora_model\nError saving merged model: name 'merged_model' is not defined\n\nTesting merged model (optional)...\nError during merged model inference test: name 'merged_model' is not defined\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe: LoRA vs. Full Fine-Tuning Showdown"
      ],
      "metadata": {
        "id": "ScwF1KedcapB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T10:26:41.278106Z",
          "iopub.execute_input": "2025-08-30T10:26:41.278823Z",
          "iopub.status.idle": "2025-08-30T10:26:44.992858Z",
          "shell.execute_reply.started": "2025-08-30T10:26:41.278798Z",
          "shell.execute_reply": "2025-08-30T10:26:44.991810Z"
        },
        "id": "7VXPpNRMh2od",
        "outputId": "4f17077b-49bb-4e25-dedc-115587233ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2024.12.0\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: LoRA vs. Full Fine-Tuning Showdown ---\n",
        "# Goal: Compare performance metrics of a LoRA/QLoRA model vs. a fully fine-tuned model.\n",
        "# Method: Loads saved models from previous recipes (Full FT and LoRA/QLoRA)\n",
        "#         and evaluates them on the same test set.\n",
        "# Note: Assumes you have run and saved models from:\n",
        "#       - A Full FT recipe (e.g., IFT on Gemma-2B, Ch 7 equivalent but full FT)\n",
        "#       - A LoRA/QLoRA recipe (e.g., ch8_recipe_qlora_ft)\n",
        "#       Both MUST use the same base model and task/dataset for valid comparison.\n",
        "\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM, # Or specific task head like SequenceClassification\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding, # Or specific collator\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel # Needed to load LoRA adapter\n",
        "import os\n",
        "import math\n",
        "\n",
        "# --- Configuration ---\n",
        "# --- !!! UPDATE THESE PATHS !!! ---\n",
        "# Path to the saved FULLY Fine-Tuned model (e.g., from Ch 6/7 equivalent)\n",
        "FULL_FT_MODEL_PATH = \"./instruction_finetune_output_FULL_FT\" # NEEDS TO BE CREATED SEPARATELY\n",
        "# Path to the saved PEFT (LoRA/QLoRA) adapter\n",
        "PEFT_ADAPTER_PATH = \"./qlora_finetune_output\" # From ch8_recipe_qlora_ft\n",
        "# Base model checkpoint - MUST be the same for both models being compared\n",
        "BASE_MODEL_CHECKPOINT = \"google/gemma-2b\"\n",
        "# Dataset and task details - MUST be the same used for training both models\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "TASK_TYPE = \"IFT\" # Indicate task ('IFT', 'Classification', 'NER', etc.)\n",
        "NUM_SAMPLES_FOR_EVAL = 200 # Use a subset of test data for quicker evaluation demo\n",
        "BATCH_SIZE = 4 # Evaluation batch size (adjust based on memory)\n",
        "\n",
        "# --- Check if models exist ---\n",
        "if not os.path.exists(FULL_FT_MODEL_PATH):\n",
        "    print(f\"Error: Fully Fine-Tuned model not found at {FULL_FT_MODEL_PATH}\")\n",
        "    print(\"Please run a full fine-tuning script first and update the path.\")\n",
        "    exit()\n",
        "if not os.path.exists(PEFT_ADAPTER_PATH):\n",
        "    print(f\"Error: PEFT adapter not found at {PEFT_ADAPTER_PATH}\")\n",
        "    print(\"Please run the LoRA/QLoRA fine-tuning recipe first and update the path.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Load Test Data and Tokenizer ---\n",
        "print(f\"Loading dataset: {DATASET_NAME}\")\n",
        "try:\n",
        "    # Load the split used for testing during training runs\n",
        "    # Assuming a validation split was used, or load 'test' if available/appropriate\n",
        "    # For Dolly, there's no predefined test split, so we sample from train\n",
        "    raw_dataset_full = load_dataset(DATASET_NAME, split=\"train\")\n",
        "    # Create a deterministic test split from the full data\n",
        "    # IMPORTANT: Ensure this split was NOT used during training of EITHER model\n",
        "    test_dataset_raw = raw_dataset_full.select(range(NUM_SAMPLES_PER_EVAL)) # Simple subset for demo\n",
        "    print(f\"Using {len(test_dataset_raw)} samples for evaluation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nLoading tokenizer: {BASE_MODEL_CHECKPOINT}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT)\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Preprocess Test Data ---\n",
        "# Use the *exact same* preprocessing as during training\n",
        "# Reusing IFT preprocessing from previous recipes\n",
        "MAX_LENGTH = 512 # Must match training\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")\n",
        "def format_and_tokenize_ift(example):\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_context = example.get(\"context\", \"\")\n",
        "    output = example.get(\"response\", \"\")\n",
        "    if input_context and input_context.strip():\n",
        "        prompt_start = PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        prompt_start = PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "    # For evaluation, we often don't need the full text + EOS, just the prompt part\n",
        "    # However, for loss calculation (perplexity), we need labels. Tokenize full text.\n",
        "    full_text = prompt_start + output + tokenizer.eos_token\n",
        "    tokenized_full = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    tokenized_prompt = tokenizer(prompt_start, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    prompt_length = len(tokenized_prompt[\"input_ids\"])\n",
        "    labels = copy.deepcopy(tokenized_full[\"input_ids\"])\n",
        "    for i in range(prompt_length):\n",
        "         if i < len(labels): labels[i] = -100\n",
        "    tokenized_full[\"labels\"] = labels\n",
        "    return tokenized_full\n",
        "\n",
        "print(\"\\nTokenizing evaluation dataset...\")\n",
        "try:\n",
        "    if TASK_TYPE == \"IFT\":\n",
        "         tokenized_test_dataset = test_dataset_raw.map(\n",
        "             format_and_tokenize_ift, remove_columns=test_dataset_raw.column_names\n",
        "         )\n",
        "    # Add elif blocks here for preprocessing specific to other tasks (Classification, NER)\n",
        "    # elif TASK_TYPE == \"Classification\": ...\n",
        "    else:\n",
        "         raise ValueError(f\"Preprocessing for TASK_TYPE='{TASK_TYPE}' not implemented in this recipe.\")\n",
        "    tokenized_test_dataset.set_format(\"torch\")\n",
        "    print(\"Tokenization complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during tokenization: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Setup Collator, Metrics, Eval Args ---\n",
        "if TASK_TYPE == \"IFT\":\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "    metric_to_report = \"eval_loss\" # Use loss for IFT/Causal LM\n",
        "    metric_for_best = \"loss\"\n",
        "    compute_metrics_fn = None # Trainer calculates loss by default\n",
        "# Add elif blocks for other task collators and metrics\n",
        "# elif TASK_TYPE == \"Classification\": ...\n",
        "else:\n",
        "     raise ValueError(f\"Collator/Metrics for TASK_TYPE='{TASK_TYPE}' not implemented.\")\n",
        "\n",
        "# Minimal args for evaluation\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir=\"./eval_comparison_output\", # Temp dir\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    do_train=False, do_eval=True, report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
        ")\n",
        "\n",
        "# --- 4. Evaluate FULLY Fine-Tuned Model ---\n",
        "print(f\"\\n--- Evaluating FULLY Fine-Tuned Model ({FULL_FT_MODEL_PATH}) ---\")\n",
        "full_ft_results = {\"eval_loss\": \"Error\"} # Default\n",
        "try:\n",
        "    # Load the saved full model\n",
        "    # Adjust dtype/device_map as needed based on how it was saved/resource limits\n",
        "    compute_dtype = torch.bfloat16 if eval_args.bf16 else (torch.float16 if eval_args.fp16 else torch.float32)\n",
        "    full_ft_model = AutoModelForCausalLM.from_pretrained(\n",
        "        FULL_FT_MODEL_PATH,\n",
        "        torch_dtype=compute_dtype,\n",
        "        device_map={'':torch.cuda.current_device()}\n",
        "    )\n",
        "    print(\"Fully Fine-Tuned model loaded.\")\n",
        "\n",
        "    full_trainer = Trainer(\n",
        "        model=full_ft_model, args=eval_args, eval_dataset=tokenized_test_dataset,\n",
        "        tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics_fn\n",
        "    )\n",
        "    print(\"Running evaluation on Fully Fine-Tuned model...\")\n",
        "    full_ft_results = full_trainer.evaluate()\n",
        "    print(\"\\nFully Fine-Tuned Model Evaluation Results:\")\n",
        "    print(full_ft_results)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Fully Fine-Tuned model evaluation: {e}\")\n",
        "    if 'full_ft_model' in locals(): del full_ft_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 5. Evaluate PEFT (LoRA/QLoRA) Model ---\n",
        "print(f\"\\n--- Evaluating PEFT Model ({PEFT_ADAPTER_PATH}) ---\")\n",
        "peft_results = {\"eval_loss\": \"Error\"} # Default\n",
        "try:\n",
        "    # Load base model (quantized if PEFT was QLoRA)\n",
        "    is_qlora = \"qlora\" in PEFT_ADAPTER_PATH.lower() # Simple check based on path name\n",
        "    bnb_config = None\n",
        "    if is_qlora:\n",
        "        print(\"Loading base model in 4-bit for QLoRA evaluation...\")\n",
        "        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "                                        bnb_4bit_compute_dtype=compute_dtype)\n",
        "\n",
        "    base_model_peft = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_CHECKPOINT,\n",
        "        quantization_config=bnb_config, # None if not QLoRA\n",
        "        torch_dtype=compute_dtype if not is_qlora else None, # Load native if not quantized\n",
        "        device_map={'':torch.cuda.current_device()}\n",
        "    )\n",
        "    if base_model_peft.config.pad_token_id is None: base_model_peft.config.pad_token_id = tokenizer.pad_token_id\n",
        "    print(\"Base model loaded.\")\n",
        "\n",
        "    # Load the PEFT adapter onto the base model\n",
        "    peft_model = PeftModel.from_pretrained(base_model_peft, PEFT_ADAPTER_PATH)\n",
        "    print(\"PEFT adapter loaded.\")\n",
        "\n",
        "    # Note: For evaluation, merging might be slightly faster if memory allows\n",
        "    # peft_model = peft_model.merge_and_unload()\n",
        "    # print(\"PEFT adapter merged for evaluation.\")\n",
        "\n",
        "    peft_trainer = Trainer(\n",
        "        model=peft_model, args=eval_args, eval_dataset=tokenized_test_dataset,\n",
        "        tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics_fn\n",
        "    )\n",
        "    print(\"Running evaluation on PEFT model...\")\n",
        "    peft_results = peft_trainer.evaluate()\n",
        "    print(\"\\nPEFT Model Evaluation Results:\")\n",
        "    print(peft_results)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during PEFT model evaluation: {e}\")\n",
        "    if 'base_model_peft' in locals(): del base_model_peft\n",
        "    if 'peft_model' in locals(): del peft_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 6. Comparison ---\n",
        "print(\"\\n--- Performance Comparison ---\")\n",
        "# Compare based on the primary metric (loss for IFT)\n",
        "full_metric = full_ft_results.get(metric_to_report, \"N/A\")\n",
        "peft_metric = peft_results.get(metric_to_report, \"N/A\")\n",
        "\n",
        "print(f\"Task Type: {TASK_TYPE}\")\n",
        "print(f\"Metric Compared: {metric_to_report}\")\n",
        "print(f\"Fully Fine-Tuned Model: {full_metric:.4f}\" if isinstance(full_metric, float) else full_metric)\n",
        "print(f\"PEFT (LoRA/QLoRA) Model: {peft_metric:.4f}\" if isinstance(peft_metric, float) else peft_metric)\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Compare the metric values. Lower loss is better for IFT.\")\n",
        "print(\"- Full FT *might* achieve slightly better peak performance if resources allow.\")\n",
        "print(\"- PEFT achieves comparable or slightly lower performance with drastically fewer trainable parameters and lower resource usage during training.\")\n",
        "print(\"- Consider the trade-off: peak performance vs. training/deployment efficiency.\")\n",
        "print(\"\\nResource Usage Comparison (Typical Expectations):\")\n",
        "print(\"- Training VRAM: Full FT >> LoRA > QLoRA / Unsloth\")\n",
        "print(\"- Training Time: Full FT > LoRA / QLoRA (Unsloth often fastest)\")\n",
        "print(\"- Saved Artifact Size: Full FT (Full Model) >> LoRA / QLoRA (Adapter only)\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "164fb00f7d1e48c18e2412c9f3cd1523",
            "bbee290257fd4d8b88985d441313adb7",
            "98eec3c0104e44fba2283903ef507f45",
            "e05f0a4d01cb497d854fcfea003a5056",
            "7d5d97928e21444cafffdfa2cc6f3eaa",
            "d768c10547a044e98bc7e445cdd4a9bc",
            "feb328c4b65c465c8e36f569aaaeeafd",
            "f7050ec625a144efb612bad34270e5ab",
            "0eaadbfd9106448d98917dc27a8fa49c",
            "7220d831d8ef4f6eb5e76c7e31fcec18",
            "dc85867fc32344f28210708615f07a5d",
            "8b30b52a1c58481b9c313902b825ac01",
            "62b1144c54e34bddb1cf1d8c1be4150f",
            "9f5995820505431bb7a0775d9ce582b6"
          ]
        },
        "id": "ToYnG8Iuca2x",
        "outputId": "d26087e8-1c2e-4fd2-cbed-fdec7dc7961f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-30T10:27:21.165345Z",
          "iopub.execute_input": "2025-08-30T10:27:21.165645Z",
          "iopub.status.idle": "2025-08-30T10:28:15.670309Z",
          "shell.execute_reply.started": "2025-08-30T10:27:21.165623Z",
          "shell.execute_reply": "2025-08-30T10:28:15.669591Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-08-30 10:27:26.627583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756549646.806144      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756549646.861172      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Error: Fully Fine-Tuned model not found at ./instruction_finetune_output_FULL_FT\nPlease run a full fine-tuning script first and update the path.\nError: PEFT adapter not found at ./qlora_finetune_output\nPlease run the LoRA/QLoRA fine-tuning recipe first and update the path.\nLoading dataset: databricks/databricks-dolly-15k\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "164fb00f7d1e48c18e2412c9f3cd1523"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbee290257fd4d8b88985d441313adb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98eec3c0104e44fba2283903ef507f45"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Error loading dataset: name 'NUM_SAMPLES_PER_EVAL' is not defined\n\nLoading tokenizer: google/gemma-2b\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e05f0a4d01cb497d854fcfea003a5056"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d5d97928e21444cafffdfa2cc6f3eaa"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d768c10547a044e98bc7e445cdd4a9bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "feb328c4b65c465c8e36f569aaaeeafd"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nTokenizing evaluation dataset...\nError during tokenization: name 'test_dataset_raw' is not defined\n\n--- Evaluating FULLY Fine-Tuned Model (./instruction_finetune_output_FULL_FT) ---\nError during Fully Fine-Tuned model evaluation: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './instruction_finetune_output_FULL_FT'.\n\n--- Evaluating PEFT Model (./qlora_finetune_output) ---\nLoading base model in 4-bit for QLoRA evaluation...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7050ec625a144efb612bad34270e5ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eaadbfd9106448d98917dc27a8fa49c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7220d831d8ef4f6eb5e76c7e31fcec18"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc85867fc32344f28210708615f07a5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b30b52a1c58481b9c313902b825ac01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62b1144c54e34bddb1cf1d8c1be4150f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f5995820505431bb7a0775d9ce582b6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Base model loaded.\nError during PEFT model evaluation: Can't find 'adapter_config.json' at './qlora_finetune_output'\n\n--- Performance Comparison ---\nTask Type: IFT\nMetric Compared: eval_loss\nError\nError\n\nObservations:\n- Compare the metric values. Lower loss is better for IFT.\n- Full FT *might* achieve slightly better peak performance if resources allow.\n- PEFT achieves comparable or slightly lower performance with drastically fewer trainable parameters and lower resource usage during training.\n- Consider the trade-off: peak performance vs. training/deployment efficiency.\n\nResource Usage Comparison (Typical Expectations):\n- Training VRAM: Full FT >> LoRA > QLoRA / Unsloth\n- Training Time: Full FT > LoRA / QLoRA (Unsloth often fastest)\n- Saved Artifact Size: Full FT (Full Model) >> LoRA / QLoRA (Adapter only)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkydqmuFcgp6"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}