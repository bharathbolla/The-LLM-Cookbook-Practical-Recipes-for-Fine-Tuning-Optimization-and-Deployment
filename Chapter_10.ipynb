{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathbolla/The-LLM-Cookbook-Practical-Recipes-for-Fine-Tuning-Optimization-and-Deployment/blob/main/Chapter_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "kGJmBAuUZoAo"
      },
      "source": [
        "## Recipe: Effortless Shrinking with `bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VjwmaPaoAl_",
        "outputId": "eb353dea-111d-43f4-8565-329697d06bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'type': 'user', 'id': '65feba1b57cc48d9d30d11cf', 'name': 'kalpasubbaiah', 'fullname': 'Kalpa Subbaiah', 'email': 'kalpa.subbaiah@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/319094e0eb55ce89334d7bd3685ceeb0.svg', 'orgs': [{'type': 'org', 'id': '681b0cb0dba891d54be0773d', 'name': 'mcp-course', 'fullname': 'Hugging Face MCP Course', 'email': None, 'canPay': False, 'periodEnd': None, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/itgTDqMrnvgNfJZJ4YmCt.png', 'roleInOrg': 'read', 'isEnterprise': False}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'hugging_face_token_read', 'role': 'read', 'createdAt': '2025-08-31T13:41:46.429Z'}}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub import login\n",
        "\n",
        "api = HfApi()\n",
        "whoami = api.whoami(token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "print(whoami)\n",
        "login(\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-1:Effortless Quantization with bitsandbytes"
      ],
      "metadata": {
        "id": "pcHczVHez_oW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": false,
        "execution": {
          "iopub.execute_input": "2025-08-31T13:31:10.495632Z",
          "iopub.status.busy": "2025-08-31T13:31:10.494956Z",
          "iopub.status.idle": "2025-08-31T13:31:13.544240Z",
          "shell.execute_reply": "2025-08-31T13:31:13.543389Z",
          "shell.execute_reply.started": "2025-08-31T13:31:10.495608Z"
        },
        "id": "OCpU4I-LiLEa",
        "outputId": "9f392420-522b-4d60-d9a9-64c539c839f8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dutI7moH3gz7"
      },
      "source": [
        "## Recipe-1: Effortless Quantization with bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ee9bb757fcfe489a86aacd1f6a58c0bd",
            "3d5ed4c79c3345c5a7dc4cda961a7e15",
            "3ef2eb59ad764cd18c33f6ca77167363",
            "ebce55b3e0cf43eea23789c4a3b6731b",
            "e63104a188fa46e0aafe0c3cda169d16",
            "2c9be19d8eb24bfab3e15e3e742dcfa0",
            "3375d8b9971e44dd981dadc4c66d3198",
            "5b9884b2317540158ba20281772d8779",
            "5cd1fa05c1a54a20a4d76db0c6b9f1ae",
            "0bc1a7020440496db5eb68d324893e04",
            "8041c6b3a633478fa0036aae4be6170b",
            "d28766fa01e2454d9fb3ca4a469f4308",
            "c6bad588c7b54719bc0cb885bd906193"
          ]
        },
        "editable": false,
        "execution": {
          "iopub.execute_input": "2025-08-31T09:22:49.692468Z",
          "iopub.status.busy": "2025-08-31T09:22:49.692251Z",
          "iopub.status.idle": "2025-08-31T09:24:26.695683Z",
          "shell.execute_reply": "2025-08-31T09:24:26.695015Z",
          "shell.execute_reply.started": "2025-08-31T09:22:49.692449Z"
        },
        "id": "PSvxcdWKYf8c",
        "outputId": "14991e19-be5a-4213-b134-6e74c9f57c9e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer for: google/gemma-2b\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee9bb757fcfe489a86aacd1f6a58c0bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d5ed4c79c3345c5a7dc4cda961a7e15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ef2eb59ad764cd18c33f6ca77167363",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebce55b3e0cf43eea23789c4a3b6731b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading Model in Native Precision (BF16/FP16) ---\n",
            "Using compute dtype: torch.bfloat16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e63104a188fa46e0aafe0c3cda169d16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-31 09:23:09.338127: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756632189.670346      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756632189.770573      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c9be19d8eb24bfab3e15e3e742dcfa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3375d8b9971e44dd981dadc4c66d3198",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b9884b2317540158ba20281772d8779",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cd1fa05c1a54a20a4d76db0c6b9f1ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc1a7020440496db5eb68d324893e04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8041c6b3a633478fa0036aae4be6170b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Native model loaded.\n",
            "Native Model Memory Footprint: 4.67 GB\n",
            "\n",
            "--- Loading Model in 8-bit ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d28766fa01e2454d9fb3ca4a469f4308",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8-bit model loaded.\n",
            "8-bit Model Memory Footprint: 2.82 GB\n",
            "Reduction vs Native: 39.5%\n",
            "\n",
            "--- Loading Model in 4-bit (NF4) ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6bad588c7b54719bc0cb885bd906193",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4-bit model loaded.\n",
            "4-bit Model Memory Footprint: 1.90 GB\n",
            "Reduction vs Native: 59.3%\n",
            "\n",
            "--- Running Inference (8-bit) ---\n",
            "Prompt: Instruction: Write a short description of quantization.\n",
            "Response:\n",
            "Response:\n",
            "Instruction: Write a short description of quantization.\n",
            "Response:\n",
            "Quantization is the process of converting a continuous signal into a discrete signal. The process of quantization is done by dividing the continuous signal into a number of discrete values. The process of quantization is done by dividing the continuous signal into a number of discrete\n",
            "Inference Time: 6.58 seconds\n",
            "\n",
            "--- Running Inference (4-bit) ---\n",
            "Prompt: Instruction: Write a short description of quantization.\n",
            "Response:\n",
            "Response:\n",
            "Instruction: Write a short description of quantization.\n",
            "Response:\n",
            "Quantization is the process of converting a continuous-time signal into a discrete-time signal. The quantization process is a linear operation. The quantization process is a linear operation. The quantization process is a linear operation. The quantization process is a linear\n",
            "Inference Time: 2.92 seconds\n",
            "\n",
            "Note: Memory footprint is approximate. Inference time depends heavily on hardware.\n"
          ]
        }
      ],
      "source": [
        "# --- Recipe: Effortless Shrinking with `bitsandbytes` ---\n",
        "# Goal: Load a pre-trained model using 8-bit and 4-bit quantization via transformers + bitsandbytes.\n",
        "# Libraries: transformers, torch, accelerate, bitsandbytes, sentencepiece\n",
        "# Note: Requires `bitsandbytes` installation. `accelerate` needed for `device_map`.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import time # For basic timing\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"google/gemma-2b\" # Choose a model\n",
        "# MODEL_ID = \"mistralai/Mistral-7B-v0.1\" # Larger model to see more significant memory savings\n",
        "\n",
        "# --- 1. Load Tokenizer ---\n",
        "print(f\"Loading tokenizer for: {MODEL_ID}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load Model in Native Precision (Reference) ---\n",
        "print(\"\\n--- Loading Model in Native Precision (BF16/FP16) ---\")\n",
        "# Determine compute dtype\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "print(f\"Using compute dtype: {compute_dtype}\")\n",
        "try:\n",
        "    model_native = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=compute_dtype,\n",
        "        device_map=\"auto\" # Use GPU if available\n",
        "    )\n",
        "    print(\"Native model loaded.\")\n",
        "    mem_footprint_native = model_native.get_memory_footprint()\n",
        "    print(f\"Native Model Memory Footprint: {mem_footprint_native / 1024**3:.2f} GB\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading native model: {e}\")\n",
        "    model_native = None # Ensure variable exists\n",
        "\n",
        "# --- 3. Load Model in 8-bit ---\n",
        "print(\"\\n--- Loading Model in 8-bit ---\")\n",
        "try:\n",
        "    bnb_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config_8bit,\n",
        "        device_map=\"auto\" # device_map handles quantized models too\n",
        "    )\n",
        "    print(\"8-bit model loaded.\")\n",
        "    mem_footprint_8bit = model_8bit.get_memory_footprint()\n",
        "    print(f\"8-bit Model Memory Footprint: {mem_footprint_8bit / 1024**3:.2f} GB\")\n",
        "    if model_native:\n",
        "         print(f\"Reduction vs Native: {(1 - mem_footprint_8bit / mem_footprint_native) * 100:.1f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading 8-bit model: {e}\")\n",
        "    print(\"Ensure 'bitsandbytes' is installed correctly.\")\n",
        "    model_8bit = None\n",
        "\n",
        "# --- 4. Load Model in 4-bit (NF4) ---\n",
        "print(\"\\n--- Loading Model in 4-bit (NF4) ---\")\n",
        "try:\n",
        "    bnb_config_4bit = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\", # NormalFloat4 data type\n",
        "        bnb_4bit_compute_dtype=compute_dtype, # Compute in bf16/fp16\n",
        "        bnb_4bit_use_double_quant=True, # Enable double quantization\n",
        "    )\n",
        "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config_4bit,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(\"4-bit model loaded.\")\n",
        "    mem_footprint_4bit = model_4bit.get_memory_footprint()\n",
        "    print(f\"4-bit Model Memory Footprint: {mem_footprint_4bit / 1024**3:.2f} GB\")\n",
        "    if model_native:\n",
        "        print(f\"Reduction vs Native: {(1 - mem_footprint_4bit / mem_footprint_native) * 100:.1f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading 4-bit model: {e}\")\n",
        "    print(\"Ensure 'bitsandbytes' is installed correctly.\")\n",
        "    model_4bit = None\n",
        "\n",
        "# --- 5. Test Inference (Optional) ---\n",
        "# Run generation to see if models work after loading\n",
        "prompt = \"Instruction: Write a short description of quantization.\\nResponse:\"\n",
        "max_new_tokens_inf = 50\n",
        "\n",
        "def run_inference(model, model_name):\n",
        "    if model is None:\n",
        "        print(f\"\\nSkipping inference for {model_name} (not loaded).\")\n",
        "        return\n",
        "    print(f\"\\n--- Running Inference ({model_name}) ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        start_time = time.time()\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens_inf,\n",
        "            pad_token_id=tokenizer.eos_token_id # Use EOS token ID for padding in generation\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"Response:\\n{response}\")\n",
        "        print(f\"Inference Time: {end_time - start_time:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during {model_name} inference: {e}\")\n",
        "\n",
        "# Run inference on loaded models\n",
        "# run_inference(model_native, \"Native Precision\") # Can be slow\n",
        "run_inference(model_8bit, \"8-bit\")\n",
        "run_inference(model_4bit, \"4-bit\")\n",
        "\n",
        "print(\"\\nNote: Memory footprint is approximate. Inference time depends heavily on hardware.\")\n",
        "# --- End of Recipe ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "zt-PWJEyZxm8"
      },
      "source": [
        "\n",
        "##  Recipe: Quantizing with AutoGPTQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7C3VHxk0FCs"
      },
      "source": [
        "## Recipe-2: Quantizing with AutoGPTQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "tm25UcXD0Fo3"
      },
      "source": [
        "\n",
        "##  Recipe: Quantizing with AutoGPTQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": false,
        "execution": {
          "iopub.execute_input": "2025-08-31T12:25:23.863475Z",
          "iopub.status.busy": "2025-08-31T12:25:23.862952Z",
          "iopub.status.idle": "2025-08-31T12:26:52.703258Z",
          "shell.execute_reply": "2025-08-31T12:26:52.702222Z",
          "shell.execute_reply.started": "2025-08-31T12:25:23.863454Z"
        },
        "id": "00zSoHLUiLEc",
        "outputId": "fd259ee8-86aa-449e-991c-7a80bd82ace9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.7.1.tar.gz (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.1/126.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 174, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 162, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "                       ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 53, in _iter_built\n",
            "    candidate = func()\n",
            "                ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 185, in _make_candidate_from_link\n",
            "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 231, in _make_base_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "                                       ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 303, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
            "    self.dist = self._prepare()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 314, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 527, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 642, in _prepare_linked_requirement\n",
            "    dist = _get_prepared_distribution(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 72, in _get_prepared_distribution\n",
            "    abstract_dist.prepare_distribution_metadata(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/distributions/sdist.py\", line 69, in prepare_distribution_metadata\n",
            "    self.req.prepare_metadata()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/req/req_install.py\", line 580, in prepare_metadata\n",
            "    self.metadata_directory = generate_metadata_legacy(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/build/metadata_legacy.py\", line 64, in generate_metadata\n",
            "    call_subprocess(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1586, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1682, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1651, in makeRecord\n",
            "    rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 298, in __init__\n",
            "    def __init__(self, name, level, pathname, lineno,\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "pip install auto-gptq optimum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "editable": false,
        "execution": {
          "iopub.execute_input": "2025-08-31T12:26:52.704682Z",
          "iopub.status.busy": "2025-08-31T12:26:52.704422Z",
          "iopub.status.idle": "2025-08-31T12:30:09.555685Z",
          "shell.execute_reply": "2025-08-31T12:30:09.554836Z",
          "shell.execute_reply.started": "2025-08-31T12:26:52.704659Z"
        },
        "id": "DXpcwbpxZx6x",
        "outputId": "98c3ebcd-ca83-4009-c2d1-f62cfeaf41c1",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'auto_gptq'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2488333007.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextGenerationPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mauto_gptq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoGPTQForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseQuantizeConfig\u001b[0m \u001b[0;31m# AutoGPTQ specific imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'auto_gptq'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# --- Recipe: Precise Compression with GPTQ ---\n",
        "# Goal: Quantize a pre-trained model using the AutoGPTQ library.\n",
        "# Libraries: transformers, torch, optimum, datasets, auto-gptq\n",
        "# Note: Requires installing AutoGPTQ: pip install auto-gptq optimum\n",
        "#       Requires a GPU compatible with AutoGPTQ kernels (usually NVIDIA).\n",
        "#       Uses gpt2-medium as an example and C4 dataset for calibration.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
        "from datasets import load_dataset\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig # AutoGPTQ specific imports\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# --- Configuration ---\n",
        "# Choose a base model supported by AutoGPTQ (check their GitHub)\n",
        "# Smaller models quantize faster. gpt2 variants are common examples.\n",
        "MODEL_CHECKPOINT = \"gpt2-medium\" # ~355M parameters\n",
        "# Calibration dataset - needs to be representative of text the model will see\n",
        "CALIBRATION_DATASET = \"allenai/c4\"\n",
        "CALIBRATION_SPLIT = \"train\" # Use train split\n",
        "NUM_CALIBRATION_SAMPLES = 128 # Number of samples for calibration (e.g., 128)\n",
        "CALIBRATION_SEQ_LEN = 512 # Sequence length for calibration data\n",
        "# GPTQ Quantization Config\n",
        "QUANTIZE_BITS = 4 # Target bits (e.g., 4, 3, 8)\n",
        "QUANTIZE_GROUP_SIZE = 128 # Group size for quantization (e.g., 32, 64, 128, -1 for per-channel)\n",
        "QUANTIZE_DESC_ACT = False # Or True - Whether to quantize using act_order=True, can improve accuracy but slower\n",
        "# Output directory for quantized model\n",
        "QUANTIZED_MODEL_DIR = f\"./{MODEL_CHECKPOINT.split('/')[-1]}-gptq-{QUANTIZE_BITS}bit\"\n",
        "\n",
        "# Setup logging for AutoGPTQ\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "# --- 1. Load Tokenizer and Calibration Data ---\n",
        "print(f\"Loading tokenizer: {MODEL_CHECKPOINT}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "except Exception as e: print(f\"Error loading tokenizer: {e}\"); exit()\n",
        "\n",
        "print(f\"\\nLoading calibration data: {CALIBRATION_DATASET} (subset)\")\n",
        "try:\n",
        "    # Load calibration data (streaming recommended for large datasets like C4)\n",
        "    calibration_dataset = load_dataset(CALIBRATION_DATASET, name=\"en\", split=CALIBRATION_SPLIT, streaming=True)\n",
        "    # Take a sample and tokenize\n",
        "    samples = []\n",
        "    for data in calibration_dataset.take(NUM_CALIBRATION_SAMPLES):\n",
        "        # Tokenize, ensuring padding/truncation to fixed length for calibration\n",
        "        tokenized_sample = tokenizer(data['text'], return_tensors='pt', max_length=CALIBRATION_SEQ_LEN, padding='max_length', truncation=True)\n",
        "        samples.append({\n",
        "        \"input_ids\": tokenized_sample[\"input_ids\"].squeeze(0),          # LongTensor [SEQ]\n",
        "        \"attention_mask\": tokenized_sample[\"attention_mask\"].squeeze(0) # LongTensor [SEQ]\n",
        "        })\n",
        "        # Alternative: provide list of strings directly to quantize method if supported by backend\n",
        "        # samples.append(data['text'])\n",
        "    if not samples: raise ValueError(\"No calibration samples loaded.\")\n",
        "    print(f\"Loaded {len(samples)} calibration samples.\")\n",
        "    # If using input_ids, stack them if needed by quantize method, otherwise keep as list\n",
        "    # calibration_data_final = torch.stack(samples)\n",
        "\n",
        "    # AutoGPTQ often expects a list of strings or dicts\n",
        "    #calibration_data_final = [tokenizer.decode(s, skip_special_tokens=True) for s in samples]\n",
        "    calibration_data_final = samples\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or processing calibration data: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load Base Model ---\n",
        "print(f\"\\nLoading base model: {MODEL_CHECKPOINT}\")\n",
        "try:\n",
        "    # Load in native precision on CPU first maybe, or directly to GPU if memory allows\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "    # model.to('cuda:0') # Move to GPU if not done automatically\n",
        "    print(\"Base model loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading base model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Define Quantization Config ---\n",
        "print(\"\\nDefining GPTQ quantization config...\")\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=QUANTIZE_BITS, # Number of bits for quantization\n",
        "    group_size=QUANTIZE_GROUP_SIZE, # Group size\n",
        "    desc_act=QUANTIZE_DESC_ACT, # Activation order; True might improve accuracy, False is faster\n",
        "    damp_percent=0.01, # Dampening percentage for Hessian computation\n",
        "    sym=True # Use symmetric quantization\n",
        ")\n",
        "\n",
        "# --- 4. Quantize Model ---\n",
        "print(\"\\nStarting GPTQ quantization process...\")\n",
        "print(f\"Bits: {QUANTIZE_BITS}, Group Size: {QUANTIZE_GROUP_SIZE}, Desc Act: {QUANTIZE_DESC_ACT}\")\n",
        "print(\"This can take a while...\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    # Wrap model with AutoGPTQ wrapper\n",
        "    quantized_model_gptq = AutoGPTQForCausalLM.from_pretrained(\n",
        "        MODEL_CHECKPOINT,\n",
        "        quantize_config=quantize_config, # Pass the config\n",
        "        # Optional: Pass model directly if already loaded\n",
        "        # model=model, # Pass the pre-loaded model object\n",
        "        torch_dtype=torch.float16, # Ensure consistency\n",
        "        trust_remote_code=True, # Often needed\n",
        "        device_map=\"auto\" # Let AutoGPTQ handle device placement\n",
        "    )\n",
        "\n",
        "    # Run the quantization process\n",
        "    quantized_model_gptq.quantize(\n",
        "        calibration_data_final, # Pass the prepared calibration data\n",
        "        batch_size=1, # Calibration batch size\n",
        "        use_triton=torch.cuda.is_available(), # Use Triton kernels if available (faster)\n",
        "        # cache_examples_on_gpu=True # If VRAM allows\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    print(f\"Quantization finished in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # --- 5. Save Quantized Model ---\n",
        "    print(f\"\\nSaving quantized model to: {QUANTIZED_MODEL_DIR}\")\n",
        "    # Use export_quantized=True argument or specific save methods depending on AutoGPTQ version\n",
        "    # Option 1: Standard save_pretrained (might work for newer versions)\n",
        "    #quantized_model_gptq.save_pretrained(QUANTIZED_MODEL_DIR, safe_serialization=True)\n",
        "\n",
        "    # Option 2: Use export_quantized (check AutoGPTQ docs for current best practice)\n",
        "    # Example for older versions might differ\n",
        "    quantized_model_gptq.save_pretrained(QUANTIZED_MODEL_DIR, use_safetensors=True)\n",
        "\n",
        "    tokenizer.save_pretrained(QUANTIZED_MODEL_DIR) # Save tokenizer too\n",
        "    print(\"Quantized model and tokenizer saved.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during GPTQ quantization or saving: {e}\")\n",
        "    print(\"Ensure AutoGPTQ and its dependencies (like optimum) are installed.\")\n",
        "    exit()\n",
        "\n",
        "# --- 6. Load and Test Quantized Model (Optional) ---\n",
        "print(\"\\nLoading and testing quantized model...\")\n",
        "try:\n",
        "    # Load the quantized model using the AutoGPTQ class\n",
        "    # Important: Ensure the environment loading the model has AutoGPTQ installed\n",
        "    model_loaded_gptq = AutoGPTQForCausalLM.from_quantized(\n",
        "        QUANTIZED_MODEL_DIR,\n",
        "        device_map=\"auto\", # Load onto GPU\n",
        "        use_triton=torch.cuda.is_available(),\n",
        "        trust_remote_code=True,\n",
        "        # inject_fused_attention=True, # Optional: Speed up inference\n",
        "        # inject_fused_mlp=True # Optional: Speed up inference\n",
        "    )\n",
        "    print(\"Quantized model loaded successfully.\")\n",
        "\n",
        "    # Test inference\n",
        "    prompt = \"Quantization in deep learning is\"\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    # Use pipeline for easy generation\n",
        "    #pipeline_gptq = TextGenerationPipeline(model=model_loaded_gptq, tokenizer=tokenizer, device=model_loaded_gptq.device)\n",
        "    pipeline_gptq = TextGenerationPipeline(model=model_loaded_gptq, tokenizer=tokenizer)\n",
        "    start_time = time.time()\n",
        "    outputs = pipeline_gptq(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
        "    end_time = time.time()\n",
        "    print(f\"Generated Text:\\n{outputs[0]['generated_text']}\")\n",
        "    print(f\"Inference Time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or testing quantized model: {e}\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": false,
        "execution": {
          "iopub.execute_input": "2025-08-31T12:30:29.756890Z",
          "iopub.status.busy": "2025-08-31T12:30:29.756578Z",
          "iopub.status.idle": "2025-08-31T12:30:36.086811Z",
          "shell.execute_reply": "2025-08-31T12:30:36.085795Z",
          "shell.execute_reply.started": "2025-08-31T12:30:29.756863Z"
        },
        "id": "Hm4OA0t5iLEc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pip install autoawq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p0QwZbK3w2T"
      },
      "source": [
        "## Recipe-3: Activation-Aware Quantization (AWQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": false,
        "execution": {
          "execution_failed": "2025-08-31T12:39:25.027Z",
          "iopub.execute_input": "2025-08-31T12:30:40.096497Z",
          "iopub.status.busy": "2025-08-31T12:30:40.096130Z"
        },
        "id": "hO3bvZgKPRg7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# --- Procedure 3: Activation-Aware Quantization (AWQ) ---\n",
        "# Goal: Quantize a pre-trained model using the Activation-aware Weight Quantization (AWQ) method.\n",
        "# Libraries: transformers, torch, optimum, datasets, autoawq\n",
        "# Note: Requires installing AutoAWQ: pip install autoawq\n",
        "#       Requires a GPU and is often faster than GPTQ for the same model size.\n",
        "#       Uses gpt2-medium as an example and C4 dataset for calibration.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from awq import AutoAWQForCausalLM\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# --- Configuration ---\n",
        "# Choose a base model. AWQ works well with many modern architectures.\n",
        "MODEL_CHECKPOINT = \"gpt2-medium\" # ~355M parameters\n",
        "# Calibration dataset - a small, representative sample of text.\n",
        "CALIBRATION_DATASET = \"allenai/c4\"\n",
        "CALIBRATION_SPLIT = \"train\"\n",
        "NUM_CALIBRATION_SAMPLES = 128\n",
        "# AWQ Quantization Config\n",
        "QUANTIZE_BITS = 4\n",
        "QUANTIZE_GROUP_SIZE = 128\n",
        "# Output directory for the quantized model\n",
        "QUANTIZED_MODEL_DIR = f\"./{MODEL_CHECKPOINT.split('/')[-1]}-awq-{QUANTIZE_BITS}bit\"\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "# --- 1. Load Tokenizer and Calibration Data ---\n",
        "print(f\"Loading tokenizer: {MODEL_CHECKPOINT}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\"); exit()\n",
        "\n",
        "print(f\"\\nLoading calibration data: {CALIBRATION_DATASET} (subset)\")\n",
        "try:\n",
        "    # Load a small subset of the data for calibration\n",
        "    calibration_dataset = load_dataset(CALIBRATION_DATASET, name=\"en\", split=f\"{CALIBRATION_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
        "    # AWQ expects a list of strings\n",
        "    calibration_data_final = [example['text'] for example in calibration_dataset]\n",
        "    print(f\"Loaded {len(calibration_data_final)} calibration samples.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or processing calibration data: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load Base Model ---\n",
        "# AWQ handles loading the model internally, so we just need the path.\n",
        "print(f\"\\nPreparing to load base model for AWQ: {MODEL_CHECKPOINT}\")\n",
        "\n",
        "# --- 3. Define Quantization Config ---\n",
        "# For AWQ, the configuration is passed directly to the quantize method.\n",
        "# The core parameters are the number of bits and the group size.\n",
        "awq_config = {\n",
        "    \"w_bit\": QUANTIZE_BITS,\n",
        "    \"q_group_size\": QUANTIZE_GROUP_SIZE,\n",
        "    \"zero_point\": True # Use a zero point for better accuracy\n",
        "}\n",
        "print(f\"\\nDefined AWQ config: {awq_config}\")\n",
        "\n",
        "# --- 4. Quantize Model ---\n",
        "print(\"\\nStarting AWQ quantization process...\")\n",
        "print(\"This involves loading the model, analyzing activations, and quantizing weights.\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    # Load the model and quantize it in one step\n",
        "    model = AutoAWQForCausalLM.from_pretrained(MODEL_CHECKPOINT, low_cpu_mem_usage=True, device_map=\"auto\")\n",
        "\n",
        "    # Run the quantization process\n",
        "    model.quantize(\n",
        "        tokenizer,\n",
        "        quant_config=awq_config,\n",
        "        calo_data=calibration_data_final\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    print(f\"Quantization finished in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # --- 5. Save Quantized Model ---\n",
        "    print(f\"\\nSaving quantized model to: {QUANTIZED_MODEL_DIR}\")\n",
        "    # The `save_quantized` method saves the model in a format that can be loaded for fast inference.\n",
        "    model.save_quantized(QUANTIZED_MODEL_DIR)\n",
        "    tokenizer.save_pretrained(QUANTIZED_MODEL_DIR)\n",
        "    print(\"Quantized model and tokenizer saved.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during AWQ quantization or saving: {e}\")\n",
        "    print(\"Ensure AutoAWQ is installed correctly.\")\n",
        "    exit()\n",
        "\n",
        "# --- 6. Load and Test Quantized Model (Optional) ---\n",
        "print(\"\\nLoading and testing quantized model...\")\n",
        "try:\n",
        "    # Load the quantized model using the AutoAWQ class again\n",
        "    model_quantized = AutoAWQForCausalLM.from_quantized(QUANTIZED_MODEL_DIR, device_map=\"auto\")\n",
        "    print(\"Quantized model loaded successfully.\")\n",
        "\n",
        "    # Test inference\n",
        "    prompt = \"Activation-aware Weight Quantization is a technique that\"\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    # Use the model directly for generation\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    start_time = time.time()\n",
        "    outputs = model_quantized.generate(**tokens, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
        "    end_time = time.time()\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Generated Text:\\n{generated_text}\")\n",
        "    print(f\"Inference Time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or testing quantized model: {e}\")\n",
        "\n",
        "# --- End of Recipe ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtm8rxHn3-Ab"
      },
      "source": [
        "## Recipe-4: Measuring Quantization Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": false,
        "id": "rqdmbJrKaapj"
      },
      "source": [
        "## Recipe: Measuring Quantization Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "28afb61cf5d64bae9d6381a5d78ad8f9",
            "a881e76cf0004823b82b18d68b81257d",
            "d98aef03f9c5438b8b7d95cdffc26d34",
            "e76fd78549234544b70bfe84726de3d8",
            "70249c329e3a4a07823405b507704ddf",
            "63355a7231e9488db38b97449b8d868c",
            "dbc117543cf54c8b99bba0a742d9dacd",
            "14d6774a604f49c886ede1ab2e619387",
            "b17f94acb9704dc0a0d2c45fa1f16902",
            "3a57704b78c141fd9d6ff5f824d93800",
            "42b7d1d25e1d4dda96b279a146a13014",
            "5bbb037ac87d459691da191a1c2014bc"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-08-31T13:31:34.165121Z",
          "iopub.status.busy": "2025-08-31T13:31:34.164323Z",
          "iopub.status.idle": "2025-08-31T13:32:58.149944Z",
          "shell.execute_reply": "2025-08-31T13:32:58.149170Z",
          "shell.execute_reply.started": "2025-08-31T13:31:34.165092Z"
        },
        "id": "7lT1nI2_iLEd",
        "outputId": "f295c7a6-6cc2-43a7-e544-3d7b04203cd1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer for: google/gemma-2b\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28afb61cf5d64bae9d6381a5d78ad8f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a881e76cf0004823b82b18d68b81257d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d98aef03f9c5438b8b7d95cdffc26d34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e76fd78549234544b70bfe84726de3d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading and Measuring Native Model (BF16/FP16) ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70249c329e3a4a07823405b507704ddf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-31 13:31:49.768205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756647110.089381      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756647110.215475      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63355a7231e9488db38b97449b8d868c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbc117543cf54c8b99bba0a742d9dacd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14d6774a604f49c886ede1ab2e619387",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b17f94acb9704dc0a0d2c45fa1f16902",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a57704b78c141fd9d6ff5f824d93800",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42b7d1d25e1d4dda96b279a146a13014",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Native model loaded in torch.bfloat16.\n",
            "Native Memory Footprint: 4.67 GB\n",
            "Running inference (5 runs)...\n",
            "Native Avg. Latency (100 tokens): 1.431 seconds\n",
            "Native model unloaded.\n",
            "\n",
            "--- Loading and Measuring 4-bit Model (NF4) ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bbb037ac87d459691da191a1c2014bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4-bit model loaded.\n",
            "4-bit Memory Footprint: 1.90 GB\n",
            "Running inference (5 runs)...\n",
            "4-bit Avg. Latency (100 tokens): 2.481 seconds\n",
            "4-bit model unloaded.\n",
            "\n",
            "--- Performance Comparison Summary ---\n",
            "Metric                 | Native (bfloat16) | 4-bit (NF4)\n",
            "-----------------------|-------------------|-----------------\n",
            "Memory Footprint       | 4.67 GB           | 1.90 GB        \n",
            "Avg. Latency (100 toks) | 1.431 s           | 2.481 s        \n",
            "\n",
            "Memory Reduction (4-bit vs Native): 59.3%\n",
            "Inference Speedup (4-bit vs Native): 0.58x\n",
            "\n",
            "Note: Results are indicative and highly dependent on hardware, model, batch size, and specific generation parameters.\n"
          ]
        }
      ],
      "source": [
        "# --- Recipe: Measuring the Gains (Quantization Performance) ---\n",
        "# Goal: Compare memory usage and inference speed before and after quantization.\n",
        "# Method: Uses BitsAndBytes 4-bit quantization for easy comparison within one script.\n",
        "# Libraries: transformers, torch, accelerate, bitsandbytes, sentencepiece, time\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"google/gemma-2b\" # Choose a model to test\n",
        "# MODEL_ID = \"gpt2-large\" # Another option\n",
        "PROMPT = \"Explain the concept of transfer learning in machine learning in about 50 words.\"\n",
        "NUM_TOKENS_TO_GENERATE = 100\n",
        "NUM_INFERENCE_RUNS = 5 # Number of times to run inference for averaging speed\n",
        "\n",
        "# --- 1. Load Tokenizer ---\n",
        "print(f\"Loading tokenizer for: {MODEL_ID}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "except Exception as e: print(f\"Error loading tokenizer: {e}\"); exit()\n",
        "\n",
        "# --- 2. Load Native Model & Measure ---\n",
        "print(\"\\n--- Loading and Measuring Native Model (BF16/FP16) ---\")\n",
        "native_results = {\"memory_gb\": \"N/A\", \"avg_latency_s\": \"N/A\"}\n",
        "model_native = None # Define variable outside try block\n",
        "try:\n",
        "    compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "    model_native = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=compute_dtype,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    if model_native.config.pad_token_id is None: model_native.config.pad_token_id = tokenizer.pad_token_id\n",
        "    print(f\"Native model loaded in {compute_dtype}.\")\n",
        "\n",
        "    # Measure Memory\n",
        "    mem_footprint_native = model_native.get_memory_footprint()\n",
        "    native_results[\"memory_gb\"] = mem_footprint_native / 1024**3\n",
        "    print(f\"Native Memory Footprint: {native_results['memory_gb']:.2f} GB\")\n",
        "\n",
        "    # Measure Inference Speed\n",
        "    print(f\"Running inference ({NUM_INFERENCE_RUNS} runs)...\")\n",
        "    latencies = []\n",
        "    inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model_native.device)\n",
        "    for _ in range(NUM_INFERENCE_RUNS + 1): # +1 for warmup run\n",
        "        torch.cuda.synchronize() # Ensure sync before timing\n",
        "        start_time = time.time()\n",
        "        _ = model_native.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=NUM_TOKENS_TO_GENERATE,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False # Use greedy for consistent timing\n",
        "        )\n",
        "        torch.cuda.synchronize() # Ensure sync after generation\n",
        "        end_time = time.time()\n",
        "        latencies.append(end_time - start_time)\n",
        "\n",
        "    avg_latency = np.mean(latencies[1:]) # Exclude warmup run\n",
        "    native_results[\"avg_latency_s\"] = avg_latency\n",
        "    print(f\"Native Avg. Latency ({NUM_TOKENS_TO_GENERATE} tokens): {avg_latency:.3f} seconds\")\n",
        "\n",
        "    # Clean up memory\n",
        "    del model_native\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Native model unloaded.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during native model loading or inference: {e}\")\n",
        "    if 'model_native' in locals() and model_native is not None: del model_native\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 3. Load 4-bit Quantized Model & Measure ---\n",
        "print(\"\\n--- Loading and Measuring 4-bit Model (NF4) ---\")\n",
        "quantized_results = {\"memory_gb\": \"N/A\", \"avg_latency_s\": \"N/A\"}\n",
        "model_4bit = None # Define variable outside try block\n",
        "try:\n",
        "    compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "    bnb_config_4bit = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config_4bit,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    if model_4bit.config.pad_token_id is None: model_4bit.config.pad_token_id = tokenizer.pad_token_id\n",
        "    print(\"4-bit model loaded.\")\n",
        "\n",
        "    # Measure Memory\n",
        "    mem_footprint_4bit = model_4bit.get_memory_footprint()\n",
        "    quantized_results[\"memory_gb\"] = mem_footprint_4bit / 1024**3\n",
        "    print(f\"4-bit Memory Footprint: {quantized_results['memory_gb']:.2f} GB\")\n",
        "\n",
        "    # Measure Inference Speed\n",
        "    print(f\"Running inference ({NUM_INFERENCE_RUNS} runs)...\")\n",
        "    latencies_4bit = []\n",
        "    inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model_4bit.device)\n",
        "    for _ in range(NUM_INFERENCE_RUNS + 1): # Warmup run\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "        _ = model_4bit.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=NUM_TOKENS_TO_GENERATE,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        latencies_4bit.append(end_time - start_time)\n",
        "\n",
        "    avg_latency_4bit = np.mean(latencies_4bit[1:]) # Exclude warmup\n",
        "    quantized_results[\"avg_latency_s\"] = avg_latency_4bit\n",
        "    print(f\"4-bit Avg. Latency ({NUM_TOKENS_TO_GENERATE} tokens): {avg_latency_4bit:.3f} seconds\")\n",
        "\n",
        "    # Clean up memory\n",
        "    del model_4bit\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"4-bit model unloaded.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during 4-bit model loading or inference: {e}\")\n",
        "    if 'model_4bit' in locals() and model_4bit is not None: del model_4bit\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- 4. Comparison Summary ---\n",
        "print(\"\\n--- Performance Comparison Summary ---\")\n",
        "dtype_str = str(compute_dtype).replace(\"torch.\", \"\") if isinstance(compute_dtype, torch.dtype) else \"N/A\"\n",
        "print(f\"Metric                 | Native ({dtype_str}) | 4-bit (NF4)\")\n",
        "print(f\"-----------------------|-------------------|-----------------\")\n",
        "mem_native_str = f\"{native_results['memory_gb']:.2f} GB\" if isinstance(native_results['memory_gb'], float) else native_results['memory_gb']\n",
        "mem_4bit_str = f\"{quantized_results['memory_gb']:.2f} GB\" if isinstance(quantized_results['memory_gb'], float) else quantized_results['memory_gb']\n",
        "print(f\"Memory Footprint       | {mem_native_str:<17} | {mem_4bit_str:<15}\")\n",
        "\n",
        "lat_native_str = f\"{native_results['avg_latency_s']:.3f} s\" if isinstance(native_results['avg_latency_s'], float) else native_results['avg_latency_s']\n",
        "lat_4bit_str = f\"{quantized_results['avg_latency_s']:.3f} s\" if isinstance(quantized_results['avg_latency_s'], float) else quantized_results['avg_latency_s']\n",
        "print(f\"Avg. Latency ({NUM_TOKENS_TO_GENERATE} toks) | {lat_native_str:<17} | {lat_4bit_str:<15}\")\n",
        "\n",
        "# Calculate relative changes if possible\n",
        "if isinstance(native_results['memory_gb'], float) and isinstance(quantized_results['memory_gb'], float):\n",
        "    mem_reduction = (1 - quantized_results['memory_gb'] / native_results['memory_gb']) * 100\n",
        "    print(f\"\\nMemory Reduction (4-bit vs Native): {mem_reduction:.1f}%\")\n",
        "if isinstance(native_results['avg_latency_s'], float) and isinstance(quantized_results['avg_latency_s'], float):\n",
        "    speedup = native_results['avg_latency_s'] / quantized_results['avg_latency_s']\n",
        "    print(f\"Inference Speedup (4-bit vs Native): {speedup:.2f}x\")\n",
        "\n",
        "print(\"\\nNote: Results are indicative and highly dependent on hardware, model, batch size, and specific generation parameters.\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}