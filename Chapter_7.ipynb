{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathbolla/The-LLM-Cookbook-Practical-Recipes-for-Fine-Tuning-Optimization-and-Deployment/blob/main/Chapter_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-1. Formatting Instructions with a Prompt Template\n"
      ],
      "metadata": {
        "id": "IH9-YDBJglLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T08:37:13.850940Z",
          "iopub.execute_input": "2025-06-03T08:37:13.851185Z",
          "iopub.status.idle": "2025-06-03T08:37:18.637859Z",
          "shell.execute_reply.started": "2025-06-03T08:37:13.851160Z",
          "shell.execute_reply": "2025-06-03T08:37:18.636986Z"
        },
        "id": "0exKLzOPaz3R",
        "outputId": "6366068b-ae0d-4b1d-b947-d731db270217"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Formatting the Instructions ---\n",
        "# Goal: Apply a specific prompt template to structure raw instruction data.\n",
        "# Method: Uses a simple Python function and demonstrates the Alpaca template.\n",
        "\n",
        "import json\n",
        "\n",
        "# --- 1. Sample Raw Data (List of Dictionaries) ---\n",
        "# Represents data you might load from a file or API\n",
        "raw_data = [\n",
        "    {\n",
        "        \"instruction\": \"Provide a brief summary of the concept of transfer learning in machine learning.\",\n",
        "        \"input\": \"\", # No additional input needed\n",
        "        \"output\": \"Transfer learning is a machine learning technique where a model developed for a task is reused as the starting point for a model on a second, related task. It leverages knowledge gained from the source task to improve performance on the target task, often reducing the need for large amounts of target-specific data.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Translate the following sentence to Spanish.\",\n",
        "        \"input\": \"Hello, how are you?\",\n",
        "        \"output\": \"Hola, ¿cómo estás?\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"List three common types of renewable energy sources.\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"1. Solar Energy\\n2. Wind Energy\\n3. Hydroelectric Energy\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- 2. Define Prompt Templates ---\n",
        "# Using the Alpaca format as an example\n",
        "\n",
        "# Template for instructions WITH input context\n",
        "PROMPT_WITH_INPUT_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "\"\"\" # Output will be appended here during training/inference\n",
        "\n",
        "# Template for instructions WITHOUT input context\n",
        "PROMPT_NO_INPUT_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\" # Output will be appended here during training/inference\n",
        "\n",
        "# --- 3. Formatting Function ---\n",
        "def format_instruction_data(example):\n",
        "    \"\"\"Applies the appropriate Alpaca prompt template.\"\"\"\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_context = example.get(\"input\", \"\")\n",
        "    output = example.get(\"output\", \"\") # Output is needed for training labels\n",
        "\n",
        "    if input_context and input_context.strip():\n",
        "        # Use the template with input\n",
        "        prompt_start = PROMPT_WITH_INPUT_TEMPLATE.format(\n",
        "            instruction=instruction,\n",
        "            input=input_context\n",
        "        )\n",
        "    else:\n",
        "        # Use the template without input\n",
        "        prompt_start = PROMPT_NO_INPUT_TEMPLATE.format(\n",
        "            instruction=instruction\n",
        "        )\n",
        "\n",
        "    # For training, we concatenate the prompt start and the expected output\n",
        "    # For inference, we only use prompt_start\n",
        "    formatted_text_for_training = prompt_start + output\n",
        "\n",
        "    return {\n",
        "        \"formatted_prompt\": prompt_start, # Useful for inference later\n",
        "        \"formatted_training_text\": formatted_text_for_training\n",
        "    }\n",
        "\n",
        "# --- 4. Apply Formatting ---\n",
        "print(\"--- Formatting Raw Data ---\")\n",
        "formatted_data = []\n",
        "for example in raw_data:\n",
        "    formatted_example = format_instruction_data(example)\n",
        "    formatted_data.append(formatted_example)\n",
        "\n",
        "# --- 5. Display Results ---\n",
        "for i, item in enumerate(formatted_data):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Original: {raw_data[i]}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Formatted Prompt (for Inference):\\n{item['formatted_prompt']}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Formatted Text (for Training):\\n{item['formatted_training_text']}\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "# --- Notes ---\n",
        "# - This formatted_training_text is what you would tokenize for Causal LM fine-tuning.\n",
        "# - During tokenization for training, you need to identify which tokens belong to the\n",
        "#   'output' part to avoid masking them during loss calculation.\n",
        "# - Remember to add the EOS token to the end of formatted_training_text before tokenization.\n",
        "\n",
        "# --- End of Recipe ---"
      ],
      "metadata": {
        "id": "7tI_4OHngSwX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T09:58:29.602105Z",
          "iopub.execute_input": "2025-06-01T09:58:29.602414Z",
          "iopub.status.idle": "2025-06-01T09:58:29.612160Z",
          "shell.execute_reply.started": "2025-06-01T09:58:29.602387Z",
          "shell.execute_reply": "2025-06-01T09:58:29.611513Z"
        },
        "outputId": "1fada94a-363e-4c95-8c11-b6165f3fad05"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Formatting Raw Data ---\n\n--- Example 1 ---\nOriginal: {'instruction': 'Provide a brief summary of the concept of transfer learning in machine learning.', 'input': '', 'output': 'Transfer learning is a machine learning technique where a model developed for a task is reused as the starting point for a model on a second, related task. It leverages knowledge gained from the source task to improve performance on the target task, often reducing the need for large amounts of target-specific data.'}\n--------------------\nFormatted Prompt (for Inference):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nProvide a brief summary of the concept of transfer learning in machine learning.\n\n### Response:\n\n--------------------\nFormatted Text (for Training):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nProvide a brief summary of the concept of transfer learning in machine learning.\n\n### Response:\nTransfer learning is a machine learning technique where a model developed for a task is reused as the starting point for a model on a second, related task. It leverages knowledge gained from the source task to improve performance on the target task, often reducing the need for large amounts of target-specific data.\n========================================\n\n--- Example 2 ---\nOriginal: {'instruction': 'Translate the following sentence to Spanish.', 'input': 'Hello, how are you?', 'output': 'Hola, ¿cómo estás?'}\n--------------------\nFormatted Prompt (for Inference):\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nTranslate the following sentence to Spanish.\n\n### Input:\nHello, how are you?\n\n### Response:\n\n--------------------\nFormatted Text (for Training):\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nTranslate the following sentence to Spanish.\n\n### Input:\nHello, how are you?\n\n### Response:\nHola, ¿cómo estás?\n========================================\n\n--- Example 3 ---\nOriginal: {'instruction': 'List three common types of renewable energy sources.', 'input': '', 'output': '1. Solar Energy\\n2. Wind Energy\\n3. Hydroelectric Energy'}\n--------------------\nFormatted Prompt (for Inference):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nList three common types of renewable energy sources.\n\n### Response:\n\n--------------------\nFormatted Text (for Training):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nList three common types of renewable energy sources.\n\n### Response:\n1. Solar Energy\n2. Wind Energy\n3. Hydroelectric Energy\n========================================\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-2. The IFT Training Loop with Loss Masking"
      ],
      "metadata": {
        "id": "EX2zAGuigsqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub import login\n",
        "\n",
        "api = HfApi()\n",
        "whoami = api.whoami(token=\"hf_xxxxxxxxxxxxxx\")\n",
        "print(whoami)\n",
        "login(\"hf_xxxxxxxxxxxxxxxxxxx\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T08:28:09.254546Z",
          "iopub.execute_input": "2025-06-03T08:28:09.254846Z",
          "iopub.status.idle": "2025-06-03T08:28:09.941699Z",
          "shell.execute_reply.started": "2025-06-03T08:28:09.254803Z",
          "shell.execute_reply": "2025-06-03T08:28:09.940988Z"
        },
        "id": "OgU2rWoBaz3V",
        "outputId": "a9923eeb-c3a5-4a63-a3e1-5c92ca6719a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'type': 'user', 'id': '65feba1b57cc48d9d30d11cf', 'name': 'kalpasubbaiah', 'fullname': 'Kalpa Subbaiah', 'email': 'kalpa.subbaiah@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/319094e0eb55ce89334d7bd3685ceeb0.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'hugging_face_token_read', 'role': 'read', 'createdAt': '2025-04-22T09:03:46.223Z'}}}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: The IFT Training Loop ---\n",
        "# Goal: Fine-tune a base Causal LM on a formatted instruction dataset using Trainer.\n",
        "# Method: Includes prompt formatting and loss masking for instruction tokens.\n",
        "# Libraries: transformers, datasets, torch, accelerate\n",
        "# Note: Requires significant VRAM, especially for models like Gemma-2B.\n",
        "#       Uses a tiny dummy dataset for demonstration. Replace with a real dataset.\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset # To create a dummy dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "import copy\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_CHECKPOINT = \"distilgpt2\" # Use small model for demo; Replace with e.g., \"google/gemma-2b\" for better results\n",
        "# DATASET_NAME = \"databricks/databricks-dolly-15k\" # Example real dataset (requires loading)\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 2 # Keep very small for demo\n",
        "GRADIENT_ACCUMULATION_STEPS = 2 # Simulate batch size 4\n",
        "LEARNING_RATE = 2e-5\n",
        "OUTPUT_DIR = \"./instruction_finetune_output\"\n",
        "MAX_LENGTH = 256 # Max sequence length for tokenization\n",
        "\n",
        "# --- 1. Load Tokenizer & Prepare Prompt Templates (Alpaca Style) ---\n",
        "print(f\"Loading tokenizer for checkpoint: {MODEL_CHECKPOINT}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Set PAD token to EOS token: {tokenizer.pad_token}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")\n",
        "\n",
        "# --- 2. Create/Load and Format Dataset ---\n",
        "# Using a tiny dummy dataset for demonstration\n",
        "dummy_data = [\n",
        "    {\"instruction\": \"Say hello.\", \"input\": \"\", \"output\": \"Hello!\"},\n",
        "    {\"instruction\": \"Add two numbers.\", \"input\": \"5 and 3\", \"output\": \"5 + 3 = 8\"},\n",
        "    {\"instruction\": \"Describe the sun.\", \"input\": \"\", \"output\": \"The sun is a star at the center of our solar system.\"},\n",
        "    {\"instruction\": \"Translate to German.\", \"input\": \"Thank you\", \"output\": \"Danke schön\"},\n",
        "]\n",
        "# Convert to Hugging Face Dataset object\n",
        "raw_dataset = Dataset.from_list(dummy_data)\n",
        "print(\"Dummy Dataset:\")\n",
        "print(raw_dataset)\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    \"\"\"Formats, tokenizes, and prepares labels with masking for IFT.\"\"\"\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_context = example.get(\"input\", \"\")\n",
        "    output = example.get(\"output\", \"\")\n",
        "\n",
        "    # Choose prompt template\n",
        "    if input_context and input_context.strip():\n",
        "        prompt_start = PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        prompt_start = PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "    # Concatenate prompt and output, add EOS token\n",
        "    full_text = prompt_start + output + tokenizer.eos_token\n",
        "\n",
        "    # Tokenize the full text\n",
        "    tokenized_full = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "\n",
        "    # Tokenize the prompt part *only* to find its length\n",
        "    tokenized_prompt = tokenizer(prompt_start, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    prompt_length = len(tokenized_prompt[\"input_ids\"])\n",
        "\n",
        "    # Create labels - initially same as input_ids\n",
        "    labels = copy.deepcopy(tokenized_full[\"input_ids\"])\n",
        "\n",
        "    # --- Crucial Step: Mask prompt tokens in labels ---\n",
        "    # Set label IDs for prompt tokens to -100 so they are ignored in loss calculation\n",
        "    for i in range(prompt_length):\n",
        "        labels[i] = -100\n",
        "\n",
        "    # Ensure attention mask is included\n",
        "    tokenized_full[\"labels\"] = labels\n",
        "\n",
        "    # Sanity check (optional): Decode labels, ignoring -100\n",
        "    # decoded_labels = tokenizer.decode([l for l in labels if l != -100])\n",
        "    # print(f\"Decoded Labels (should match output + EOS): {decoded_labels}\")\n",
        "\n",
        "    return tokenized_full\n",
        "\n",
        "print(\"\\nFormatting and tokenizing dataset...\")\n",
        "try:\n",
        "    # Apply formatting and tokenization\n",
        "    # Remove original columns as they are now part of the tokenized structure\n",
        "    tokenized_dataset = raw_dataset.map(\n",
        "        format_and_tokenize,\n",
        "        remove_columns=raw_dataset.column_names\n",
        "    )\n",
        "    tokenized_dataset.set_format(\"torch\")\n",
        "    # Split (if needed - using full small dataset for train/eval here for demo)\n",
        "    # train_val_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    # final_datasets = {\"train\": train_val_split[\"train\"], \"validation\": train_val_split[\"test\"]}\n",
        "    final_datasets = {\"train\": tokenized_dataset, \"validation\": tokenized_dataset} # Use same for demo\n",
        "\n",
        "    print(\"Dataset processed. Sample tokenized input and labels:\")\n",
        "    print(f\"Input IDs: {final_datasets['train'][0]['input_ids']}\")\n",
        "    print(f\"Labels:    {final_datasets['train'][0]['labels']}\") # Note the -100 values\n",
        "except Exception as e:\n",
        "    print(f\"Error processing dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Data Collator ---\n",
        "# Use standard LM collator; masking is handled in preprocessing\n",
        "# mlm=False for Causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# --- 4. Load Model ---\n",
        "print(f\"\\nLoading base Causal LM: {MODEL_CHECKPOINT}\")\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "    # Ensure model pad token id is set if tokenizer's was changed\n",
        "    if model.config.pad_token_id is None:\n",
        "         model.config.pad_token_id = tokenizer.pad_token_id\n",
        "         print(f\"Set model.config.pad_token_id to: {model.config.pad_token_id}\")\n",
        "    print(f\"Model loaded. Initial device: {model.device}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 5. Training Arguments ---\n",
        "print(\"\\nDefining Training Arguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1, # Log frequently for small dataset\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "print(f\"Training on device: {training_args.device}\")\n",
        "\n",
        "# --- 6. Initialize Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_datasets[\"train\"],\n",
        "    eval_dataset=final_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    # No compute_metrics needed unless calculating perplexity explicitly\n",
        ")\n",
        "\n",
        "# --- 7. Train ---\n",
        "print(\"\\nStarting instruction fine-tuning...\")\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "    trainer.save_model(OUTPUT_DIR) # Save final best model\n",
        "    print(f\"Model and training state saved to {OUTPUT_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 8. Evaluate (Optional - reports loss by default) ---\n",
        "print(\"\\nEvaluating final model...\")\n",
        "try:\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation Results (Loss):\")\n",
        "    print(eval_results)\n",
        "    # Calculate perplexity if desired\n",
        "    # perplexity = math.exp(eval_results['eval_loss'])\n",
        "    # print(f\"Perplexity: {perplexity:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during evaluation: {e}\")\n",
        "\n",
        "# --- End of Recipe ---\n",
        "\n",
        "###########################################################\n",
        "#Recipe: Qualitative Evaluation of IFT Model¶\n",
        "# --- Recipe: Checking for Understanding (Qualitative Evaluation) ---\n",
        "# Goal: Test the instruction-following capabilities of the fine-tuned model.\n",
        "# Method: Load the IFT model and generate responses for unseen instructions.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to the saved instruction-fine-tuned model\n",
        "IFT_MODEL_PATH = \"./instruction_finetune_output\" # From ch7_recipe_ift_trainer\n",
        "\n",
        "# Base model checkpoint (needed to load tokenizer if not saved with model)\n",
        "# Or load tokenizer from IFT_MODEL_PATH if it was saved there\n",
        "TOKENIZER_CHECKPOINT = \"distilgpt2\" # Must match the base model used for IFT\n",
        "\n",
        "# Use GPU if available\n",
        "device_index = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# --- 1. Load Fine-Tuned Model and Tokenizer ---\n",
        "print(f\"Loading fine-tuned IFT model from: {IFT_MODEL_PATH}\")\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(IFT_MODEL_PATH)\n",
        "    # Try loading tokenizer from the fine-tuned path first, fallback to base\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(IFT_MODEL_PATH)\n",
        "    except OSError:\n",
        "        print(f\"Tokenizer not found in {IFT_MODEL_PATH}, loading from {TOKENIZER_CHECKPOINT}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_CHECKPOINT)\n",
        "\n",
        "    # Ensure PAD token is set correctly (important for generation)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Set PAD token to EOS token: {tokenizer.pad_token}\")\n",
        "    # Ensure model config has pad token id\n",
        "    if model.config.pad_token_id is None:\n",
        "         model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    print(\"IFT Model and Tokenizer loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model/tokenizer: {e}\")\n",
        "    print(\"Ensure the IFT training recipe ran successfully and saved the model.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Create Inference Pipeline ---\n",
        "# Using pipeline simplifies generation\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device_index\n",
        ")\n",
        "\n",
        "# --- 3. Define Prompt Formatting Function (Must match training template!) ---\n",
        "# Re-use the templates from the formatting recipe\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")\n",
        "\n",
        "def format_inference_prompt(instruction, input_context=\"\"):\n",
        "    \"\"\"Formats the prompt for inference using the Alpaca template.\"\"\"\n",
        "    if input_context and input_context.strip():\n",
        "        return PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        return PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "# --- 4. Test with Unseen Instructions ---\n",
        "test_instructions = [\n",
        "    {\"instruction\": \"What is the capital of France?\"},\n",
        "    {\"instruction\": \"Write a short story about a friendly robot.\", \"input\": \"\"},\n",
        "    {\"instruction\": \"Convert the following temperature from Celsius to Fahrenheit.\", \"input\": \"25°C\"},\n",
        "    {\"instruction\": \"List the planets in our solar system.\"},\n",
        "    {\"instruction\": \"Generate a python function to calculate factorial\"} # Task likely unseen in dummy data\n",
        "]\n",
        "\n",
        "print(\"\\n--- Testing IFT Model with Unseen Instructions ---\")\n",
        "for i, test_case in enumerate(test_instructions):\n",
        "    instruction = test_case[\"instruction\"]\n",
        "    input_context = test_case.get(\"input\", \"\")\n",
        "\n",
        "    # Format the prompt exactly as done during training (excluding the output)\n",
        "    prompt = format_inference_prompt(instruction, input_context)\n",
        "\n",
        "    print(f\"\\n--- Test Case {i+1} ---\")\n",
        "    print(f\"Instruction: {instruction}\")\n",
        "    if input_context: print(f\"Input: {input_context}\")\n",
        "    print(f\"Formatted Prompt (Input to Model):\\n{prompt}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    try:\n",
        "        # Generate response\n",
        "        # Adjust generation parameters as needed\n",
        "        outputs = generator(\n",
        "            prompt,\n",
        "            max_new_tokens=100, # Limit generated length\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id # Often needed for Causal LMs\n",
        "        )\n",
        "        # Extract only the generated part (the response)\n",
        "        # The output includes the prompt, so we split/slice based on prompt length\n",
        "        response = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "        print(f\"Generated Response:\\n{response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation for this test case: {e}\")\n",
        "\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "# --- 5. Qualitative Assessment ---\n",
        "# Review the generated responses. Does the model:\n",
        "# - Understand the instruction?\n",
        "# - Provide a relevant and coherent answer?\n",
        "# - Follow formatting requests (if any were given)?\n",
        "# - Avoid simply repeating the prompt?\n",
        "# - Handle tasks it likely didn't see in the (dummy) training data?\n",
        "#\n",
        "# Note: With the dummy dataset and small model used in the training recipe,\n",
        "# the results here will likely be poor. Using a larger base model and a real\n",
        "# instruction dataset (like Dolly) would yield much better instruction following.\n",
        "\n",
        "# --- End of Recipe ---"
      ],
      "metadata": {
        "id": "XU3v8WaOgtdD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T08:47:23.219801Z",
          "iopub.execute_input": "2025-06-03T08:47:23.220431Z",
          "iopub.status.idle": "2025-06-03T08:47:31.530977Z",
          "shell.execute_reply.started": "2025-06-03T08:47:23.220404Z",
          "shell.execute_reply": "2025-06-03T08:47:31.530155Z"
        },
        "outputId": "009f2299-f179-4ca9-8a86-caa250479296",
        "colab": {
          "referenced_widgets": [
            "09617e9f85074853b5ed123343c0a73b"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading tokenizer for checkpoint: distilgpt2\nSet PAD token to EOS token: <|endoftext|>\nDummy Dataset:\nDataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 4\n})\n\nFormatting and tokenizing dataset...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/4 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09617e9f85074853b5ed123343c0a73b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset processed. Sample tokenized input and labels:\nInput IDs: tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n        21017, 46486,    25,   198, 25515, 23748,    13,   198,   198, 21017,\n        18261,    25,   198, 15496,     0, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256])\nLabels:    tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100])\n\nLoading base Causal LM: distilgpt2\nSet model.config.pad_token_id to: 50256\nModel loaded. Initial device: cpu\n\nDefining Training Arguments...\nTraining on device: cuda:0\n\nStarting instruction fine-tuning...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_31/3092935292.py:165: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:02, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.203800</td>\n      <td>3.903072</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training finished.\n***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =      243GF\n  train_loss               =     2.2038\n  train_runtime            = 0:00:02.42\n  train_samples_per_second =      1.651\n  train_steps_per_second   =      0.413\nModel and training state saved to ./instruction_finetune_output\n\nEvaluating final model...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Evaluation Results (Loss):\n{'eval_loss': 3.9030723571777344, 'eval_runtime': 0.0921, 'eval_samples_per_second': 43.441, 'eval_steps_per_second': 10.86, 'epoch': 1.0}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading fine-tuned IFT model from: ./instruction_finetune_output\nIFT Model and Tokenizer loaded.\n\n--- Testing IFT Model with Unseen Instructions ---\n\n--- Test Case 1 ---\nInstruction: What is the capital of France?\nFormatted Prompt (Input to Model):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat is the capital of France?\n\n### Response:\n\n--------------------\nGenerated Response:\nHow do we know that we have a capital?\n### Response:\nWhat is the capital of France?\n### Response:\nHow do we know that we have a capital?\n### Response:\nHow do we know that we have a capital?\n### Response:\nWhat is the capital of France?\n### Response:\nWhat is the capital of France?\n### Response:\nWhat is the capital of France?\n### Response:\nHow do we know that we have\n========================================\n\n--- Test Case 2 ---\nInstruction: Write a short story about a friendly robot.\nFormatted Prompt (Input to Model):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWrite a short story about a friendly robot.\n\n### Response:\n\n--------------------\nGenerated Response:\nWrite a short story about a friendly robot.\n### Response:\nWrite a short story about a friendly robot.\n### Response:\nWrite a short story about a friendly robot.\n### Response:\nWrite a short story about a friendly robot.\n### Response:\nWrite a short story about a friendly robot.\n### Response:\nWrite a short story about a friendly robot.\n### Response:\nWrite a short story about a friendly robot.\n### Response:\nWrite a\n========================================\n\n--- Test Case 3 ---\nInstruction: Convert the following temperature from Celsius to Fahrenheit.\nInput: 25°C\nFormatted Prompt (Input to Model):\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert the following temperature from Celsius to Fahrenheit.\n\n### Input:\n25°C\n\n### Response:\n\n--------------------\nGenerated Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n### Response:\n25°C\n========================================\n\n--- Test Case 4 ---\nInstruction: List the planets in our solar system.\nFormatted Prompt (Input to Model):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nList the planets in our solar system.\n\n### Response:\n\n--------------------\nGenerated Response:\nList the planets in our solar system.\n###\n###\n### The data is sent to the data stream.\n###\n###\n### The data is sent to the data stream.\n###\n###\n### The data is sent to the data stream.\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n###\n========================================\n\n--- Test Case 5 ---\nInstruction: Generate a python function to calculate factorial\nFormatted Prompt (Input to Model):\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a python function to calculate factorial\n\n### Response:\n\n--------------------\nGenerated Response:\nGenerate a python function to compute factorial\n### Error:\nA simple function to calculate factorial\n###\n## Generate a python function to compute factorial\n###\n## Generate a python function to compute factorial\n### Error:\nA simple function to calculate factorial\n###\n### Response:\nGenerate a python function to compute factorial\n### Error:\nA simple function to calculate factorial\n### Error:\nA simple function to calculate factorial\n========================================\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGUuv5eUCHxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-3: Qualitative Evaluation of IFT Model"
      ],
      "metadata": {
        "id": "34xE5zBECT5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Recipe: Qualitative Evaluation of IFT Model¶\n",
        "# --- Recipe: Checking for Understanding (Qualitative Evaluation) ---\n",
        "# Goal: Test the instruction-following capabilities of the fine-tuned model.\n",
        "# Method: Load the IFT model and generate responses for unseen instructions.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to the saved instruction-fine-tuned model\n",
        "IFT_MODEL_PATH = \"./instruction_finetune_output\" # From ch7_recipe_ift_trainer\n",
        "\n",
        "# Base model checkpoint (needed to load tokenizer if not saved with model)\n",
        "# Or load tokenizer from IFT_MODEL_PATH if it was saved there\n",
        "TOKENIZER_CHECKPOINT = \"distilgpt2\" # Must match the base model used for IFT\n",
        "\n",
        "# Use GPU if available\n",
        "device_index = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# --- 1. Load Fine-Tuned Model and Tokenizer ---\n",
        "print(f\"Loading fine-tuned IFT model from: {IFT_MODEL_PATH}\")\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(IFT_MODEL_PATH)\n",
        "    # Try loading tokenizer from the fine-tuned path first, fallback to base\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(IFT_MODEL_PATH)\n",
        "    except OSError:\n",
        "        print(f\"Tokenizer not found in {IFT_MODEL_PATH}, loading from {TOKENIZER_CHECKPOINT}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_CHECKPOINT)\n",
        "\n",
        "    # Ensure PAD token is set correctly (important for generation)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Set PAD token to EOS token: {tokenizer.pad_token}\")\n",
        "    # Ensure model config has pad token id\n",
        "    if model.config.pad_token_id is None:\n",
        "         model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    print(\"IFT Model and Tokenizer loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model/tokenizer: {e}\")\n",
        "    print(\"Ensure the IFT training recipe ran successfully and saved the model.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Create Inference Pipeline ---\n",
        "# Using pipeline simplifies generation\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device_index\n",
        ")\n",
        "\n",
        "# --- 3. Define Prompt Formatting Function (Must match training template!) ---\n",
        "# Re-use the templates from the formatting recipe\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")\n",
        "\n",
        "def format_inference_prompt(instruction, input_context=\"\"):\n",
        "    \"\"\"Formats the prompt for inference using the Alpaca template.\"\"\"\n",
        "    if input_context and input_context.strip():\n",
        "        return PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        return PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "# --- 4. Test with Unseen Instructions ---\n",
        "test_instructions = [\n",
        "    {\"instruction\": \"What is the capital of France?\"},\n",
        "    {\"instruction\": \"Write a short story about a friendly robot.\", \"input\": \"\"},\n",
        "    {\"instruction\": \"Convert the following temperature from Celsius to Fahrenheit.\", \"input\": \"25°C\"},\n",
        "    {\"instruction\": \"List the planets in our solar system.\"},\n",
        "    {\"instruction\": \"Generate a python function to calculate factorial\"} # Task likely unseen in dummy data\n",
        "]\n",
        "\n",
        "print(\"\\n--- Testing IFT Model with Unseen Instructions ---\")\n",
        "for i, test_case in enumerate(test_instructions):\n",
        "    instruction = test_case[\"instruction\"]\n",
        "    input_context = test_case.get(\"input\", \"\")\n",
        "\n",
        "    # Format the prompt exactly as done during training (excluding the output)\n",
        "    prompt = format_inference_prompt(instruction, input_context)\n",
        "\n",
        "    print(f\"\\n--- Test Case {i+1} ---\")\n",
        "    print(f\"Instruction: {instruction}\")\n",
        "    if input_context: print(f\"Input: {input_context}\")\n",
        "    print(f\"Formatted Prompt (Input to Model):\\n{prompt}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    try:\n",
        "        # Generate response\n",
        "        # Adjust generation parameters as needed\n",
        "        outputs = generator(\n",
        "            prompt,\n",
        "            max_new_tokens=100, # Limit generated length\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id # Often needed for Causal LMs\n",
        "        )\n",
        "        # Extract only the generated part (the response)\n",
        "        # The output includes the prompt, so we split/slice based on prompt length\n",
        "        response = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "        print(f\"Generated Response:\\n{response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation for this test case: {e}\")\n",
        "\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "# --- 5. Qualitative Assessment ---\n",
        "# Review the generated responses. Does the model:\n",
        "# - Understand the instruction?\n",
        "# - Provide a relevant and coherent answer?\n",
        "# - Follow formatting requests (if any were given)?\n",
        "# - Avoid simply repeating the prompt?\n",
        "# - Handle tasks it likely didn't see in the (dummy) training data?\n",
        "#\n",
        "# Note: With the dummy dataset and small model used in the training recipe,\n",
        "# the results here will likely be poor. Using a larger base model and a real\n",
        "# instruction dataset (like Dolly) would yield much better instruction following.\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "id": "lFOp3zK8CXJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-4: Comparing Self-Instruct vs Human-Curated\n"
      ],
      "metadata": {
        "id": "Q6wQUyZKlqvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Comparing IFT Performance: Self-Instruct vs. Human-Curated ---\n",
        "# Goal: Fine-tune the same base model on two dataset types (Alpaca-style vs Dolly-style)\n",
        "#       and compare their performance qualitatively and via validation loss.\n",
        "# Method: Uses subsets of yahma/alpaca-cleaned and databricks/dolly-15k.\n",
        "# Libraries: transformers, datasets, torch, accelerate\n",
        "# Note: Uses distilgpt2 for speed; results differ greatly with larger models.\n",
        "#       Requires sufficient disk space for datasets and model checkpoints.\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_CHECKPOINT = \"distilgpt2\" # Small model for faster demo\n",
        "# Dataset 1: Self-Instruct style (Alpaca cleaned subset)\n",
        "DATASET_1_NAME = \"yahma/alpaca-cleaned\"\n",
        "OUTPUT_DIR_1 = \"./ift_alpaca_output\"\n",
        "# Dataset 2: Human-Generated style (Dolly subset)\n",
        "DATASET_2_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR_2 = \"./ift_dolly_output\"\n",
        "\n",
        "# Training Params (keep consistent for comparison)\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 2 # Keep small for demo\n",
        "GRADIENT_ACCUMULATION_STEPS = 4 # Effective batch size 8\n",
        "LEARNING_RATE = 2e-5\n",
        "MAX_LENGTH = 256 # Max sequence length\n",
        "NUM_SAMPLES_PER_DATASET = 500 # Use small subsets for faster demo run\n",
        "\n",
        "# --- 1. Load Tokenizer & Define Prompt Template ---\n",
        "# Use the same tokenizer and template for both fine-tuning runs\n",
        "print(f\"Loading tokenizer for checkpoint: {MODEL_CHECKPOINT}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Set PAD token to EOS token: {tokenizer.pad_token}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Using Alpaca-style template for consistency\n",
        "PROMPT_WITH_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
        ")\n",
        "PROMPT_NO_INPUT_TEMPLATE = (\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        ")\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing Function ---\n",
        "def format_and_tokenize(example, dataset_type):\n",
        "    \"\"\"Formats (Alpaca style), tokenizes, and prepares labels with masking.\"\"\"\n",
        "    # Adapt field names based on dataset\n",
        "    if dataset_type == 'alpaca':\n",
        "        instruction = example.get(\"instruction\", \"\")\n",
        "        input_context = example.get(\"input\", \"\")\n",
        "        output = example.get(\"output\", \"\")\n",
        "    elif dataset_type == 'dolly':\n",
        "        instruction = example.get(\"instruction\", \"\")\n",
        "        input_context = example.get(\"context\", \"\") # Dolly uses 'context'\n",
        "        output = example.get(\"response\", \"\")      # Dolly uses 'response'\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset_type\")\n",
        "\n",
        "    # Choose prompt template\n",
        "    if input_context and input_context.strip():\n",
        "        prompt_start = PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        prompt_start = PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "    # Concatenate prompt and output, add EOS token\n",
        "    full_text = prompt_start + output + tokenizer.eos_token\n",
        "\n",
        "    # Tokenize the full text\n",
        "    tokenized_full = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "\n",
        "    # Tokenize the prompt part *only* to find its length for masking\n",
        "    tokenized_prompt = tokenizer(prompt_start, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    prompt_length = len(tokenized_prompt[\"input_ids\"])\n",
        "\n",
        "    # Create labels - initially same as input_ids\n",
        "    labels = copy.deepcopy(tokenized_full[\"input_ids\"])\n",
        "\n",
        "    # --- Crucial Step: Mask prompt tokens in labels ---\n",
        "    for i in range(prompt_length):\n",
        "        if i < len(labels): # Ensure index is within bounds\n",
        "             labels[i] = -100\n",
        "\n",
        "    # Ensure attention mask is included\n",
        "    tokenized_full[\"labels\"] = labels\n",
        "    return tokenized_full\n",
        "\n",
        "# --- 3. Fine-Tune on Dataset 1 (Alpaca - Self-Instruct) ---\n",
        "print(f\"\\n--- Processing Dataset 1: {DATASET_1_NAME} ---\")\n",
        "model_1_results = {}\n",
        "try:\n",
        "    # Load subset\n",
        "    raw_dataset_1 = load_dataset(DATASET_1_NAME, split=f\"train[:{NUM_SAMPLES_PER_DATASET}]\")\n",
        "    # Filter out examples that might be too long after formatting (optional but good practice)\n",
        "    # raw_dataset_1 = raw_dataset_1.filter(lambda x: len(x['instruction']) + len(x['input']) + len(x['output']) < 1500)\n",
        "    tokenized_dataset_1 = raw_dataset_1.map(\n",
        "        lambda x: format_and_tokenize(x, 'alpaca'),\n",
        "        remove_columns=raw_dataset_1.column_names\n",
        "    )\n",
        "    # Create dummy validation set for demo if needed\n",
        "    split_ds_1 = tokenized_dataset_1.train_test_split(test_size=0.1, seed=42)\n",
        "    final_datasets_1 = {\"train\": split_ds_1[\"train\"], \"validation\": split_ds_1[\"test\"]}\n",
        "    print(\"Dataset 1 processed.\")\n",
        "\n",
        "    # Load Base Model\n",
        "    model_1 = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "    if model_1.config.pad_token_id is None: model_1.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Training Args\n",
        "    training_args_1 = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR_1, num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE, weight_decay=0.01, eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\", logging_strategy=\"steps\", logging_steps=10,\n",
        "        load_best_model_at_end=True, metric_for_best_model=\"loss\",\n",
        "        fp16=torch.cuda.is_available(), push_to_hub=False, report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Data Collator\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Trainer\n",
        "    trainer_1 = Trainer( model=model_1, args=training_args_1,\n",
        "        train_dataset=final_datasets_1[\"train\"], eval_dataset=final_datasets_1[\"validation\"],\n",
        "        tokenizer=tokenizer, data_collator=data_collator )\n",
        "\n",
        "    # Train\n",
        "    print(f\"\\nStarting fine-tuning on {DATASET_1_NAME}...\")\n",
        "    train_result_1 = trainer_1.train()\n",
        "    trainer_1.save_model(OUTPUT_DIR_1)\n",
        "    print(f\"Model 1 (Alpaca) saved to {OUTPUT_DIR_1}\")\n",
        "    # Store eval results\n",
        "    eval_results_1 = trainer_1.evaluate(eval_dataset=final_datasets_1[\"validation\"])\n",
        "    model_1_results = {\"eval_loss\": eval_results_1.get(\"eval_loss\", None)}\n",
        "    print(f\"Model 1 (Alpaca) Validation Results: {model_1_results}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Dataset 1 processing or training: {e}\")\n",
        "    # Clean up potentially loaded model\n",
        "    if 'model_1' in locals(): del model_1\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 4. Fine-Tune on Dataset 2 (Dolly - Human-Generated) ---\n",
        "print(f\"\\n--- Processing Dataset 2: {DATASET_2_NAME} ---\")\n",
        "model_2_results = {}\n",
        "# It's crucial to reload the *base* model to avoid continuing training\n",
        "if os.path.exists(OUTPUT_DIR_2):\n",
        "     print(f\"Output directory {OUTPUT_DIR_2} already exists. Skipping training for Model 2 assuming it's done.\")\n",
        "     # Attempt to load previous results if needed for comparison later\n",
        "     try:\n",
        "          # Placeholder: In a real scenario you might load metrics saved by Trainer\n",
        "          # For this demo, we'll assume it needs re-running if dir exists but no results loaded\n",
        "          print(\"Cannot load previous results in this demo script. Re-run required if comparison needed.\")\n",
        "     except:\n",
        "          print(\"Could not load previous results for Model 2.\")\n",
        "\n",
        "else:\n",
        "    try:\n",
        "        # Load subset\n",
        "        raw_dataset_2 = load_dataset(DATASET_2_NAME, split=f\"train[:{NUM_SAMPLES_PER_DATASET}]\")\n",
        "         # Filter out examples that might be too long after formatting\n",
        "        # raw_dataset_2 = raw_dataset_2.filter(lambda x: len(x['instruction']) + len(x['context']) + len(x['response']) < 1500)\n",
        "        tokenized_dataset_2 = raw_dataset_2.map(\n",
        "            lambda x: format_and_tokenize(x, 'dolly'),\n",
        "            remove_columns=raw_dataset_2.column_names\n",
        "        )\n",
        "        split_ds_2 = tokenized_dataset_2.train_test_split(test_size=0.1, seed=42)\n",
        "        final_datasets_2 = {\"train\": split_ds_2[\"train\"], \"validation\": split_ds_2[\"test\"]}\n",
        "        print(\"Dataset 2 processed.\")\n",
        "\n",
        "        # --- IMPORTANT: Reload the BASE model ---\n",
        "        print(f\"Reloading BASE model: {MODEL_CHECKPOINT}\")\n",
        "        model_2 = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "        if model_2.config.pad_token_id is None: model_2.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "        # Training Args (use different output dir)\n",
        "        training_args_2 = TrainingArguments(\n",
        "            output_dir=OUTPUT_DIR_2, num_train_epochs=NUM_EPOCHS,\n",
        "            per_device_train_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "            learning_rate=LEARNING_RATE, weight_decay=0.01, eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\", logging_strategy=\"steps\", logging_steps=10,\n",
        "            load_best_model_at_end=True, metric_for_best_model=\"loss\",\n",
        "            fp16=torch.cuda.is_available(), push_to_hub=False, report_to=\"none\"\n",
        "        )\n",
        "\n",
        "        # Data Collator (can reuse)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "        # Trainer\n",
        "        trainer_2 = Trainer( model=model_2, args=training_args_2,\n",
        "            train_dataset=final_datasets_2[\"train\"], eval_dataset=final_datasets_2[\"validation\"],\n",
        "            tokenizer=tokenizer, data_collator=data_collator )\n",
        "\n",
        "        # Train\n",
        "        print(f\"\\nStarting fine-tuning on {DATASET_2_NAME}...\")\n",
        "        train_result_2 = trainer_2.train()\n",
        "        trainer_2.save_model(OUTPUT_DIR_2)\n",
        "        print(f\"Model 2 (Dolly) saved to {OUTPUT_DIR_2}\")\n",
        "        # Store eval results\n",
        "        eval_results_2 = trainer_2.evaluate(eval_dataset=final_datasets_2[\"validation\"])\n",
        "        model_2_results = {\"eval_loss\": eval_results_2.get(\"eval_loss\", None)}\n",
        "        print(f\"Model 2 (Dolly) Validation Results: {model_2_results}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Dataset 2 processing or training: {e}\")\n",
        "        # Clean up\n",
        "        if 'model_2' in locals(): del model_2\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 5. Comparison ---\n",
        "print(\"\\n--- Comparison Summary ---\")\n",
        "\n",
        "# Compare Validation Loss (lower is generally better)\n",
        "loss1 = model_1_results.get('eval_loss', 'N/A')\n",
        "loss2 = model_2_results.get('eval_loss', 'N/A')\n",
        "print(f\"Model 1 (Alpaca) Final Validation Loss: {loss1}\")\n",
        "print(f\"Model 2 (Dolly) Final Validation Loss: {loss2}\")\n",
        "if isinstance(loss1, float) and isinstance(loss2, float):\n",
        "    if loss1 < loss2:\n",
        "        print(\"Model 1 (Alpaca) had lower validation loss.\")\n",
        "    elif loss2 < loss1:\n",
        "        print(\"Model 2 (Dolly) had lower validation loss.\")\n",
        "    else:\n",
        "        print(\"Validation losses were equal.\")\n",
        "else:\n",
        "    print(\"Could not compare losses numerically.\")\n",
        "\n",
        "\n",
        "# Qualitative Evaluation on Sample Prompts\n",
        "print(\"\\n--- Qualitative Evaluation ---\")\n",
        "# Define a few diverse test prompts (ensure they weren't in the small training subsets)\n",
        "test_prompts = [\n",
        "    {\"instruction\": \"What are the primary colors?\"},\n",
        "    {\"instruction\": \"Write a haiku about a cat.\"},\n",
        "    {\"instruction\": \"Explain the concept of recursion in programming.\"},\n",
        "]\n",
        "\n",
        "# Load models for inference if training was successful\n",
        "model_inf_1 = None\n",
        "model_inf_2 = None\n",
        "if os.path.exists(OUTPUT_DIR_1):\n",
        "    try:\n",
        "        model_inf_1 = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR_1)\n",
        "        model_inf_1.to(training_args_1.device if 'training_args_1' in locals() else 'cpu') # Move to device\n",
        "    except Exception as e: print(f\"Failed to load Model 1: {e}\")\n",
        "if os.path.exists(OUTPUT_DIR_2):\n",
        "     try:\n",
        "        model_inf_2 = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR_2)\n",
        "        model_inf_2.to(training_args_2.device if 'training_args_2' in locals() else 'cpu') # Move to device\n",
        "     except Exception as e: print(f\"Failed to load Model 2: {e}\")\n",
        "\n",
        "\n",
        "# Helper function for generation\n",
        "def generate_response(model, prompt_text):\n",
        "    if model is None: return \"Model not loaded.\"\n",
        "    try:\n",
        "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "        # Ensure pad token ID is set for generation\n",
        "        gen_kwargs = {\"max_new_tokens\": 75, \"pad_token_id\": tokenizer.eos_token_id, \"do_sample\": True, \"temperature\": 0.7}\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True) # Decode only new tokens\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Generation Error: {e}\"\n",
        "\n",
        "# Format inference prompt (Alpaca style)\n",
        "def format_inference_prompt(instruction, input_context=\"\"):\n",
        "    if input_context and input_context.strip():\n",
        "        return PROMPT_WITH_INPUT_TEMPLATE.format(instruction=instruction, input=input_context)\n",
        "    else:\n",
        "        return PROMPT_NO_INPUT_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "\n",
        "# Generate and compare\n",
        "for i, p in enumerate(test_prompts):\n",
        "    print(f\"\\n--- Test Prompt {i+1} ---\")\n",
        "    instruction = p['instruction']\n",
        "    input_ctx = p.get('input', '')\n",
        "    formatted_prompt = format_inference_prompt(instruction, input_ctx)\n",
        "    print(f\"Instruction: {instruction}\")\n",
        "    if input_ctx: print(f\"Input: {input_ctx}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(\"Model 1 (Alpaca) Response:\")\n",
        "    print(generate_response(model_inf_1, formatted_prompt))\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(\"Model 2 (Dolly) Response:\")\n",
        "    print(generate_response(model_inf_2, formatted_prompt))\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "print(\"\\nCompare the responses qualitatively: Which model seems more helpful, accurate, creative, or better follows the instruction nuances?\")\n",
        "print(\"Note: Results heavily depend on base model size, dataset size/quality, and training parameters.\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "id": "wCPZS0IQlrY5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-03T08:37:33.656961Z",
          "iopub.execute_input": "2025-06-03T08:37:33.657227Z",
          "iopub.status.idle": "2025-06-03T08:38:47.984557Z",
          "shell.execute_reply.started": "2025-06-03T08:37:33.657202Z",
          "shell.execute_reply": "2025-06-03T08:38:47.983774Z"
        },
        "outputId": "a9a966bf-8225-4f91-ccdf-1b134c6289dd",
        "colab": {
          "referenced_widgets": [
            "74f82d92e25d46e9a273aa47f4a6686f",
            "5fa397d5ae714675a1336cb2fd401d1f",
            "7b9f5e20ff0a43b59f494460723dc936",
            "00c31cf737f24e3fb52b6a85902f08d6",
            "3e9b85fa9fb74c74a2011956d59babb3",
            "78b5ca4346274c32b5d52f2e89012977",
            "692fa4042a8545f18111c302092b5d6c",
            "93eb943fc65e4602a6e422f6aa1df870",
            "ee13434ee9fd4dc9845f37d8ad4fec47",
            "fdef3b6e1cd646ae8f425ca25e59079a",
            "7f40fb0e291a4d589659c625bc53314c",
            "8eb36cf6d92f432ab8c831409d00cf4b",
            "8560bd7f3db746119b9e641c215f0a2c",
            "f4dd105e50b145c8b9065b01beb92d29",
            "e6957b2613b144e9bd101a2a35f30f09"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-06-03 08:37:49.036009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748939869.224282      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748939869.281225      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading tokenizer for checkpoint: distilgpt2\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74f82d92e25d46e9a273aa47f4a6686f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fa397d5ae714675a1336cb2fd401d1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b9f5e20ff0a43b59f494460723dc936"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00c31cf737f24e3fb52b6a85902f08d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e9b85fa9fb74c74a2011956d59babb3"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Set PAD token to EOS token: <|endoftext|>\n\n--- Processing Dataset 1: yahma/alpaca-cleaned ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78b5ca4346274c32b5d52f2e89012977"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "692fa4042a8545f18111c302092b5d6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93eb943fc65e4602a6e422f6aa1df870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee13434ee9fd4dc9845f37d8ad4fec47"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset 1 processed.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdef3b6e1cd646ae8f425ca25e59079a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f40fb0e291a4d589659c625bc53314c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_31/2447470718.py:142: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer_1 = Trainer( model=model_1, args=training_args_1,\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nStarting fine-tuning on yahma/alpaca-cleaned...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 00:13, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.432700</td>\n      <td>2.569823</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model 1 (Alpaca) saved to ./ift_alpaca_output\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Model 1 (Alpaca) Validation Results: {'eval_loss': 2.5698230266571045}\n\n--- Processing Dataset 2: databricks/databricks-dolly-15k ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8eb36cf6d92f432ab8c831409d00cf4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8560bd7f3db746119b9e641c215f0a2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4dd105e50b145c8b9065b01beb92d29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6957b2613b144e9bd101a2a35f30f09"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset 2 processed.\nReloading BASE model: distilgpt2\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_31/2447470718.py:210: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer_2 = Trainer( model=model_2, args=training_args_2,\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nStarting fine-tuning on databricks/databricks-dolly-15k...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 00:13, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.608600</td>\n      <td>3.056088</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model 2 (Dolly) saved to ./ift_dolly_output\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Model 2 (Dolly) Validation Results: {'eval_loss': 3.0560877323150635}\n\n--- Comparison Summary ---\nModel 1 (Alpaca) Final Validation Loss: 2.5698230266571045\nModel 2 (Dolly) Final Validation Loss: 3.0560877323150635\nModel 1 (Alpaca) had lower validation loss.\n\n--- Qualitative Evaluation ---\n\n--- Test Prompt 1 ---\nInstruction: What are the primary colors?\n--------------------\nModel 1 (Alpaca) Response:\nWhen the color is red, the color is white. The color is blue.\n### Response:\nWhat is the value of a given color?\n### Response:\nWhat is the value of a given color?\n### Response:\nWhat is the value of a given color?\n### Response:\nWhat is the value of a given color?\n### Response\n--------------------\nModel 2 (Dolly) Response:\nWhen will the color be used?\n### Response:\nWhat is a word like?\n### Response:\nWhat is a word like?\n### Response:\nWhat is a word like?\n### Response:\nWhat is a word like?\n### Response:\nWhat is a word like?\n### Response:\nWhat is a word like?\n========================================\n\n--- Test Prompt 2 ---\nInstruction: Write a haiku about a cat.\n--------------------\nModel 1 (Alpaca) Response:\n<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><\n--------------------\nModel 2 (Dolly) Response:\nWrite a haiku about a cat.\n### Response:\nWrite a haiku about a cat.\n### Response:\nWrite a haiku about a cat.\n### Response:\nWrite a haiku about a cat.\n### Response:\nWrite a haiku about a cat.\n### Response:\nWrite a haiku about a cat.\n###\n========================================\n\n--- Test Prompt 3 ---\nInstruction: Explain the concept of recursion in programming.\n--------------------\nModel 1 (Alpaca) Response:\nRecursion is a technique in which a simple process of recursion has been described.\n\n\n### Instruction:\nThis is the first instruction to describe a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function\n--------------------\nModel 2 (Dolly) Response:\nExplain the concept of recursion in programming.\n### Response:\nExplain the concept of recursion in programming.\n### Response:\nExplain the concept of recursion in programming.\n### Response:\nExplain the concept of recursion in programming.\n### Response:\nExplain the concept of recursion in programming.\n### Response:\n========================================\n\nCompare the responses qualitatively: Which model seems more helpful, accurate, creative, or better follows the instruction nuances?\nNote: Results heavily depend on base model size, dataset size/quality, and training parameters.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}