{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-1: Loading Open Source Model"
      ],
      "metadata": {
        "id": "tvG-6EZEfID7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Loading an Open Source Model from Hugging Face Hub ---\n",
        "# Goal: Load a pre-trained open-source model and its tokenizer.\n",
        "# Library: Hugging Face Transformers\n",
        "# Note: Ensure you have run `pip install transformers torch accelerate` in your environment.\n",
        "#       Some models (like Meta's Llama) are \"gated\" and require authentication.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T09:06:37.634216Z",
          "iopub.execute_input": "2025-04-22T09:06:37.634722Z",
          "iopub.status.idle": "2025-04-22T09:06:40.872725Z",
          "shell.execute_reply.started": "2025-04-22T09:06:37.634697Z",
          "shell.execute_reply": "2025-04-22T09:06:40.871993Z"
        },
        "id": "yLvmXMZioMkj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Choose a model ID from the Hugging Face Hub.\n",
        "# Examples:\n",
        "# 'mistralai/Mistral-7B-Instruct-v0.1' # Good performance, Apache 2.0 license\n",
        "# 'google/gemma-2b-it' # Google's Gemma instruct-tuned model\n",
        "# 'distilgpt2' # Very small, good for quick tests\n",
        "model_id = \"google/gemma-2b-it\"# Using Gemma 2B instruct as an example"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T09:06:40.874030Z",
          "iopub.execute_input": "2025-04-22T09:06:40.874455Z",
          "iopub.status.idle": "2025-04-22T09:06:40.878083Z",
          "shell.execute_reply.started": "2025-04-22T09:06:40.874428Z",
          "shell.execute_reply": "2025-04-22T09:06:40.877378Z"
        },
        "id": "lcIrXplHoMkk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "whoami = api.whoami(token=\"hf_xxxxxxxxxxxxxxx\")\n",
        "print(whoami)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T09:06:42.531241Z",
          "iopub.execute_input": "2025-04-22T09:06:42.531922Z",
          "iopub.status.idle": "2025-04-22T09:06:42.629395Z",
          "shell.execute_reply.started": "2025-04-22T09:06:42.531892Z",
          "shell.execute_reply": "2025-04-22T09:06:42.628614Z"
        },
        "id": "Sa4xybfvoMkk",
        "outputId": "2f887b11-d682-433a-b28a-781a507e8fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'type': 'user', 'id': '65feba1b57cc48d9d30d11cf', 'name': 'kalpasubbaiah', 'fullname': 'Kalpa Subbaiah', 'email': 'kalpa.subbaiah@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/319094e0eb55ce89334d7bd3685ceeb0.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'hugging_face_token_read', 'role': 'read', 'createdAt': '2025-04-22T09:03:46.223Z'}}}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Authentication (Optional - Needed for Gated Models like Llama) ---\n",
        "# If using a gated model, you need to:\n",
        "# 1. Accept the license terms on the model's Hugging Face page.\n",
        "# 2. Log in using the Hugging Face CLI: `huggingface-cli login`\n",
        "#    This saves your token locally. The library will then use it automatically.\n",
        "# Alternatively, provide the token explicitly (less secure):\n",
        "# use_auth_token = \"hf_YOUR_HUGGINGFACE_TOKEN\" # Replace with your actual token\n",
        "#use_auth_token = 'REPLACE_WITH_YOUR_HUGGING_FACE_TOKEN'# Get access to the model in hugging face by accepting the usage term and copy the Hugging face token here.\n",
        "                                                       # In case you have not created the HUGGING FACE token , create the token and copy it here once you accept the model terms.\n",
        "                                                       # make sure you have given the read permission while creating the token.\n",
        "use_auth_token = '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' # Set to None or False if model is not gated or logged in via CLI\n",
        "\n",
        "print(f\"Loading tokenizer for model: {model_id}\")\n",
        "try:\n",
        "    # 1. Load the Tokenizer\n",
        "    #    AutoTokenizer automatically selects the correct tokenizer class based on the model ID.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=use_auth_token)\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    print(\"Check model ID, internet connection, and authentication if required.\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "id": "PkTBp-DGfESt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T09:58:35.347373Z",
          "iopub.execute_input": "2025-04-22T09:58:35.347650Z",
          "iopub.status.idle": "2025-04-22T09:58:36.654656Z",
          "shell.execute_reply.started": "2025-04-22T09:58:35.347627Z",
          "shell.execute_reply": "2025-04-22T09:58:36.654004Z"
        },
        "outputId": "a386fc1a-0635-41da-f938-a6b3fff3df68"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading tokenizer for model: google/gemma-2b-it\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Tokenizer loaded successfully.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nLoading model: {model_id}\")\n",
        "print(\"This might take a while depending on model size and download speed...\")\n",
        "try:\n",
        "    # 2. Load the Model\n",
        "    #    AutoModelForCausalLM is suitable for text generation models.\n",
        "    #    Use `device_map='auto'` to automatically distribute the model across available GPUs (requires accelerate).\n",
        "    #    Use `torch_dtype=torch.bfloat16` (if supported by GPU) or torch.float16 for memory savings.\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        token=use_auth_token,\n",
        "        device_map=\"auto\", # Automatically use available GPU(s) or CPU\n",
        "        torch_dtype=torch.bfloat16 # Use bfloat16 for efficiency if available\n",
        "    )\n",
        "    print(f\"Model loaded successfully onto device: {model.device}\") # Will show cuda:0 if GPU is used\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Check model ID, internet connection, authentication, and available GPU memory.\")\n",
        "    exit()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T09:06:55.949419Z",
          "iopub.execute_input": "2025-04-22T09:06:55.950176Z",
          "iopub.status.idle": "2025-04-22T09:07:40.495013Z",
          "shell.execute_reply.started": "2025-04-22T09:06:55.950148Z",
          "shell.execute_reply": "2025-04-22T09:07:40.494401Z"
        },
        "id": "TGZW2NJxoMkm",
        "outputId": "d127e312-0bb0-4fc3-ed91-5bda6740deac",
        "colab": {
          "referenced_widgets": [
            "33e090315bc3406fb71877f39262a199",
            "296c2a8f03674b1f844a1daaad915359",
            "4e10df5b0d884091aa56251aa61a4c7a",
            "863d71900f3b4d82bf29ee53dec0a799",
            "66bf1a6cd1614f03b3bdb8f37b64815b",
            "be66e7b0d79043738f288bb72bc8e407",
            "0f8df23c478841cc8561cb3855b29ad7"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nLoading model: google/gemma-2b-it\nThis might take a while depending on model size and download speed...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33e090315bc3406fb71877f39262a199"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "2025-04-22 09:07:01.633017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745312821.836290     220 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745312821.898286     220 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "296c2a8f03674b1f844a1daaad915359"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e10df5b0d884091aa56251aa61a4c7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "863d71900f3b4d82bf29ee53dec0a799"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66bf1a6cd1614f03b3bdb8f37b64815b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be66e7b0d79043738f288bb72bc8e407"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f8df23c478841cc8561cb3855b29ad7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Model loaded successfully onto device: cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Verification ---\n",
        "# You now have the 'tokenizer' and 'model' objects ready for use in other recipes.\n",
        "print(\"\\nModel and Tokenizer are ready!\")\n",
        "\n",
        "# Example: Tokenize a sample text\n",
        "sample_text = \"Hello, LLM Chef!\"\n",
        "tokens = tokenizer.encode(sample_text)\n",
        "print(f\"\\nSample text: '{sample_text}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "decoded_text = tokenizer.decode(tokens)\n",
        "print(f\"Decoded tokens: '{decoded_text}'\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T09:08:35.595959Z",
          "iopub.execute_input": "2025-04-22T09:08:35.597195Z",
          "iopub.status.idle": "2025-04-22T09:08:35.605800Z",
          "shell.execute_reply.started": "2025-04-22T09:08:35.597157Z",
          "shell.execute_reply": "2025-04-22T09:08:35.604905Z"
        },
        "id": "xTz7vm-AoMkm",
        "outputId": "62a63994-5b48-41ff-cafd-a5d60c4433b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nModel and Tokenizer are ready!\n\nSample text: 'Hello, LLM Chef!'\nTokens: [2, 4521, 235269, 629, 18622, 36614, 235341]\nDecoded tokens: '<bos>Hello, LLM Chef!'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-2: Calling a Proprietary Model API"
      ],
      "metadata": {
        "id": "sFYBX1o8fe4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Requires setting the API key as an environment variable:\n",
        "# export OPENAI_API_KEY='sk-YOUR_ACTUAL_API_KEY' using the terminal\n",
        "# or using the python code below\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-lSxx7Mgmzxm07z2b4BqbT3BlbkFJ3shMqcS4pPz616k25tLv\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T11:04:31.858321Z",
          "iopub.execute_input": "2025-04-22T11:04:31.859005Z",
          "iopub.status.idle": "2025-04-22T11:04:31.862331Z",
          "shell.execute_reply.started": "2025-04-22T11:04:31.858974Z",
          "shell.execute_reply": "2025-04-22T11:04:31.861572Z"
        },
        "id": "DWQ7mxf6oMkm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Calling a Proprietary Model API ---\n",
        "# Goal: Demonstrate making an API call to a hypothetical closed-source LLM endpoint.\n",
        "# Library: requests (standard Python library), openai (example specific library)\n",
        "# Note: This uses hypothetical examples. Replace with actual API details from your provider.\n",
        "#       NEVER hardcode API keys directly in code for production systems. Use environment variables or secrets management.\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "\n",
        "# --- Using a specific library like OpenAI (Recommended if available) ---\n",
        "# Ensure you have the library installed: pip install openai\n",
        "# Requires setting the API key as an environment variable:\n",
        "# export OPENAI_API_KEY='sk-YOUR_ACTUAL_API_KEY'\n",
        "\n",
        "# Example using the OpenAI library structure (adapt for other providers like Anthropic, Google)\n",
        "print(\"--- Example using OpenAI library ---\")\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    # The client automatically picks up the OPENAI_API_KEY environment variable\n",
        "    client = OpenAI() # Add base_url if using a non-OpenAI compatible endpoint\n",
        "\n",
        "    # Check if API key is set (optional but good practice)\n",
        "    if not client.api_key:\n",
        "        print(\"Error: OPENAI_API_KEY environment variable not set.\")\n",
        "    else:\n",
        "        print(\"OpenAI client initialized.\")\n",
        "        prompt_text = \"Explain the concept of Parameter-Efficient Fine-Tuning (PEFT) in one paragraph.\"\n",
        "        model_to_use = \"gpt-3.5-turbo\" # Or other available model like \"gpt-4\"\n",
        "\n",
        "        print(f\"\\nSending prompt to model: {model_to_use}\")\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model_to_use,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant explaining complex ML concepts simply.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt_text}\n",
        "            ],\n",
        "            max_tokens=150, # Limit the length of the response\n",
        "            temperature=0.7 # Controls randomness (0=deterministic, >1=more random)\n",
        "        )\n",
        "\n",
        "        # Extract and print the response\n",
        "        response_text = completion.choices[0].message.content\n",
        "        print(\"\\nAPI Response:\")\n",
        "        print(response_text)\n",
        "        print(\"\\nUsage Info:\")\n",
        "        print(completion.usage) # Shows token usage\n",
        "\n",
        "except ImportError:\n",
        "    print(\"OpenAI library not found. Skipping OpenAI example.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during OpenAI API call: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:22:44.812458Z",
          "iopub.execute_input": "2025-04-22T10:22:44.813220Z",
          "iopub.status.idle": "2025-04-22T10:22:47.965659Z",
          "shell.execute_reply.started": "2025-04-22T10:22:44.813193Z",
          "shell.execute_reply": "2025-04-22T10:22:47.964966Z"
        },
        "id": "Z2S1QE-PoMkn",
        "outputId": "28e8caf5-9360-40b3-db28-f7244faac4ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Example using OpenAI library ---\nOpenAI client initialized.\n\nSending prompt to model: gpt-3.5-turbo\n\nAPI Response:\nParameter-Efficient Fine-Tuning (PEFT) is a technique in machine learning where a pre-trained model is fine-tuned on a smaller dataset with fewer parameters to achieve high performance. Instead of training the entire model from scratch, PEFT focuses on updating only a subset of the model's parameters that are essential for the new task, thereby reducing the computational resources and time required for training. This approach allows for faster adaptation of the pre-trained model to new tasks while maintaining or even improving its performance, making it a more efficient and effective way to leverage pre-trained models for various applications.\n\nUsage Info:\nCompletionUsage(completion_tokens=119, prompt_tokens=43, total_tokens=162, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#API CALL\n",
        "# --- Generic Example using 'requests' library ---\n",
        "# Useful if the provider doesn't have a dedicated Python library or for simple calls.\n",
        "print(\"\\n--- Example using generic 'requests' library ---\")\n",
        "\n",
        "# --- Configuration (Replace with actual values) ---\n",
        "# **NEVER COMMIT ACTUAL KEYS TO VERSION CONTROL**\n",
        "# Load from environment variables is best practice:\n",
        "# api_key = os.environ.get(\"VENDOR_API_KEY\")\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\") # Replace or load from env var\n",
        "api_endpoint_url = \"https://api.openai.com/v1/chat/completions\" # Replace with actual endpoint\n",
        "\n",
        "if api_key == \"YOUR_VENDOR_API_KEY_PLACEHOLDER\":\n",
        "    print(\"Warning: Using placeholder API key. Set a real key for actual use.\")\n",
        "\n",
        "# --- Prepare Request ---\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "messages =  [\n",
        "    {\"role\": \"user\", \"content\": \"Write a short tagline for an LLM cookbook.\"}\n",
        "  ]\n",
        "\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\", # Replace with actual model name\n",
        "    \"messages\": messages,\n",
        "    \"max_tokens\": 20,\n",
        "    \"temperature\": 0.8\n",
        "}\n",
        "\n",
        "print(f\"\\nSending request to generic endpoint: {api_endpoint_url}\")\n",
        "try:\n",
        "    # --- Make API Call ---\n",
        "    response = requests.post(api_endpoint_url, headers=headers, data=json.dumps(data))\n",
        "    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "\n",
        "    # --- Process Response ---\n",
        "    response_data = response.json()\n",
        "    print(\"\\nAPI Response (JSON):\")\n",
        "    print(json.dumps(response_data, indent=2))\n",
        "\n",
        "    # Extract the actual generated text (structure depends on API provider)\n",
        "    # This is a hypothetical structure, adjust based on the actual API response\n",
        "    if \"choices\" in response_data and len(response_data[\"choices\"]) > 0:\n",
        "        generated_text = response_data[\"choices\"][0][\"message\"].get(\"content\", \"N/A\")\n",
        "        print(f\"\\nGenerated Text: {generated_text.strip()}\")\n",
        "    else:\n",
        "        print(\"\\nCould not extract generated text from response.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during generic API call: {e}\")\n",
        "    if response is not None:\n",
        "        print(f\"Response status code: {response.status_code}\")\n",
        "        print(f\"Response text: {response.text}\")\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# --- End of Recipe ---\n"
      ],
      "metadata": {
        "id": "hP4mhYrDfRax",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T11:18:50.185620Z",
          "iopub.execute_input": "2025-04-22T11:18:50.185887Z",
          "iopub.status.idle": "2025-04-22T11:18:50.769438Z",
          "shell.execute_reply.started": "2025-04-22T11:18:50.185866Z",
          "shell.execute_reply": "2025-04-22T11:18:50.768771Z"
        },
        "outputId": "4e4b1e8f-195f-4d9e-bbf7-0e75608051e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Example using generic 'requests' library ---\n\nSending request to generic endpoint: https://api.openai.com/v1/chat/completions\n\nAPI Response (JSON):\n{\n  \"id\": \"chatcmpl-BP5wIeziwMdmgP4hAavUAabBAB9Dp\",\n  \"object\": \"chat.completion\",\n  \"created\": 1745320730,\n  \"model\": \"gpt-3.5-turbo-0125\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"\\\"Elevate your home cooking with delicious and innovative recipes from our LLM cookbook!\\\"\",\n        \"refusal\": null,\n        \"annotations\": []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 18,\n    \"completion_tokens\": 18,\n    \"total_tokens\": 36,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": null\n}\n\nGenerated Text: \"Elevate your home cooking with delicious and innovative recipes from our LLM cookbook!\"\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-3: Basic Inference Tasks (Generation & Zero-Shot Classification)"
      ],
      "metadata": {
        "id": "5mTLH6MNfi1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Basic Inference Tasks (Generation & Zero-Shot Classification) ---\n",
        "# Goal: Use a loaded model for common tasks via Hugging Face pipelines.\n",
        "# Library: Hugging Face Transformers\n",
        "# Prerequisite: A model and tokenizer should be loaded (e.g., from ch3_recipe_load_oss).\n",
        "#               Or use a model ID directly within the pipeline.\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# --- Option 1: Use a model ID directly in the pipeline (Easier for standard tasks) ---\n",
        "print(\"--- Option 1: Using Pipelines with Model ID ---\")\n",
        "\n",
        "# Text Generation\n",
        "print(\"\\nLoading text-generation pipeline...\")\n",
        "try:\n",
        "    # Using a small model for quick demo\n",
        "    generator = pipeline('text-generation', model='distilgpt2', device=0 if torch.cuda.is_available() else -1) # Use GPU if available\n",
        "    prompt = \"The secret ingredient in the best AI recipes is\"\n",
        "    print(f\"Generating text for prompt: '{prompt}'\")\n",
        "    outputs = generator(prompt, max_new_tokens=30, num_return_sequences=1) # max_new_tokens generates 30 tokens *after* the prompt\n",
        "    print(\"Generated Text:\")\n",
        "    print(outputs[0]['generated_text'])\n",
        "except Exception as e:\n",
        "    print(f\"Error during text generation: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T11:21:20.525295Z",
          "iopub.execute_input": "2025-04-22T11:21:20.525551Z",
          "iopub.status.idle": "2025-04-22T11:21:24.856793Z",
          "shell.execute_reply.started": "2025-04-22T11:21:20.525531Z",
          "shell.execute_reply": "2025-04-22T11:21:24.856166Z"
        },
        "id": "nI6FxnjuoMko",
        "outputId": "7a178415-8245-479f-c953-6684e6cbbba3",
        "colab": {
          "referenced_widgets": [
            "1125a10770c1443ebdadb4f522c5aa19",
            "b06c0fe2915a4f8282fbfb999146b2c5"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Option 1: Using Pipelines with Model ID ---\n\nLoading text-generation pipeline...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1125a10770c1443ebdadb4f522c5aa19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b06c0fe2915a4f8282fbfb999146b2c5"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Generating text for prompt: 'The secret ingredient in the best AI recipes is'\nGenerated Text:\nThe secret ingredient in the best AI recipes is that you can see exactly how many times you read the words, or by the amount of time you read them, or by the amount of time you\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-Shot Classification\n",
        "print(\"\\nLoading zero-shot-classification pipeline...\")\n",
        "try:\n",
        "    # Using a model fine-tuned for Natural Language Inference (NLI), suitable for zero-shot\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if torch.cuda.is_available() else -1)\n",
        "    sequence_to_classify = \"This cookbook focuses on practical LLM fine-tuning.\"\n",
        "    candidate_labels = [\"machine learning\", \"cooking\", \"finance\", \"sports\"]\n",
        "    print(f\"\\nClassifying sequence: '{sequence_to_classify}'\")\n",
        "    print(f\"With candidate labels: {candidate_labels}\")\n",
        "    results = classifier(sequence_to_classify, candidate_labels)\n",
        "    print(\"\\nClassification Results:\")\n",
        "    # Print results sorted by score\n",
        "    sorted_results = sorted(zip(results['labels'], results['scores']), key=lambda x: x[1], reverse=True)\n",
        "    for label, score in sorted_results:\n",
        "        print(f\"- {label}: {score:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during zero-shot classification: {e}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T11:21:49.222991Z",
          "iopub.execute_input": "2025-04-22T11:21:49.223548Z",
          "iopub.status.idle": "2025-04-22T11:21:57.723654Z",
          "shell.execute_reply.started": "2025-04-22T11:21:49.223529Z",
          "shell.execute_reply": "2025-04-22T11:21:57.723039Z"
        },
        "id": "plUd6hqNoMko",
        "outputId": "dc8f4bd6-45e1-4b08-b663-4a126e3af12d",
        "colab": {
          "referenced_widgets": [
            "f15f82481ef449faab166fac192db0d5",
            "63721b34cf7e483e821b1bf1d2204ea6",
            "eefc27a4cb8c461dbb2efcdc2251d8a6",
            "833be17b82244c8d9bea96a06159f3df",
            "4b233967c08041329fb431f8c50476b4",
            "9ff761c72800419ab6382cc1e5f77577"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nLoading zero-shot-classification pipeline...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f15f82481ef449faab166fac192db0d5"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63721b34cf7e483e821b1bf1d2204ea6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eefc27a4cb8c461dbb2efcdc2251d8a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "833be17b82244c8d9bea96a06159f3df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b233967c08041329fb431f8c50476b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ff761c72800419ab6382cc1e5f77577"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nClassifying sequence: 'This cookbook focuses on practical LLM fine-tuning.'\nWith candidate labels: ['machine learning', 'cooking', 'finance', 'sports']\n\nClassification Results:\n- cooking: 0.9307\n- machine learning: 0.0529\n- sports: 0.0090\n- finance: 0.0074\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Option 2: Use pre-loaded model and tokenizer (More control, useful if model already loaded) ---\n",
        "print(\"\\n--- Option 2: Using Pipelines with Pre-loaded Model/Tokenizer ---\")\n",
        "# This assumes you have 'model' and 'tokenizer' variables from ch3_recipe_load_oss\n",
        "# We'll reload Gemma here for demonstration if the previous recipe wasn't run contiguously.\n",
        "\n",
        "model_id_loaded = \"google/gemma-2b-it\" # Make sure this matches the model you intend to load/use\n",
        "preloaded_model = None\n",
        "preloaded_tokenizer = None\n",
        "\n",
        "try:\n",
        "    print(f\"\\nAttempting to load {model_id_loaded} for Option 2 demo...\")\n",
        "    preloaded_tokenizer = AutoTokenizer.from_pretrained(model_id_loaded)\n",
        "    # Load specifically for Causal LM if planning text generation\n",
        "    preloaded_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id_loaded,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16 # Use bfloat16 for efficiency if available\n",
        "    )\n",
        "    print(\"Pre-loaded model and tokenizer ready.\")\n",
        "\n",
        "    # Create pipeline using the loaded components\n",
        "    generator_loaded = pipeline('text-generation', model=preloaded_model, tokenizer=preloaded_tokenizer) # device is inferred from model.device\n",
        "\n",
        "    prompt_loaded = \"To build a great AI application, you need\"\n",
        "    print(f\"\\nGenerating text using pre-loaded model: '{prompt_loaded}'\")\n",
        "    outputs_loaded = generator_loaded(prompt_loaded, max_new_tokens=30, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
        "    print(\"Generated Text (Pre-loaded):\")\n",
        "    print(outputs_loaded[0]['generated_text'])\n",
        "\n",
        "    # Note: For zero-shot classification with a pre-loaded model, ensure the loaded model\n",
        "    # is suitable for classification (e.g., AutoModelForSequenceClassification) and the task.\n",
        "    # Using a CausalLM like Gemma directly in a zero-shot pipeline might not yield optimal results\n",
        "    # without specific fine-tuning or prompt engineering for that task.\n",
        "    print(\"\\n(Skipping zero-shot with pre-loaded CausalLM model as it's not ideal for the task without adaptation)\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nSkipping Option 2 as 'model' and 'tokenizer' variables are not defined.\")\n",
        "    print(\"(This likely means the 'ch3_recipe_load_oss' recipe wasn't run just before this one).\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during Option 2 setup or inference: {e}\")\n"
      ],
      "metadata": {
        "id": "LN7LEaRLftDJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T11:22:19.413307Z",
          "iopub.execute_input": "2025-04-22T11:22:19.413858Z",
          "iopub.status.idle": "2025-04-22T11:22:24.166571Z",
          "shell.execute_reply.started": "2025-04-22T11:22:19.413837Z",
          "shell.execute_reply": "2025-04-22T11:22:24.165920Z"
        },
        "outputId": "c2b7ace0-c30c-45a8-ff27-606115b95f04",
        "colab": {
          "referenced_widgets": [
            "0e22e03cdad241f2b911af866e0258e0"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n--- Option 2: Using Pipelines with Pre-loaded Model/Tokenizer ---\n\nAttempting to load google/gemma-2b-it for Option 2 demo...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e22e03cdad241f2b911af866e0258e0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Pre-loaded model and tokenizer ready.\n\nGenerating text using pre-loaded model: 'To build a great AI application, you need'\nGenerated Text (Pre-loaded):\nTo build a great AI application, you need to address several critical topics:\n\n**1. Data Quality and Privacy:**\n\n* **Data Sources:** Identify and assess the data sources used to train\n\n(Skipping zero-shot with pre-loaded CausalLM model as it's not ideal for the task without adaptation)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recipe-4: Comparing Model Outputs"
      ],
      "metadata": {
        "id": "qBUHdJSJgAlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recipe: Comparing Outputs of Different Models ---\n",
        "# Goal: Run the same prompt through two different models and compare their outputs.\n",
        "# Library: Hugging Face Transformers\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "import torch\n",
        "\n",
        "# --- Configuration ---\n",
        "# Choose two different model IDs\n",
        "model_id_1 = \"distilgpt2\" # Smaller, faster model\n",
        "model_id_2 = \"google/gemma-2b-it\" # Larger, potentially more capable model (requires more resources)\n",
        "\n",
        "prompt = \"The future of artificial intelligence is\"\n",
        "\n",
        "# Set seed for reproducibility of generation (if models use sampling)\n",
        "set_seed(123)\n",
        "\n",
        "# --- Load Pipelines for Both Models ---\n",
        "print(f\"Loading pipeline for Model 1: {model_id_1}\")\n",
        "try:\n",
        "    # Use device=0 for GPU if available, otherwise -1 for CPU\n",
        "    device_index = 0 if torch.cuda.is_available() else -1\n",
        "    generator1 = pipeline('text-generation', model=model_id_1, device=device_index)\n",
        "    print(\"Pipeline 1 loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pipeline 1: {e}\")\n",
        "    generator1 = None # Ensure variable exists but is None\n",
        "\n",
        "print(f\"\\nLoading pipeline for Model 2: {model_id_2}\")\n",
        "try:\n",
        "    # Gemma might require bfloat16 for efficiency on some GPUs\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
        "    generator2 = pipeline('text-generation', model=model_id_2, device=device_index, torch_dtype=dtype)\n",
        "    print(\"Pipeline 2 loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pipeline 2: {e}\")\n",
        "    print(\"This could be due to insufficient GPU memory for Gemma-2B.\")\n",
        "    generator2 = None # Ensure variable exists but is None\n",
        "\n",
        "# --- Generate and Compare Outputs ---\n",
        "print(f\"\\n--- Comparing Outputs for Prompt: '{prompt}' ---\")\n",
        "\n",
        "output1 = \"Pipeline 1 failed to load.\"\n",
        "if generator1:\n",
        "    print(f\"\\nGenerating with Model 1 ({model_id_1})...\")\n",
        "    try:\n",
        "        outputs1 = generator1(prompt, max_new_tokens=50, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
        "        output1 = outputs1[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        output1 = f\"Error during generation with Model 1: {e}\"\n",
        "print(f\"\\nOutput from {model_id_1}:\\n{output1}\")\n",
        "\n",
        "output2 = \"Pipeline 2 failed to load or generation failed.\"\n",
        "if generator2:\n",
        "    print(f\"\\nGenerating with Model 2 ({model_id_2})...\")\n",
        "    try:\n",
        "        # Gemma instruct models often need specific prompt formatting if not using pipeline defaults\n",
        "        # For basic comparison, we'll use the raw prompt here.\n",
        "        outputs2 = generator2(prompt, max_new_tokens=50, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
        "        output2 = outputs2[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        output2 = f\"Error during generation with Model 2: {e}\"\n",
        "print(f\"\\nOutput from {model_id_2}:\\n{output2}\")\n",
        "\n",
        "print(\"\\n--- Comparison Complete ---\")\n",
        "#\n",
        "by SYBOObserve differences in style, coherence, length, factual accuracy (if applicable), etc.\n"
      ],
      "metadata": {
        "id": "oobriof2gBJC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T11:22:53.485133Z",
          "iopub.execute_input": "2025-04-22T11:22:53.485816Z",
          "iopub.status.idle": "2025-04-22T11:22:59.124006Z",
          "shell.execute_reply.started": "2025-04-22T11:22:53.485792Z",
          "shell.execute_reply": "2025-04-22T11:22:59.123307Z"
        },
        "outputId": "59fe6750-1e25-45c9-de5b-6e9eb16daa6d",
        "colab": {
          "referenced_widgets": [
            "7557376627f14f23a4bafefdde1e9b06"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading pipeline for Model 1: distilgpt2\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Pipeline 1 loaded.\n\nLoading pipeline for Model 2: google/gemma-2b-it\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7557376627f14f23a4bafefdde1e9b06"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Pipeline 2 loaded.\n\n--- Comparing Outputs for Prompt: 'The future of artificial intelligence is' ---\n\nGenerating with Model 1 (distilgpt2)...\n\nOutput from distilgpt2:\nThe future of artificial intelligence is as important as ever, and that's a great question. It's a very difficult question to answer. So, I think you might be able to answer it, but there's a lot of work to be done, and it has to be done\n\nGenerating with Model 2 (google/gemma-2b-it)...\n\nOutput from google/gemma-2b-it:\nThe future of artificial intelligence is rapidly evolving, and the pace of change is accelerating. As AI becomes more sophisticated, it will have a profound impact on society, both positive and negative.\n\nHere are some of the key trends that are shaping the future of AI:\n\n* **\n\n--- Comparison Complete ---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}